[
  {
    "id": "2303.10780v2",
    "text": "A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices Kai Malcolm 1Josue Casco-Rodriguez 1 Abstract Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and underinvestigated is biologically plausible, energyefcient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_0",
    "chunk_position": 0
  },
  {
    "id": "2303.10780v2",
    "text": ". We present a literature review of recent developments in the interpretation, optimization, efciency, and accuracy of spiking neural networks. Key contributions include identication, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efciency, and evaluation, starting from rst principles so as to be accessible to new practitioners. 1",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_1",
    "chunk_position": 1
  },
  {
    "id": "2303.10780v2",
    "text": ". 1. Introduction Spiking neural networks (SNNs) offer a perspective on machine learning different than that of traditional articial neural networks (ANNs): SNNs encode data in a series of discrete time spikes, modeled after the action potentials created by neurons in biological brains. The original development of ANNs was also inspired by the human brain, which consists of nearly 100 billion neurons, each of which can have up to 15,000 synapses. Despite this potentially massive parameter space, the human brain only consumes about 20 W and takes up under 3 pounds",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_2",
    "chunk_position": 2
  },
  {
    "id": "2303.10780v2",
    "text": ". Despite this potentially massive parameter space, the human brain only consumes about 20 W and takes up under 3 pounds. For comparison, todays top-performing language models can require over 300 k Wh of power . Biological neural networks impressive energyand space-efciency is due in part to the fact that most neurons are usually inactive, meaning that the brain is a highly sparse neural network",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_3",
    "chunk_position": 3
  },
  {
    "id": "2303.10780v2",
    "text": ". SNNs attempt to combine more biologically plausible inputs, learning methods, and architectures in order to match contribution1Department of Electrical and Computer Engineering, Rice University, Texas, United States. Correspondence to: Kai Malcolm kai.malcolmrice.edu , Josue CascoRodriguez josue.casco-rodriguezrice.edu .the performance of ANNs while also achieving the energy efciency and robustness of the human brain",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_4",
    "chunk_position": 4
  },
  {
    "id": "2303.10780v2",
    "text": ". Currently, the development of SNNs is a growing eld, but lacks clear benchmarks and tool sets, complete optimization of nonlinear spiking dynamics, and robust mechanisms for encoding targets as binary spike trains . The main goal of this literature review is to chronicle recent advances in SNN optimization, with particular focused applied to biologically plausible mechanisms and advances in the optimization of non-differentiable spike trains",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_5",
    "chunk_position": 5
  },
  {
    "id": "2303.10780v2",
    "text": ". From an optimization point of view, SNNs pose a unique and interesting challenge, as they lack interpretable models, straight-forward gradient calculation, and also methods that leverage the inherent advantages of SNNs . We have compiled a meta-analysis of recent SNN advances, providing a quantitative comparison between given implementations when possible. Inherent to this process is the identication of standard datasets to act as benchmarks between different implementations",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_6",
    "chunk_position": 6
  },
  {
    "id": "2303.10780v2",
    "text": ". Inherent to this process is the identication of standard datasets to act as benchmarks between different implementations. Another focus of this work is to propose how to better showcase differences and advancements between recent innovations. For performance comparisons, we have identied the most frequently used benchmark datasets so that we can rank existing SNN architectures in an accessible and understandable format",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_7",
    "chunk_position": 7
  },
  {
    "id": "2303.10780v2",
    "text": ". In a similar vein of work, we have included which tools and frameworks were used to create each model, as there does not exist a standardized set of tools for SNN development (see ). We believe such a characterization of tools available to researchers would be a valuable resource for unifying the eld, making SNNs more accessible to new researchers. 2. Foundational Neurobiology This the groundwork for the biological analog from which SNNs originate, focusing on how neurons in the human brain transmit information. 2.1. Underlying Neuroanatomy Neurons",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_8",
    "chunk_position": 8
  },
  {
    "id": "2303.10780v2",
    "text": ". 2.1. Underlying Neuroanatomy Neurons. The neuron is the fundamental atomic unit of the brain. In humans, there here are three primary categories of neurons: sensory, motor, and inter-neurons. Neurons usuallyar Xiv:2303.10780v2 cs.NE 21 Mar 2023 A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices consist of a bulky soma (i.e., cell body), branching dendrites (which propagate stimuli received from adjacent cells to the soma), and a long axon that forms synaptic junctions with the dendrites of adjacent neurons. Synapses",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_9",
    "chunk_position": 9
  },
  {
    "id": "2303.10780v2",
    "text": ". Synapses. Synapses are the spaces between axon terminals and dendrites through which a presynpatic neuron sends a biological signal to the postsynaptic neuron. Synaptic activity typically consists of either chemical or electrical signals. In the chemical case, presynaptic neuron activation triggers the release of chemical neurotransmitters that bind to receptors lodged in the cell membrane of the postsynpatic neuron, which then cause the propagation of an electrical signal throughout the postsynaptic neuron",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_10",
    "chunk_position": 10
  },
  {
    "id": "2303.10780v2",
    "text": ". In the electrical case, the preand post-synpatic neurons are connected by gap junctions which pass the current from one neuron to the other, with the benet of quicker transmission times (Barnett Larkman, 2007). Action Potentials. Action potentials (APs) are the discrete signals (spikes) through which biological neurons communicate. Each AP is a continuous chain of depolarization events: one segment along the axon experiences a rise and fall of membrane potential (voltage across the membrane), and this rise and fall is propagated along the axon",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_11",
    "chunk_position": 11
  },
  {
    "id": "2303.10780v2",
    "text": ". More specically, APs typically originate from voltage-gated ion channels within a cells membrane, such that when the cells membrane potential increases enough to exceed an intrinsic biological threshold, the channels open, allowing charged ions to enter the cell driven by passive gradients, e.g. both a concentration gradient and an electro-chemical gradient due to difference in charges and ion concentrations inside and outside the cell",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_12",
    "chunk_position": 12
  },
  {
    "id": "2303.10780v2",
    "text": ". both a concentration gradient and an electro-chemical gradient due to difference in charges and ion concentrations inside and outside the cell. Note that the inux of charged ions is to a current owing into the cell, and this rapid increase in membrane potential is commonly referred to as depolarization. The resulting diffusion of ions across the cell membrane creates a positive feedback loop for the electro-chemical gradient: incoming ions raise the cells membrane potential at different points along the membrane, prompting downstream channels to open and allow more ions to enter",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_13",
    "chunk_position": 13
  },
  {
    "id": "2303.10780v2",
    "text": ". Once enough ions have entered the cell, the membrane potential reverses, which prompts the opening of different set of voltage-gated ion channels, this time resulting in an outward ow of ions, repeating the same process as before and bringing the cell membrane back to its resting potential. The drastic drop in membrane potential is known as hyper-polarization (Barnett Larkman, 2007), (Luo Rudy, 1991). During action potentials, the membrane potential undergoes very sharp exponential rise and fall within only a few milliseconds",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_14",
    "chunk_position": 14
  },
  {
    "id": "2303.10780v2",
    "text": ". During action potentials, the membrane potential undergoes very sharp exponential rise and fall within only a few milliseconds. The action potential always drops the neurons resting potential below the normal resting state potential, resulting in a refractory period (typically about 2 milliseconds) in which the neuron cannot undergo another AP. As 1. An illustration by Barnett Larkman (2007) showing the different phases of depolarization and polarization as they relate to action potentials and voltage-gated ion channels",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_15",
    "chunk_position": 15
  },
  {
    "id": "2303.10780v2",
    "text": ". will be discussed shortly, these biological mechanisms are the motivation and mathematical framework for todays neural networks. Important terminology borrowed from biology includes the idea of a spike train (the time indices at which spikes occurred for a given neuron) and ring (the creationpropagation of an AP). Note that APs are synonymous with spikes for this discussion (Barnett Larkman, 2007). 2.2. Dynamical Models of the Brain Hodgkin-Huxley Model",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_16",
    "chunk_position": 16
  },
  {
    "id": "2303.10780v2",
    "text": ". Note that APs are synonymous with spikes for this discussion (Barnett Larkman, 2007). 2.2. Dynamical Models of the Brain Hodgkin-Huxley Model. One of the most popular models for describing the initiation and propagation of action potentials in neurons is the Hodgkin-Huxley (HH) model, as shared by Hodgkin Huxley (1952). The HH model is a conductance-based dynamical system (see 3), in which each neurons membrane is modeled simply as a series of parallel capacitive and resistive electrical components, as shown in (1)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_17",
    "chunk_position": 17
  },
  {
    "id": "2303.10780v2",
    "text": ". Such abstractions of the intricate inner workings of neurons allow for compartmentalized modeling of many neurons. ICmd Vm dtgg Ng (1) The HH model treats each neurons total current ow I as the sum of four separate current sources described as functions of the membrane potential Vm. The rst source Cmd Vm dtarises from the cell membrane separating the neurons internal contents (negatively charged) from the external environment (positively charged), resulting in the membrane acting as a capacitor with capacitance Cm",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_18",
    "chunk_position": 18
  },
  {
    "id": "2303.10780v2",
    "text": ". The other A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices 2. Plotted measurements of (b) an observed spike train and (c) an isolated action potential by Wattanapanitch et al. (2007). three sources take the form g, wheregiis the electrical conductance of ion channel iand Viis the membrane potential threshold at which the direction of current ow in ion channel ireverses (e.g. the electrical gradient is now acting in the opposite direction due to the resulting build up of charge)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_19",
    "chunk_position": 19
  },
  {
    "id": "2303.10780v2",
    "text": ". the electrical gradient is now acting in the opposite direction due to the resulting build up of charge). The specic terms are i2K;Na;l , where Kand Narepresent potassium and sodium ions, and lrepresents membrane potential leakage instead of a specic ion channel. Note that other types of currents may also be included, but these are by far the most common",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_20",
    "chunk_position": 20
  },
  {
    "id": "2303.10780v2",
    "text": ". Note that other types of currents may also be included, but these are by far the most common. Recall from the earlier neuroanatomy, the rst set of opening channels corresponding to the rapid depolarization were the sodium channels, and the second set of channels responsible for bring the cell back to baseline were the potassium channels (Hodgkin Huxley, 1952), (Barnett Larkman, 3. The circuit of a a neurons membrane, as described by the Hodgkin-Huxley model (Thanapitak, 2012). 2007)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_21",
    "chunk_position": 21
  },
  {
    "id": "2303.10780v2",
    "text": ". The circuit of a a neurons membrane, as described by the Hodgkin-Huxley model (Thanapitak, 2012). 2007). Through simple models of neural electrophysiology, scientists can rigorously model the cell membrane, track how it approaches its ring threshold, and therefore understand how best to model the creation and propagation of spikes across neurons. Such understandings yield crucial insights upon which the fundamental ideas of information processing in neural networks can be built",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_22",
    "chunk_position": 22
  },
  {
    "id": "2303.10780v2",
    "text": ". Such understandings yield crucial insights upon which the fundamental ideas of information processing in neural networks can be built. As will be shown, the key similarities between articial and biological spiking neurons are membrane leakage and communication via action potentials (e.g. spiking activity). 3. Biological Plausibility and Interpretability This to show various insights into how spiking neural network (SNN) models can be not only humaninterpretable, but also biologically plausible in silico analogues to biological neural networks. 3.1",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_23",
    "chunk_position": 23
  },
  {
    "id": "2303.10780v2",
    "text": ". 3.1. Biological Plausibility Traditional articial neural networks (ANNs) use oating point numbers to encode the information transmission between neurons, whereas biological neurons communicate via action potentials. Given that action potentials occur and decay quickly, they can be approximated as discrete, binary spikes; when taking sparsity into account, information may be transmitted via highly sparse binary vectors instead of relying on arbitrarily precise oating point numbers. However, exactly how neural spike trains encode information is still debated",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_24",
    "chunk_position": 24
  },
  {
    "id": "2303.10780v2",
    "text": ". However, exactly how neural spike trains encode information is still debated. The two most popular information encoding schemes are discussed below. A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices Rate coding. One of the most popular spike-train information-encoding models is the rate-coding framework, which represents stimuli intensity as a ring rate. If the input stimuli consists of an image, each pixels intensity could be converted to the ring rate of a Poisson spike train",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_25",
    "chunk_position": 25
  },
  {
    "id": "2303.10780v2",
    "text": ". If the input stimuli consists of an image, each pixels intensity could be converted to the ring rate of a Poisson spike train. Rate coding has been experimentally discovered in the visual cortex as well as the motor cortex ; however, rate coding is not the only method through which neural systems encode information (Olshausen Field, 2006), since it requires long processing periods and therefore has a poor rate of information transmission. Latency coding (TTFS)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_26",
    "chunk_position": 26
  },
  {
    "id": "2303.10780v2",
    "text": ". Latency coding (TTFS). An increasingly popular method of information-encoding is the time-to-rst-spike (TTFS) model, which only requires a single spike from each neuron in a group to fully encode stimuli. Thus, information can be encoded in a binary fashion over time. Specically, TTFSencoding negatively correlates stimulus intensity with the time that a neuron takes to spike once, and hence is also known as latency-coding",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_27",
    "chunk_position": 27
  },
  {
    "id": "2303.10780v2",
    "text": ". The biological plausibility of TTFS-like neural coding has been veried by the dependence on the rst spike in systems such as the retina, much of the auditory system, and many responses to tactile stimulus that require fast reactions . 4. An illustration by Eshraghian et al. (2021) showing three common methods for encoding real-valued data as spike trains. Unlike rateand latency-coding, delta modulation is primarily used only at the sensor-level to represent time-varying stimuli. Neural coding comparisons",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_28",
    "chunk_position": 28
  },
  {
    "id": "2303.10780v2",
    "text": ". Neural coding comparisons. While both rateand latencyencoding are believed to be used in biological neural computation (Olshausen Field, 2006), SNN practitioners need only be concerned with the advantages of each method: rate coding has a higher error tolerance and provides more information with which gradient learning can converge, while latency coding is signicantly more power-efcient and fast than rate coding (since each neuron only spikes once per input)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_29",
    "chunk_position": 29
  },
  {
    "id": "2303.10780v2",
    "text": ". The choice of optimal coding scheme could depend on whether the dataset is static (e.g., images) or dynamic (e.g., speech or video data). However, multiple works have conrmed that TTFSlatency-codingis the best choice for achieving maximum accuracy while still keeping a low energy consumption (Perez-Nieves Goodman, 2021). 4 depicts the encoding mechanisms of both rateand latency-encoding. Leaky Integrate-and-Fire Modeling",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_30",
    "chunk_position": 30
  },
  {
    "id": "2303.10780v2",
    "text": ". Leaky Integrate-and-Fire Modeling. Spiking neural networks are usually modeled and understood as networks of Leaky Integrate-and-Fire (LIF) neurons whose membrane potentials Vevolve at timestep taccording to some variation of the following dynamics: Vt Vt1 WXt St Vth; (2) where20;1)is a membrane leakage constant, Xis an input (i.e., an external stimuli to the network or the spiking activity from another neuron), Wis a weight matrix, and the binary spiking function St ( 1;if Vt Vth 0;otherwiseis a function of the activation threshold voltage Vt. 3.2",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_31",
    "chunk_position": 31
  },
  {
    "id": "2303.10780v2",
    "text": ". 3.2. Spiking Neural Networks as Convex Optimizers Mancoo et al. (2020) tackle SNN biological plausibility from rst principles. After showing that SNNs solve quadratic optimization problems, the authors tested their SNNs on a multi-dimensional classication problem, and found that SNNs had irregular spike patterns, a balance of excitation and inhibition, robustness to (Gaussian noise) perturbations, and low ring rates",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_32",
    "chunk_position": 32
  },
  {
    "id": "2303.10780v2",
    "text": ". Furthermore, the authors found that SNNs were robust to cell death (since each neuron corresponds to only one piece of the feasible set boundary), have population ring rates that negatively correlate with the number of neurons",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_33",
    "chunk_position": 33
  },
  {
    "id": "2303.10780v2",
    "text": ". The authors begin by representing a generic SNN with N neurons and Kinputs as follows: F Ib; (3) where 2RNrepresents the voltage of each neuron, is the time-constant of membrane leakage, F2RNKis a feedforward matrix projecting the inputs 2RKonto the RNvoltage space, 2RNNrepresents the weights of recurrent connections, 2RNis a vector containing the neural spike trains, and Ib2RNrepresents background currents or noise",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_34",
    "chunk_position": 34
  },
  {
    "id": "2303.10780v2",
    "text": ". Meanwhile, the following is an example of a quadratic optimization problem (or linear if 0) : miny Fx Gy T 2y Tyb Ty ; (4) where y;b2RMare the optimization variable and bias, x2RKis an input to the problem, and the remaining variables F2RNK,G2RNM,T2RN, and 0are A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices the parameters of the optimization problem",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_35",
    "chunk_position": 35
  },
  {
    "id": "2303.10780v2",
    "text": ". Note that the optimization problem can be geometrically interpreted as describing a series of Nlinear inequalities, GT iy FT ix Ti, where the column vectors Giand Fiare thei-th rows of Gand F, respectively. Each inequality constraint divides the solution space RMinto two half-planes, with one side satisfying the inequality (i.e., being feasible) and the other not (i.e., being infeasible). Taking the intersection of the feasible set of each inequality constraint yields the feasible set of the optimization problem, as shown in 5 . 5. An illustration by Mancoo et al",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_36",
    "chunk_position": 36
  },
  {
    "id": "2303.10780v2",
    "text": ". 5. An illustration by Mancoo et al. (2020) of a quadratic optimization problem in R2with an input x2R3. Note how each dimension of the input xdivides the solution space R2into two half-planes, one feasible and one infeasible. One solve the optimization problem described by (4) is gradient descent: y E yyb (5) However, in order to ensure that ydoes not leave the feasible setfyj Fx Gy Tg, a term Dcan be added to ysuch that the new term reects yback into the feasible set whenever ytouches a boundary of the feasible set",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_37",
    "chunk_position": 37
  },
  {
    "id": "2303.10780v2",
    "text": ". Specically, 2RNcan be modeled a vector of delta-function bouncing events, while the matrix of bounce directions D2RMNcan be modeled as always bouncing yorthogonal to the boundary it touched (i.e., DG): yyb D (6) Connecting the quadratic problem formulation to SNNs proceeds by dening the constraint from (4) as VFx Gy T, where V;Trepresent the neurons voltages and voltage thresholds: VF x G y (7) F x Gy Gb GDs (t) (8) VGDs (t) Gb; (9)where the derivation follows from using the denitions of V and yfrom (3) and (6)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_38",
    "chunk_position": 38
  },
  {
    "id": "2303.10780v2",
    "text": ". The new formulation of V is related to (3) in that the inputs x x, the recurrent connections GD, and the background currents or noise Ib GD. The resulting reformulation shows that SNNs solve quadratic ( 0) or linear ( 0) problems. 4. Optimization This the fundamental theory and recent ndings in SNN optimization. Key challenges include performing gradient descent on non-differentiable spiking actions and leveraging the intrinsic characteristics of neuromorphic computation towards more efcient SNN training methods. 4.1. Background Backpropagation Through Time",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_39",
    "chunk_position": 39
  },
  {
    "id": "2303.10780v2",
    "text": ". 4.1. Background Backpropagation Through Time. Two of the most prevalent methods for training SNNs are shadow training and backpropagation through time (BPTT). The former involves training a SNN as an approximation of a pre-trained ANN; although such an approach is benecial for reaching competitive performance in static tasks such as image classication, shadow training will not be the focus of this review due to its inherent inefciency (requiring two networks to be trained) and its under-utilization of the temporal dynamics of SNNs",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_40",
    "chunk_position": 40
  },
  {
    "id": "2303.10780v2",
    "text": ". The latter method (BPTT) stems from recurrent neural network (RNN) optimization, and becomes necessary for SNN training due to each neurons membrane potential acting as a form of memory as it decays over time, unlike the activation of ANN neurons. Following the notation of Eshraghian et al. (2021), the expression for error as a function of SNN weights,L W, takes the following form when using BPTT: L WX t Lt L WX t X st Lt Ws Ws L WX t X st Lt Ws; (12) where Ltand Wsare the loss and weights at their respective timesteps tands",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_41",
    "chunk_position": 41
  },
  {
    "id": "2303.10780v2",
    "text": ". (11) follows from (10) since Wsdirectly determines L while also inuencing Ltfor allt s (referred to as a prior inuence ). (11) then simplies to (12) since a recurrent system constrains the weights Wto be shared along all timesteps (W0 W1 :::W), meaning that Ws W 1. A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices Surrogate Gradients. Unlike ANNs, SNNs use spikes to perform computation",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_42",
    "chunk_position": 42
  },
  {
    "id": "2303.10780v2",
    "text": ". Unlike ANNs, SNNs use spikes to perform computation. Letting Sdenote spiking activity, calculation of the gradient L Wtakes the form of L Smultiplied by additional terms (via the chain rule) that constitute S W. However, since the spiking activity Sas a function of synaptic weights Winvolves a Heaviside step function, the derivative S Wcontains an ill-behaved Dirac-delta function. As a relaxation of the non-smooth spiking nonlinearity, surrogate gradients are used as an alternative to the derivative of the Heaviside step function during backpropagation",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_43",
    "chunk_position": 43
  },
  {
    "id": "2303.10780v2",
    "text": ". The choice of surrogate gradient for SNN optimization is not unique, but recent work has shown that SNNs are robust to the choice of surrogate gradient (Zenke V ogels, 2021). 6. An illustration by Neftci et al. (2019) showing how surrogate gradients can be used to smoothen the loss landscape of SNNs. 4.2. Advancements in Surrogate Gradient Descent Differentiable Spike. Li et al. (2021) have recently brought attention to the nite difference gradients (FGD) as a novel method of computing surrogate gradients",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_44",
    "chunk_position": 44
  },
  {
    "id": "2303.10780v2",
    "text": ". Li et al. (2021) have recently brought attention to the nite difference gradients (FGD) as a novel method of computing surrogate gradients. With loss Land network parameters W, approximation of the true gradient r WLvia the FGD r;w Lis dened as: r;w L1 22 664 ::: 3 775;(13) where w2Rkis the attened vector of W,eiis the standard basis vector with 1 at its i-th index, and is a hyperparameter. As !0,r;w Lbecomes the true gradient r WL. Li et al",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_45",
    "chunk_position": 45
  },
  {
    "id": "2303.10780v2",
    "text": ". As !0,r;w Lbecomes the true gradient r WL. Li et al. (2021) have shown that, although using very small20:1;0:001 successfully approximates the true gradient in articial neural networks, SNN optimization is more sensitive to the magnitude of , since the optimal values ofmust be low enough to faithfully approximate the characteristics of the SNN gradient while being high enough so as to have a smoothening effect on the discontinuous SNN loss landscape. Li et al",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_46",
    "chunk_position": 46
  },
  {
    "id": "2303.10780v2",
    "text": ". Li et al. (2021) then proceed to dene the Differential Spike (Dspike) surrogate gradient with temperature parameter band inputx: Dspike (x;b) 8 :1; ifx1 tan) tan 2tanif0x1 0; ifx0 (14) The authors train each of their SNNs by, in each epoch e, rst computing the FDG r;w Lof the rst SNN layer, then choosing the value of b2fbe1;be1 b;be1 b to maximize the cosine similarity between r;w Landrb;w L, and nally optimizing the entire SNN using Dspike gradient descent with the newly optimized value of b. Information-Maximization and ESG. More recently, Guo et al",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_47",
    "chunk_position": 47
  },
  {
    "id": "2303.10780v2",
    "text": ". Information-Maximization and ESG. More recently, Guo et al. (2022) have introduced Information-Maximization Loss (IM-Loss) and the Evolutionary Surrogate Gradient (ESG) for differentiable spike activity estimation. The authors derive the IM-Loss by rst arguing that one key reason why SNN performance lags behind ANN performance is because of the loss of information produced by quantizing membrane voltages into binary spiking functions",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_48",
    "chunk_position": 48
  },
  {
    "id": "2303.10780v2",
    "text": ". Representing the tensor of membrane potentials as Vand the tensor of spiking activity as S, the authors seek to maximize the mutual information between vand Sby maximizing the entropy of S: arg max V;S ;(15) whereis argued to be 0 due to the binary nature of S. Since each element of Sis either 0 or 1, the entropy of S is maximized when each element of Sis likely to be 0 or 1. Using the observation that Vis often Gaussian distributed and therefore has a median to its mean V, Guo et al. (2022) propose the IM-Loss as: LIM1 LLX l0(Vth)2; (16) where Vthis the spike activation threshold voltage",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_49",
    "chunk_position": 49
  },
  {
    "id": "2303.10780v2",
    "text": ". (2022) propose the IM-Loss as: LIM1 LLX l0(Vth)2; (16) where Vthis the spike activation threshold voltage. The authors nal loss function for SNN training was LTotal LCELIM, where LCEis cross-entropy loss and 2. Similarly to Dspike , Guo et al",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_50",
    "chunk_position": 50
  },
  {
    "id": "2303.10780v2",
    "text": ". (2022) use a differentiable spike activation function (x)whose sole parameterevolves as a function of the epoch index i20;N1: (x) 1 2tan(x Vth)) 1 2(17) 1 9((10i N1)Kmax (1010i N)Kmin)(18) Sincegrows monotonically as a function of the epoch indexi, the Evolutionary Surrogate Gradient (x)aims to A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices improve SNN convergence by beginning as a function with a very wide gradient (i.e., allowing many weights to be updated) and then gradually transitioning to a function that more closely",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_51",
    "chunk_position": 51
  },
  {
    "id": "2303.10780v2",
    "text": "as a function with a very wide gradient (i.e., allowing many weights to be updated) and then gradually transitioning to a function that more closely resembles the true spiking function",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_52",
    "chunk_position": 52
  },
  {
    "id": "2303.10780v2",
    "text": ". 4.3. Leveraging Sparsity Perez-Nieves Goodman (2021) present a novel approximation of surrogate gradient descent that leverages spike train sparsity for remarkable improvements in computational efciency and speed. The authors begin with the assumption that neurons operate according to a simplied Leaky Integrate and Fire model: it ( it1;xt);if it Vth Vr; if it Vt Frepresents the dynamics governing the voltage iof thei-th neuron of layer l,Vthis a voltage threshold, and Vr is the resting potential of the neuron after spiking",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_53",
    "chunk_position": 53
  },
  {
    "id": "2303.10780v2",
    "text": ". At each time, neurons spiking according to it it), where2f0;1gequals 1if and only if v Vth. The authors use gradient descent via backpropagation through time (BPTT) with the Super Spike surrogate gradient 1 ( j VVthj1)2(Zenke Ganguli, 2018; Zenke V ogels, 2021)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_54",
    "chunk_position": 54
  },
  {
    "id": "2303.10780v2",
    "text": ". In order to make gradient optimization more efcient, the authors introduce an approximation of the surrogate gradient: d it d i it);if neuronis active 0; otherwis where thei-th neuron of layer lis dened as active if and only if j i Vthj Bth (21) By approximating the surrogate gradient as being nonzero only for neurons that are active at any timestep t, the resulting derivation of BPTT yields an optimization strategy that approximates weight updates very well, having a backward pass speedup of up to 150x and saving up to 85 more memory compared to vanilla BPTT on datasets such as",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_55",
    "chunk_position": 55
  },
  {
    "id": "2303.10780v2",
    "text": "weight updates very well, having a backward pass speedup of up to 150x and saving up to 85 more memory compared to vanilla BPTT on datasets such as Fashion-MNIST (processed via latent coding), Neuromorphic-MNIST , and Spiking Heidelberg Digits (PerezNieves Goodman, 2021)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_56",
    "chunk_position": 56
  },
  {
    "id": "2303.10780v2",
    "text": ". 4.4. Learning Rules One of the most common learning rules for training SNNs is backpropagation modeled as the reverse of a simple forward pass. However, backpropagation methods, such as backpropagation through time (BPTT), are not biologically plausible (Hunsberger, Eric, 2018), are memory-intensive, and are not fully compatible with neuromorphic hardwar, instead necessitating computationally intensive training on normal hardware. This recent extensions or alternatives to backpropagation. Spike-Timing Dependent Plasticity",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_57",
    "chunk_position": 57
  },
  {
    "id": "2303.10780v2",
    "text": ". This recent extensions or alternatives to backpropagation. Spike-Timing Dependent Plasticity. Another alternative to BPTT that focuses on spike timing is spike-timing dependent plasticity (STDP), a learning framework whose origins are not in optimization, but rather in experimental observations of neural plasticity. Unlike event-driven optimization, STDP is unsupervised and does not use gradient descent or backpropagation, instead determining the synaptic connection strength between each neuron solely as a function of the relative timings between preand post-synaptic neuronal spikes",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_58",
    "chunk_position": 58
  },
  {
    "id": "2303.10780v2",
    "text": ". Such learning rules are commonly known as Hebbian learning methods (Caporale Dan, 2008) and are characterized by the principle that neurons that re together wire together . wi;iftjti0 awi;iftjti0(22) One example of STDP learning rules for SNNs is shown in (22), where aandaare learning rates, ti andtjare the postand pre-synaptic spike times of two neurons, and wijis the strength of connection between the two neurons",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_59",
    "chunk_position": 59
  },
  {
    "id": "2303.10780v2",
    "text": ". A recent extension of this STDP is R-STDP, or reward-modulated STDP, where rewardpunishment signals are used in conjunction with traditional STDP to train the weights (Rathi Roy, 2021b). Event-Driven Optimization. One potential alternative to traditional backpropagation through time for SNNs is eventdriven optimization, specically backpropagation with respect to spike timing . Such optimization methods involve calculating time-based gradients that indicate whether a spike should happen earlier or later with respect to time. Zhu et al",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_60",
    "chunk_position": 60
  },
  {
    "id": "2303.10780v2",
    "text": ". Zhu et al. (2022) have recently been the rst to successfully train SNNs on the CIFAR-100 dataset using event-driven methods. Online Training Through Time. Xiao et al. (2022) recently proposed another alternative to traditional backpropagation through time. Their method, referred to as online training through time (OTTT), diverges from traditional SNN optimization by not using a surrogate gradient",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_61",
    "chunk_position": 61
  },
  {
    "id": "2303.10780v2",
    "text": ". Their method, referred to as online training through time (OTTT), diverges from traditional SNN optimization by not using a surrogate gradient. Instead, the authors leverage the fact that the gradient of spiking activity is naturally zero almost everywhere to approximate t1 t1t1 t0, where tandtare the voltage and spiking activity of layer lat timet. As a result, each layers voltage evolves simply as a function of its membrane leakage constant 1:t1 t I. As a result, Xiao et al",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_62",
    "chunk_position": 62
  },
  {
    "id": "2303.10780v2",
    "text": ". As a result, each layers voltage evolves simply as a function of its membrane leakage constant 1:t1 t I. As a result, Xiao et al. (2022) are able to use a loss function and a gradient whose values are instantaneous (i.e., only depending on the timestep t). Further analysis formulates A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices OTTT as a three-factor Hebbian learning rule, thus building a pathway towards biologically plausible training on neuromorphic devices. Implicit Differentiation",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_63",
    "chunk_position": 63
  },
  {
    "id": "2303.10780v2",
    "text": ". Implicit Differentiation. As another alternative to feedforward SNNs trained via BPTT, Xiao et al. (2021) bring to attention feedback spiking neural networks (FSNNs), which are SNNs with recurrent connections, and propose implicit differentiation on the state (IDE) as a method to train them. The authors dene the state aof a network fwith parameters as the state where a. Implicit differentiation on the (I a)da d",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_64",
    "chunk_position": 64
  },
  {
    "id": "2303.10780v2",
    "text": ". The authors dene the state aof a network fwith parameters as the state where a. Implicit differentiation on the (I a)da d . Writing the objective function evaluated at aas, and letting a, differentiation ofwith respect to the parameters can be expressed as: a J1 gja ; (23) where J1 gja is the inverse Jacobian of gevaluated at a. Solving the inverse Jacobian requires solving an alternative linear system JT gja x a T 0, which can be done using second-order quasi-Newton methods or by using a xed-point update scheme x JT fja x a T , since JT gja JT fja",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_65",
    "chunk_position": 65
  },
  {
    "id": "2303.10780v2",
    "text": ". Updating the parameters can then occur via gradient descent based on L . 5. Energy Efciency This on how SNNs garner impressive energy savings, covering the mathematical underpinnings as well as documenting notable gains in memory and complexity reduction. 5.1",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_66",
    "chunk_position": 66
  },
  {
    "id": "2303.10780v2",
    "text": ". 5.1. Low-Power Applications and Constraints SNNs hold particular promise for low-power and implantable applications, such as brain-machine interfaces (BMIs), where there exists a strict power requirement: chips directly in contact with brain tissue cannot emit more than 10 m W of energy before permanently damaging brain cells by heating them by over 1 degree Celsius . SNNs promise to reduce energy consumption and thus heat output, making them a promising choice for implants . However, one can only benet from SNNs when running on hardware built to take advantage of the inherent benets of SNNs",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_67",
    "chunk_position": 67
  },
  {
    "id": "2303.10780v2",
    "text": ". However, one can only benet from SNNs when running on hardware built to take advantage of the inherent benets of SNNs. such as their sparsity and inclination for parallel computation. Neuromorphic processors deviate from the traditional von Neumann architecture of todays general purpose CPUs by employing massive parallelism, asynchronous event-driven operations, and a more highly-distributed, easily-accessible memorythat reduces the number of computationsand thus, the energy consumptionrequired for information processing . As found by Dethier et al",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_68",
    "chunk_position": 68
  },
  {
    "id": "2303.10780v2",
    "text": ". As found by Dethier et al. (2011), neuromoprhic hardware implementations can result in SNNs which consume as little as 0.1 m W, well below the aforementioned maximum power threshold of 10 m W for BMIs. 5.2. Mechanisms for Energy Savings Reduced Activity. Energy consumption has been shown to be reduced by up to 3 orders of magnitude when compared to conventional ANN implementations, largely due to spiking activity, sparsity, and static data suppression (eventdriven processing)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_69",
    "chunk_position": 69
  },
  {
    "id": "2303.10780v2",
    "text": ". Neurons producing discrete spikes in sparsely-connected networks result in spatio-temporally sparse activity, where activity can be dened as the following: A100a BTN; (24) whereais the number of active neurons, Bis the batch size,Tis the total number of time steps, and Nis the number of neurons in the layer (Perez-Nieves Goodman, 2021). As one would intuitively guess, the level of activity is inversely proportional to the sparsity of the inputs: as more neurons become active such that the fraction of neurons ring,a N, increases, the network is by denition less sparse",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_70",
    "chunk_position": 70
  },
  {
    "id": "2303.10780v2",
    "text": ". The energy-efciency of spike-based computation becomes especially apparent when considering that only a fraction of neurons in real brains use rate-coding, whereas all others opt for much sparser representations of activity (Olshausen Field, 2006). Estimating Computational Expenditure. Computational energy for neural networks is typically assessed in two ways: the number and type of operations performed, and the frequency of memory access. For the former, many papers have quantied the energy cost of AC (accumulation) and MAC (multiply and accumulate) operations for various CMOS processors",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_71",
    "chunk_position": 71
  },
  {
    "id": "2303.10780v2",
    "text": ". In ANNs, the summation of the weighted inputs to a neuron require a MAC operation (e.g. one MAC operation per input), whereas for SNNs, spikes rely on the much cheaper AC operation per input. For comparison, one conservative estimate is that three AC operations to one MAC operation . Note that SNNs need to update their membrane potentials every time step, accounting for an additional MAC operation per each neuron, as opposed to ANNs where there is a MAC operation per input to each neuron. Memory Bottleneck",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_72",
    "chunk_position": 72
  },
  {
    "id": "2303.10780v2",
    "text": ". As for memory, the continual increase of neural network parameter complexity produces an increasing difculty of storing everything on-chip: when accessing memory that is further away (for example, there is a substantial jump in delay for accessing larger memory bases; instead of requiring data from the CPUs cache, havA Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices ing to read from RAM, or the SSD), the time and energy required grows, thus presenting a key bottleneck in the implementation of all neural network architectures",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_73",
    "chunk_position": 73
  },
  {
    "id": "2303.10780v2",
    "text": ". Many popular learning methods in todays SNNs (e.g., BPTT variants) are biologically implausible, since they require access to past states to use as inputs (i.e., data must be pulled from memory): thus, the memory consumed scales with time, limiting BPTT to small temporal dependencies . Another example of this hardware bottleneck is that, in dense back propagation situations, there can only be a few thousand time steps, as gradients must be computed at each of those time steps",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_74",
    "chunk_position": 74
  },
  {
    "id": "2303.10780v2",
    "text": ". Notably, many of these gradients have little effect on the output; Perez-Nieves Goodman (2021) present a backpropagation technique that only computes the gradients of active neurons (i.e., only calculates the gradients that meaningfully affect the output), which facilitates training on GPUs, whereas traditional BPTT runs out of memory before it can converge. As Perez-Nieves Goodman (2021) point out, this effectively means that such SNN implementations can be trained with more data for a longer amount of time when compared to previous generations of SNNs, given the same computing resources",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_75",
    "chunk_position": 75
  },
  {
    "id": "2303.10780v2",
    "text": ". 5.3. Observed Energy Efciency Gains An observed issue with reporting the energy efciency or decrease in memoryenergy consumption by SNNs throughout the literature was that there were no standardized measurements for quantifying the energy savings offered by SNNs",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_76",
    "chunk_position": 76
  },
  {
    "id": "2303.10780v2",
    "text": ". A number of works reported the number of estimated MAC and ACadd operations that their implementation required, frequently contrasted against the number of said operations required for ANN ; while this method is simpler to estimate on the researchers behalf, it remains unstandardized, as different researchers estimate the number of operations in different ways, and the number of operations is fundamentally a proxy for true energy consumption",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_77",
    "chunk_position": 77
  },
  {
    "id": "2303.10780v2",
    "text": ". As for the works that do report the expected energy consumption, different teams make different assumptions about how many AC operations are to one MAC operation, and thus introduce a wide range of reported energy savings. We recommend that researchers take the conservative approach, using the methodology suggested by Horowitz (2014) of using 3 additions as the to 1 MAC operation. Note that while this is of course a proxy for actual energy consumption, the amount of energy consumed is also dependent on the actual hardware used to train and deploy the model. HIRE-SNN",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_78",
    "chunk_position": 78
  },
  {
    "id": "2303.10780v2",
    "text": ". HIRE-SNN. HIRE-SNN VGG11 (a directSNN) compared against a rateSNN and an ANN achieved 4.6x and 10x less energy consumption, respectively .Sparse Spiking Gradient Descent. Perez-Nieves Goodman (2021) introduced a method that speeds up backward execution by up to two orders of magnitude, and thus they expected that this method would likewise reduce the energy consumption by a similar factor. By constraining back propagation to active neurons, 98 of the neurons (i.e., 98 of the gradient calculations) can be skipped",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_79",
    "chunk_position": 79
  },
  {
    "id": "2303.10780v2",
    "text": ". By constraining back propagation to active neurons, 98 of the neurons (i.e., 98 of the gradient calculations) can be skipped. Such accelerations yield much faster back propagation times, lower memory requirements, and less energy consumption on GPU hardware while not affecting test accuracy. These laudable improvements are possible because the surrogate gradient used to approximate the derivative of the spiking function is larger when the membrane potential is closer to the threshold, resulting in most of the gradient being concentrated on active neurons (Perez-Nieves Goodman, 2021)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_80",
    "chunk_position": 80
  },
  {
    "id": "2303.10780v2",
    "text": ". SNN Brain-Machine Interface. Liao et al. (2022) compared ANNs and SNNs for an implantable BMI that decodes nger velocity. They found that an ANN required 529K total operations and 2116K memory accesses, and an SNN trained to approximate the ANN similarly required 293K total operations and 2615K memory accesses. However, the authors proposed SNN required just 36K total operations and only 199K memory accesses. Their proposed SNN produced reductions of 95 and 90 in the number of total operations required, and 91 and 93 in the number of memory accesses, respectively . 6",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_81",
    "chunk_position": 81
  },
  {
    "id": "2303.10780v2",
    "text": ". 6. Best Practices This to present the best practices in SNN training and evaluation: widely used benchmarks, baseline standards, architectures, and frameworks. 6.1. Benchmarks Datasets Background. Throughout the development of ANNs, certain datasetssuch as the MNIST (Deng, 2012) and CIFAR datasets (Krizhevsky Hinton, 2009)have served as reliable benchmarks for all researchers to compare against",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_82",
    "chunk_position": 82
  },
  {
    "id": "2303.10780v2",
    "text": ". The development of SNNs has come as a subset of the growing neuromorphic engineering landscape, wherein advances made in software and hardware must be accompanied by advances in benchmarks such as standardized neuromorphic datasets. A key prerequisite to SNN implementation is to encode the input data as spikes: thus, event-based sensors (such as the ATIS vision sensors) that record pixellevel spikeof the data in the form of inherently neuromorphic datasets have gained traction, as have software conversions of traditional datasets into neuromorphic form",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_83",
    "chunk_position": 83
  },
  {
    "id": "2303.10780v2",
    "text": ". Note that, as discussed earlier, the precise mechanism through which to encode data as binary spike trains is not unique, with the primary two modes of doing so being rate-coding and latency-coding. A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices Neuromorphic Datasets. Many imaging datasets now have neuromorphic counterparts, wherein the changes in intensity for each pixel (measured from 3 vantage points via sensors) are represented as spikes occurring at the given pixel locations",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_84",
    "chunk_position": 84
  },
  {
    "id": "2303.10780v2",
    "text": ". The most common neuromorphic datasets in the literature include the N-MNIST dataset (spiking of the classic MNIST dataset), DVS Gestures (video data of hand gestures recorded under different lighting conditions), and SHD (spiking of the Heidelberg Digits in the form of audio data converted via a simulated cochlear model). A number of other neuromorphic visionimaging datasets exist, such as ASL-DVS, DA VIS, MVSEC, POKER DVS, and DSEC, and other common audio datasets simulating cochlear activity include SSC and N-TIDIGITS . Non-Neuromorphic Datasets",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_85",
    "chunk_position": 85
  },
  {
    "id": "2303.10780v2",
    "text": ". Non-Neuromorphic Datasets. Despite the proliferation of neuromorphic datasets, they remain niche and unstandardized, indicating that many practitioners prefer to convert non-neuromorphic datasets to spiking inputs themselves. That said, there are a number of commonly-used datasets, particularly the MNIST and CIFAR-10 datasets. Of the total benchmarks reviewed, 26.6 of all models were compared using the MNIST dataset (33.1 when including MNIST variants such as N-MNIST and F-MNIST), and CIFAR-10 was the second most common with 30.5 of models being benchmarked against it",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_86",
    "chunk_position": 86
  },
  {
    "id": "2303.10780v2",
    "text": ". Notably, while more papers choose to use MNIST, the papers reviewed that used CIFAR tended to have many more models per paper. It is important to acknowledge that many SNN implementations utilized in neuroengineering applications are trained and deployed directly onto private collected data (for instance, EEG data collected from an implant within a rhesus monkey) as opposed to a standardized benchmark, and thus these were not included in the prior calculations. Recommendations",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_87",
    "chunk_position": 87
  },
  {
    "id": "2303.10780v2",
    "text": ". Recommendations. We recommend that aspiring researchers use either the MNIST dataset or CIFAR-10, as they remain the most common and thus have the benchmarking network effect. However, in a further effort to standardize results across the eld, we also recommend using the publicly available N-MNIST or DVS Gestures datasets, as these would help ensure repeatability since these latter two datasets are already converted to a neuromorphic input form. Baselines",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_88",
    "chunk_position": 88
  },
  {
    "id": "2303.10780v2",
    "text": ". Baselines. One primary result of our literature review is a compilation of some of the latest state of the art baselines in SNN optimization, which we have compiled in . 6.2. Architectures and Frameworks Software. There exists a plethora of development options for implementing SNNs, each with subtle differences, but many attempting to ll the same niche. Common frameworks include Binds NET, Norse, Nengo, Spyke Torch, and SNNTool Box. Binds NET is built on top of Py Torch as is focused on simulating SNNs for general machine learning and reinforcement learning purposes",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_89",
    "chunk_position": 89
  },
  {
    "id": "2303.10780v2",
    "text": ". Binds NET is built on top of Py Torch as is focused on simulating SNNs for general machine learning and reinforcement learning purposes . Norse similarly expands Py Torch with SNNs (El-Allami et al., 2020). Nengo is built on top of Keras Tensor Flow, and has specic modules for deep learning and simulation backends (e.g. on Open CL, FPGAs, Loihi) (Rasmussen, 2019). Spyke Torch is also built from Py Torch, and is focused on simulating convolutional SNNs with one spike per neuron, rank-order information encoding, and learning via STDP or R-STDP",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_90",
    "chunk_position": 90
  },
  {
    "id": "2303.10780v2",
    "text": ". Finally, SNNTool Box (SSN-TB) is a framework solely focused on transforming traditional ANNs to SNNs and supports a number of different encoding schemes: SNN-TB can be used with models from Keras TF, Py Torch, and other popular deep learning libraries, and provides an interface for both simulation backends (py NN, brian2, etc.) and deployment (via Spi NNaker or Loihi)",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_91",
    "chunk_position": 91
  },
  {
    "id": "2303.10780v2",
    "text": ". Many modern machine learning frameworks do not offer ways to accelerate the convolution of binary spike activation, and furthermore struggle to implement custom continuous-time differential expressions given the inherent discrete execution of their built-in operations . Hardware. Another important aspect of SNN implementation is the hardware itself: when implemented on non-neuromorphic hardware (i.e., traditional von-Neumann architectures), the benets of SNNs are less pronounced since the hardware is physically incapable of taking advantage of the massive parallelism offered by SNNs",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_92",
    "chunk_position": 92
  },
  {
    "id": "2303.10780v2",
    "text": ". By far, the most common hardware for SNNs is the Loihi chip by Intel (whose successor, the Loihi 2, was recently released in late 2021). The Loihi is a neuromorphic many-core processor that supports on-chip learning, making it a viable option for both inference and deployment. The Loihi has 128 cores, simulates up to 131,072 neurons with 130,000,000 synapses possible; furthermore, each Loihi can be put in parallel with up to 16,384 other chips, reaping the benets of massive parallelism by allowing the number of effective on-chip cores to be 4,096",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_93",
    "chunk_position": 93
  },
  {
    "id": "2303.10780v2",
    "text": ". Another common implementation tool for SNNs is Spi NNaker, a million-core, open-access, ARM-based neuromorphic computing platform developed and housed in the University of Machester: Spi NNaker is used for simulation and testing for applications that do not require on-site implementations . Architectural Techniques",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_94",
    "chunk_position": 94
  },
  {
    "id": "2303.10780v2",
    "text": ". Architectural Techniques. The nal, but just as important, subsection of methodologies required to enable SNNs to reach their potential as universal approximators (Zhang Zhou, 2022) consists of how practitioners can best structure, initialize, or normalize the connections between spiking layers to maximize performance. As a prominent example, Fang et al",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_95",
    "chunk_position": 95
  },
  {
    "id": "2303.10780v2",
    "text": ". As a prominent example, Fang et al. (2021) have recently developed a spike-elementA Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices wise (SEW) residual network that faithfully implements key properties of ANN residual networks in SNNs: namely, the ability to learn an identity function and the ability to train very deep SNN architectures (over 100 layers) by virtue of mitigating vanishingexploding gradient problems",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_96",
    "chunk_position": 96
  },
  {
    "id": "2303.10780v2",
    "text": ". New work in SNN initialization has extended weight initialization methods that target a specic variance for neuronal activitysuch as Glorot and He initialization (Glorot Bengio, 2010; He et al., 2015)to spiking networks. Specically, Rossbroich et al. (2022) present a new SNN initialization formula that surpasses He initialization for deep SNNs (i.e., more than 7 layers) by simply targeting a specic membrane potential variance",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_97",
    "chunk_position": 97
  },
  {
    "id": "2303.10780v2",
    "text": ". As for SNN normalization, one of the more fundamental works is threshold-dependent batch normalization (td BN) , which extends traditional batch normalization (BN) (Ioffe Szegedy, 2015) to the additional temporal dimension brought by SNNs and makes the normalized pre-activation variance dependent on the ring threshold Vth. Newer developments include batch normalization through time (BNTT) (Kim Panda, 2021) and temporal effective batch normalization (TEBN) , which both aim to mitigate temporal co-variate shift of spiking activity. 7",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_98",
    "chunk_position": 98
  },
  {
    "id": "2303.10780v2",
    "text": ". 7. Conclusion Our review has covered the fundamental principles and the cutting-edge methods for reaching competitive performance in spiking neural networks (SNNs) while also improving SNN memory and energy efciency. This literature search has identied the most common benchmarks for aspiring SNN practitioners to use, the current state of the art performances on those benchmarks, and the trajectories of recent advances in SNN optimization",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_99",
    "chunk_position": 99
  },
  {
    "id": "2303.10780v2",
    "text": ". SNNs offer an exciting new technology, particularly for low-power, implanted, mobile, or other hardware-constrained applications, such as brain-machine interfaces and wearable technologies. Our understanding of SNNs as the third generation of neural networks continues to grow; with such growth comes not only the potential to improve neural network performance while reducing energy costs, but also the potential to further develop our understanding of neuroscience and neural computation. Acknowledgements We thank Anastasios Kyrillidis for his guidance and support during the writing process",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_100",
    "chunk_position": 100
  },
  {
    "id": "2303.10780v2",
    "text": ". Acknowledgements We thank Anastasios Kyrillidis for his guidance and support during the writing process. References Barnett, M. W. and Larkman, P. M. The action potential. Practical neurology , 7(3):192197, 2007.Bohte, S. M., Kok, J. N., and La Poutre, H. Errorbackpropagation in temporally encoded networks of spiking neurons. Neurocomputing , 48(1-4):1737, 2002. Boyd, S., Boyd, S. P., and Vandenberghe, L. Convex optimization . 2004. Caporale, N. and Dan, Y . Spike timing-dependent plasticity: a hebbian learning rule. Annual review of neuroscience , 31(1):2546, 2008",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_101",
    "chunk_position": 101
  },
  {
    "id": "2303.10780v2",
    "text": ". 2004. Caporale, N. and Dan, Y . Spike timing-dependent plasticity: a hebbian learning rule. Annual review of neuroscience , 31(1):2546, 2008. Ceolini, E., Frenkel, C., Shrestha, S. B., Taverni, G., Khacef, L., Payvand, M., and Donati, E. Hand-gesture recognition based on emg and event-based camera sensor fusion: A benchmark in neuromorphic computing. Frontiers in Neuroscience , 14, 2020. 10.3389fnins.2020.00637. Cramer, B., Stradmann, Y ., Schemmel, J., and Zenke, F. The Heidelberg spiking data sets for the systematic evaluation of spiking neural networks",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_102",
    "chunk_position": 102
  },
  {
    "id": "2303.10780v2",
    "text": ". Cramer, B., Stradmann, Y ., Schemmel, J., and Zenke, F. The Heidelberg spiking data sets for the systematic evaluation of spiking neural networks. IEEE Transactions on Neural Networks and Learning Systems , 33(7):27442757, 2022. 10.1109tnnls.2020.3044364. Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y ., Choday, S. H., Dimou, G., Joshi, P., Imam, N., Jain, S., Liao, Y ., Lin, C.-K., Lines, A., Liu, R., Mathaikutty, D., Mc Coy, S., Paul, A., Tse, J., Venkataramanan, G., Weng, Y .-H., Wild, A., Yang, Y ., and Wang, H. Loihi: A neuromorphic manycore processor with on-chip learning",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_103",
    "chunk_position": 103
  },
  {
    "id": "2303.10780v2",
    "text": ". Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro , 38(1):8299, 2018. 10.1109MM.2018. 112130359. Davies, M., Wild, A., Orchard, G., Sandamirskaya, Y ., Guerra, G. A. F., Joshi, P., Plank, P., and Risbud, S. R. Advancing neuromorphic computing with loihi: A survey of results and outlook. Proceedings of the IEEE , 109(5): 911934, 2021. 10.1109JPROC.2021.3067593. Deng, L. The mnist database of handwritten digit images for machine learning research. IEEE signal processing magazine , 29(6), 2012",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_104",
    "chunk_position": 104
  },
  {
    "id": "2303.10780v2",
    "text": ". Deng, L. The mnist database of handwritten digit images for machine learning research. IEEE signal processing magazine , 29(6), 2012. Dethier, J., Nuyujukian, P., Eliasmith, C., Stewart, T., Elasaad, S., Shenoy, K. V ., and Boahen, K. A. A brainmachine interface operating with a real-time spiking neural network control algorithm. In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K. (eds.), Advances in Neural Information Processing Systems , volume 24. Curran Associates, Inc., 2011. Diehl, P. U. and Cook, M",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_105",
    "chunk_position": 105
  },
  {
    "id": "2303.10780v2",
    "text": ". (eds.), Advances in Neural Information Processing Systems , volume 24. Curran Associates, Inc., 2011. Diehl, P. U. and Cook, M. Unsupervised learning of digit recognition using spike-timing-dependent plasticity. Frontiers in Computational Neuroscience , 9, 2015. Duan, C., Ding, J., Chen, S., Yu, Z., and Huang, T. Temporal effective batch normalization in spiking neural networks. In Advances in Neural Information Processing Systems , 2022",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_106",
    "chunk_position": 106
  },
  {
    "id": "2303.10780v2",
    "text": ". Temporal effective batch normalization in spiking neural networks. In Advances in Neural Information Processing Systems , 2022. A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices El-Allami, R., Marchisio, A., Shaque, M., and Alouani, I. Securing deep spiking neural networks against adversarial attacks through inherent structural parameters. Co RR , abs2012.05321, 2020. Eshraghian, J. K., Ward, M., Neftci, E., Wang, X., Lenz, G., Dwivedi, G., Bennamoun, M., Jeong, D. S., and Lu, W. D",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_107",
    "chunk_position": 107
  },
  {
    "id": "2303.10780v2",
    "text": ". Co RR , abs2012.05321, 2020. Eshraghian, J. K., Ward, M., Neftci, E., Wang, X., Lenz, G., Dwivedi, G., Bennamoun, M., Jeong, D. S., and Lu, W. D. Training spiking neural networks using lessons from deep learning, 2021. Fang, W., Yu, Z., Chen, Y ., Huang, T., Masquelier, T., and Tian, Y . Deep residual learning in spiking neural networks. In Advances in Neural Information Processing Systems , volume 34, 2021. Furber, S. B., Galluppi, F., Temple, S., and Plana, L. A. The spinnaker project. Proceedings of the IEEE , 102(5): 652665, 2014. 10.1109JPROC.2014.2304638. Glorot, X. and Bengio, Y",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_108",
    "chunk_position": 108
  },
  {
    "id": "2303.10780v2",
    "text": ". A. The spinnaker project. Proceedings of the IEEE , 102(5): 652665, 2014. 10.1109JPROC.2014.2304638. Glorot, X. and Bengio, Y . Understanding the difculty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Articial Intelligence and Statistics , volume 9 of Proceedings of Machine Learning Research , 2010. Guo, W., Fouda, M. E., Eltawil, A. M., and Salama, K. N. Neural coding in spiking neural networks: A comparative study for robust neuromorphic systems. Frontiers in Neuroscience , 15, 2021. 10.3389fnins.2021.638474",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_109",
    "chunk_position": 109
  },
  {
    "id": "2303.10780v2",
    "text": ". Frontiers in Neuroscience , 15, 2021. 10.3389fnins.2021.638474. Guo, Y ., Chen, Y ., Zhang, L., Liu, X., Wang, Y ., Huang, X., and Ma, Z. Im-loss: Information maximization loss for spiking neural networks. In Advances in Neural Information Processing Systems , 2022. Hazan, H., Saunders, D. J., Khan, H., Patel, D., Sanghavi, D. T., Siegelmann, H. T., and Kozma, R. Bindsnet: A machine learning-oriented spiking neural networks library in python. Frontiers in Neuroinformatics , 12, 2018. 10.3389fninf.2018.00089. He, K., Zhang, X., Ren, S., and Sun, J",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_110",
    "chunk_position": 110
  },
  {
    "id": "2303.10780v2",
    "text": ". Frontiers in Neuroinformatics , 12, 2018. 10.3389fninf.2018.00089. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiers: Surpassing human-level performance on imagenet classication. In 2015 IEEE International Conference on Computer Vision (ICCV) , 2015. 10.1109ICCV .2015.123. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2016. Hodgkin, A. L. and Huxley, A. F",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_111",
    "chunk_position": 111
  },
  {
    "id": "2303.10780v2",
    "text": ". In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2016. Hodgkin, A. L. and Huxley, A. F. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of physiology , 117 (4):500, 1952.Horowitz, M. 1.1 computings energy problem (and what we can do about it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC) , 1014, 2014. 10.1109ISSCC.2014.6757323. Hunsberger, Eric. Spiking Deep Neural Networks: Engineered and Biological Approaches to Object Recognition",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_112",
    "chunk_position": 112
  },
  {
    "id": "2303.10780v2",
    "text": ". 10.1109ISSCC.2014.6757323. Hunsberger, Eric. Spiking Deep Neural Networks: Engineered and Biological Approaches to Object Recognition . Ph D thesis, 2018. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning , 2015. Iyer, L. R., Chua, Y ., and Li, H. Is neuromorphic mnist neuromorphic? analyzing the discriminative power of neuromorphic datasets in the time domain. Frontiers in Neuroscience , 15, 2021. 10.3389fnins.2021.608567. Kaiser, J., Mostafa, H., and Neftci, E",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_113",
    "chunk_position": 113
  },
  {
    "id": "2303.10780v2",
    "text": ". Frontiers in Neuroscience , 15, 2021. 10.3389fnins.2021.608567. Kaiser, J., Mostafa, H., and Neftci, E. Synaptic plasticity dynamics for deep continuous local learning (decolle). Frontiers in Neuroscience , 14, 2020. 10.3389fnins. 2020.00424. Kheradpisheh, S. R., Ganjtabesh, M., Thorpe, S. J., and Masquelier, T. STDP-based spiking deep convolutional neural networks for object recognition. Neural Networks , 2018. Kim, Y . and Panda, P. Revisiting batch normalization for training low-latency deep spiking neural networks from scratch. Frontiers in neuroscience , 2021. Krizhevsky, A",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_114",
    "chunk_position": 114
  },
  {
    "id": "2303.10780v2",
    "text": ". Revisiting batch normalization for training low-latency deep spiking neural networks from scratch. Frontiers in neuroscience , 2021. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. 2009. Kulkarni, S. R., Alexiades, J. M., and Rajendran, B. Learning and real-time classication of hand-written digits with spiking neural networks. 2017 24th IEEE International Conference on Electronics, Circuits and Systems , 128 131, 2017. Kundu, S., Pedram, M., and Beerel, P. A",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_115",
    "chunk_position": 115
  },
  {
    "id": "2303.10780v2",
    "text": ". 2017 24th IEEE International Conference on Electronics, Circuits and Systems , 128 131, 2017. Kundu, S., Pedram, M., and Beerel, P. A. Hire-snn: Harnessing the inherent robustness of energy-efcient deep spiking neural networks by training with crafted input noise. In Proceedings of the IEEECVF International Conference on Computer Vision , 52095218, 2021. Li, Y ., Guo, Y ., Zhang, S., Deng, S., Hai, Y ., and Gu, S. Differentiable spike: Rethinking gradient-descent for training spiking neural networks. In Advances in Neural Information Processing Systems , volume 34, 2342623439, 2021",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_116",
    "chunk_position": 116
  },
  {
    "id": "2303.10780v2",
    "text": ". In Advances in Neural Information Processing Systems , volume 34, 2342623439, 2021. Li, Y ., Guo, Y ., Zhang, S., Deng, S., Hai, Y ., and Gu, S. Differentiable spike: Rethinking gradient-descent for training spiking neural networks. In Advances in Neural Information Processing Systems , 2022. A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices Liao, J., Widmer, L., Wang, X., Mauro, A. D., NasonTomaszewski, S. R., Chestek, C. A., Benini, L., and Jang, T",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_117",
    "chunk_position": 117
  },
  {
    "id": "2303.10780v2",
    "text": ". D., NasonTomaszewski, S. R., Chestek, C. A., Benini, L., and Jang, T. An energy-efcient spiking neural network for nger velocity decoding for implantable brain-machine interface. In 2022 IEEE 4th International Conference on Articial Intelligence Circuits and Systems , 2022. 10.1109aicas54282.2022.9869846. Liu, D. and Yue, S. Fast unsupervised learning for visual pattern recognition using spike timing dependent plasticity. Neurocomputing , 249:212224, 2017. Luo, C.-h. and Rudy, Y . A model of the ventricular cardiac action potential. depolarization, repolarization, and their interaction",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_118",
    "chunk_position": 118
  },
  {
    "id": "2303.10780v2",
    "text": ". Luo, C.-h. and Rudy, Y . A model of the ventricular cardiac action potential. depolarization, repolarization, and their interaction. Circulation research , 68(6):15011526, 1991. Mancoo, A., Keemink, S., and Machens, C. K. Understanding spiking networks through convex optimization. Advances in Neural Information Processing Systems , 33: 88248835, 2020. Mozafari, M., Ganjtabesh, M., Nowzari-Dalini, A., and Masquelier, T. Spyketorch: Efcient simulation of convolutional spiking neural networks with at most one spike per neuron. Frontiers in Neuroscience , 13, 2019. 10.3389fnins.2019.00625",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_119",
    "chunk_position": 119
  },
  {
    "id": "2303.10780v2",
    "text": ". Frontiers in Neuroscience , 13, 2019. 10.3389fnins.2019.00625. Neftci, E. O., Mostafa, H., and Zenke, F. Surrogate gradient learning in spiking neural networks, 2019. Olshausen, B. A. and Field, D. J. What is the other 85 percent of V1 doing? Problems in Systems Neuroscience , 23:182211, 2006. Orchard, G., Jayawant, A., Cohen, G. K., and Thakor, N. Converting static image datasets to spiking neuromorphic datasets using saccades. Frontiers in Neuroscience , 9, 2015. 10.3389fnins.2015.00437",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_120",
    "chunk_position": 120
  },
  {
    "id": "2303.10780v2",
    "text": ". Converting static image datasets to spiking neuromorphic datasets using saccades. Frontiers in Neuroscience , 9, 2015. 10.3389fnins.2015.00437. Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., and Dean, J., 2021. Perez-Nieves, N. and Goodman, D. F. M. Sparse spiking gradient descent. In Advances in Neural Information Processing Systems , 2021. Rasmussen, D. Nengodl: Combining deep learning and neuromorphic modelling methods. Neuroinformatics , 17: 611628, 2019. Rathi, N. and Roy, K",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_121",
    "chunk_position": 121
  },
  {
    "id": "2303.10780v2",
    "text": ". Rasmussen, D. Nengodl: Combining deep learning and neuromorphic modelling methods. Neuroinformatics , 17: 611628, 2019. Rathi, N. and Roy, K. Stdp based unsupervised multimodal learning with cross-modal processing in spiking neural networks. IEEE Transactions on Emerging Topics in Computational Intelligence , 5(1):143153, 2021a. 10.1109TETCI.2018.2872014.Rathi, N. and Roy, K. Stdp based unsupervised multimodal learning with cross-modal processing in spiking neural networks. IEEE Transactions on Emerging Topics in Computational Intelligence , 5(1):143153, 2021b. 10.1109TETCI.2018.2872014",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_122",
    "chunk_position": 122
  },
  {
    "id": "2303.10780v2",
    "text": ". IEEE Transactions on Emerging Topics in Computational Intelligence , 5(1):143153, 2021b. 10.1109TETCI.2018.2872014. Rossbroich, J., Gygax, J., and Zenke, F. Fluctuation-driven initialization for spiking neural network training. Neuromorphic Computing and Engineering , 2(4), 2022. 10.10882634-4386ac97bb. Rueckauer, B., Lungu, I.-A., Hu, Y ., Pfeiffer, M., and Liu, S.-C. Conversion of continuous-valued deep networks to efcient event-driven networks for image classication. Frontiers in Neuroscience , 11, 2017. 10.3389fnins. 2017.00682",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_123",
    "chunk_position": 123
  },
  {
    "id": "2303.10780v2",
    "text": ". Frontiers in Neuroscience , 11, 2017. 10.3389fnins. 2017.00682. Salaj, D., Subramoney, A., Krai snikovi c, C., Bellec, G., Legenstein, R., and Maass, W. Spike-frequency adaptation provides a long short-term memory to networks of spiking neurons. 2020. 10.11012020.05.11.081513. Sharma, D., Ankit, A., and Roy, K. Identifying efcient dataows for spiking neural networks. In Proceedings of the ACMIEEE International Symposium on Low Power Electronics and Design , 2022. 10.11453531437. 3539704. She, X., Dash, S., Kim, D., and Mukhopadhyay, S",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_124",
    "chunk_position": 124
  },
  {
    "id": "2303.10780v2",
    "text": ". 10.11453531437. 3539704. She, X., Dash, S., Kim, D., and Mukhopadhyay, S. A heterogeneous spiking neural network for unsupervised learning of spatiotemporal patterns. Frontiers in Neuroscience , 14, 2021. 10.3389fnins.2020.615756. She, X., Dash, S., and Mukhopadhyay, S. Sequence approximation using feedforward spiking neural network for spatiotemporal learning: Theory and optimization methods. In International Conference on Learning Representations , 2022. Sironi, A., Brambilla, M., Bourdis, N., Lagorce, X., and Benosman, R",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_125",
    "chunk_position": 125
  },
  {
    "id": "2303.10780v2",
    "text": ". In International Conference on Learning Representations , 2022. Sironi, A., Brambilla, M., Bourdis, N., Lagorce, X., and Benosman, R. Hats: Histograms of averaged time surfaces for robust event-based object classication. In 2018 IEEECVF Conference on Computer Vision and Pattern Recognition , 17311740, 2018. 10.1109CVPR. 2018.00186. Skatchkovsky, N., Simeone, O., and Jang, H. Learning to time-decode in spiking neural networks through the information bottleneck, 2021. Thanapitak, S. Bionics Chemical Synapse . Ph D thesis, 01 2012. Wade, J. J., Mc Daid, L., Santos, J. A., and Sayers, H. M",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_126",
    "chunk_position": 126
  },
  {
    "id": "2303.10780v2",
    "text": ". Thanapitak, S. Bionics Chemical Synapse . Ph D thesis, 01 2012. Wade, J. J., Mc Daid, L., Santos, J. A., and Sayers, H. M. Swat: A spiking neural network training classication problems. IEEE Transactions on Neural Networks , 21:18171830, 2010. A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices Wattanapanitch, W., Fee, M., and Sarpeshkar, R. An energyefcient micropower neural recording amplier. IEEE Transactions on Biomedical Circuits and Systems , 1(2): 136147, 2007. Xiao, H., Rasul, K., and V ollgraf, R",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_127",
    "chunk_position": 127
  },
  {
    "id": "2303.10780v2",
    "text": ". IEEE Transactions on Biomedical Circuits and Systems , 1(2): 136147, 2007. Xiao, H., Rasul, K., and V ollgraf, R. Fashion-MNIST: a novel image dataset for benchmarking machine learning , 2017. Xiao, M., Meng, Q., Zhang, Z., Wang, Y ., and Lin, Z. Training feedback spiking neural networks by implicit differentiation on the state. In Advances in Neural Information Processing Systems , 2021. Xiao, M., Meng, Q., Zhang, Z., He, D., and Lin, Z. Online training through time for spiking neural networks. In Advances in Neural Information Processing Systems , 2022. 10.48550ARXIV .2210.04195. Zenke, F",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_128",
    "chunk_position": 128
  },
  {
    "id": "2303.10780v2",
    "text": ". In Advances in Neural Information Processing Systems , 2022. 10.48550ARXIV .2210.04195. Zenke, F. and Ganguli, S. Super Spike: Supervised learning in multilayer spiking neural networks. Neural Computation , 30(6):15141541, 2018. 10.1162 neco a01086. Zenke, F. and V ogels, T. P. The remarkable robustness of surrogate gradient learning for instilling complex function in spiking neural networks. Neural Computation , 33(4): 899925, 2021. 10.1162neco a01367. Zhang, S.-Q. and Zhou, Z.-H. Theoretically provable spiking neural networks. In Advances in Neural Information Processing Systems , 2022",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_129",
    "chunk_position": 129
  },
  {
    "id": "2303.10780v2",
    "text": ". Zhang, S.-Q. and Zhou, Z.-H. Theoretically provable spiking neural networks. In Advances in Neural Information Processing Systems , 2022. Zhang, Y ., Li, P., Jin, Y ., and Choe, Y . A digital liquid state machine with biologically inspired learning and its application to speech recognition. IEEE Transactions on Neural Networks and Learning Systems , 26(11):2635 2649, 2015. 10.1109TNNLS.2015.2388544. Zheng, H., Wu, Y ., Deng, L., Hu, Y ., and Li, G. Going deeper with directly-trained larger spiking neural networks",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_130",
    "chunk_position": 130
  },
  {
    "id": "2303.10780v2",
    "text": ". 10.1109TNNLS.2015.2388544. Zheng, H., Wu, Y ., Deng, L., Hu, Y ., and Li, G. Going deeper with directly-trained larger spiking neural networks. In Proceedings of the AAAI Conference on Articial Intelligence , volume 35, 2021. Zhu, Y ., Yu, Z., Fang, W., Xie, X., Huang, T., and Masquelier, T. Training spiking neural networks with eventdriven backpropagation. In Advances in Neural Information Processing Systems , 2022. A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices A",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_131",
    "chunk_position": 131
  },
  {
    "id": "2303.10780v2",
    "text": ". Performance Comparisons A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best Practices Model Architecture Learning Method Training Framework Dataset Accuracy (Perez-Nieves Goodman, 2021)3 layer SNN Backpropagation Pytorch CUDA Extension SHD 77.5 (Perez-Nieves Goodman, 2021)3 layer SNN Sparse Backpropagation Pytorch CUDA Extension SHD 71.7 (Diehl Cook, 2015) 2 layer SNN Unsupervised BRIAN MNIST 95.0 (Liu Yue, 2017) 3 layer SNN Unsupervised BRIAN MNIST 81.9 3 layer SNN Supervised BRIAN MNIST 98.0 6 layer SNN Supervised BRIAN MNIST 98.4 2 layer SNN",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_132",
    "chunk_position": 132
  },
  {
    "id": "2303.10780v2",
    "text": "(Liu Yue, 2017) 3 layer SNN Unsupervised BRIAN MNIST 81.9 3 layer SNN Supervised BRIAN MNIST 98.0 6 layer SNN Supervised BRIAN MNIST 98.4 2 layer SNN TTFS Encoding, STDP Not Listed MNIST 88.6 2 layer SNN Phase Encoding, STDP Not Listed MNIST 88.2 2 layer SNN Burst Encoding, STDP Not Listed MNIST 88.4 2 layer SNN Rate Encoding, STDP Not Listed MNIST 87.5 (Rathi Roy, 2021a) 2 layer SNN Unsupervised STDP BRIAN MNIST 93.2 (Rathi Roy, 2021a) 4 layer SNN Unsupervised STDP BRIAN MNIST, TI46 98.0 (Perez-Nieves Goodman, 2021)3 layer SNN Backpropagation Pytorch CUDA Extension N-MNIST 92.7 (Perez-Nieves",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_133",
    "chunk_position": 133
  },
  {
    "id": "2303.10780v2",
    "text": "Unsupervised STDP BRIAN MNIST, TI46 98.0 (Perez-Nieves Goodman, 2021)3 layer SNN Backpropagation Pytorch CUDA Extension N-MNIST 92.7 (Perez-Nieves Goodman, 2021)3 layer SNN Sparse Backpropagation Pytorch CUDA Extension N-MNIST 97.4 (Perez-Nieves Goodman, 2021)3 layer SNN Backpropagation Pytorch CUDA Extension F-MNIST 82.2 (Perez-Nieves Goodman, 2021)3 layer SNN Sparse Backpropagation Pytorch CUDA Extension F-MNIST 80.1 (Perez-Nieves Goodman, 2021)6 layer SNN Sparse Backpropagation Pytorch CUDA Extension F-MNIST 82.7 (Perez-Nieves Goodman, 2021)CSNN Backpropagation Pytorch CUDA Extension",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_134",
    "chunk_position": 134
  },
  {
    "id": "2303.10780v2",
    "text": "2021)6 layer SNN Sparse Backpropagation Pytorch CUDA Extension F-MNIST 82.7 (Perez-Nieves Goodman, 2021)CSNN Backpropagation Pytorch CUDA Extension F-MNIST 86.9 (Perez-Nieves Goodman, 2021)CSNN Sparse Backpropagation Pytorch CUDA Extension F-MNIST 86.7 2 layer SNN Rate Encoding, STDP Not Listed F-MNIST 68.3 2 layer SNN TTFS Encoding, STDP Not Listed F-MNIST 71.3 2 layer SNN Phase Encoding, STDP Not Listed F-MNIST 71.4 2 layer SNN Burst Encoding, STDP Not Listed F-MNIST 71.3 CSNN RSNN BPTT Not Listed DVS 97.1 CSNN Surrogate GD Not Listed DVS 97.5 CSNN Surrogate GD Not Listed DVS 97.8 SVM Time",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_135",
    "chunk_position": 135
  },
  {
    "id": "2303.10780v2",
    "text": "STDP Not Listed F-MNIST 71.3 CSNN RSNN BPTT Not Listed DVS 97.1 CSNN Surrogate GD Not Listed DVS 97.5 CSNN Surrogate GD Not Listed DVS 97.8 SVM Time Surfaces SVM Not Listed DVS 95.2 CSNN STDP Not Listed DVS 96.2 m MND (SNN) STDP Not Listed DVS 96.6 m MND (SNN) BPTT Not Listed DVS 98.0 3 layer SNN Supervised BRIAN TI46 95.3 Reservoir SNN Supervised BRIAN TI46 99.8 (Rathi Roy, 2021a) 3 layer SNN Unsupervised STDP BRIAN TI46 96.0 Res Net-19 DSpike Not Listed CIFAR-10 94.2 Res Net-19 IM-Loss, ESG, td BN Not Listed CIFAR-10 95.4 VGG-16 IM-Loss, ESG, td BN Not Listed CIFAR-10 93.8 CIFARNet IM-Loss,",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_136",
    "chunk_position": 136
  },
  {
    "id": "2303.10780v2",
    "text": "Listed CIFAR-10 94.2 Res Net-19 IM-Loss, ESG, td BN Not Listed CIFAR-10 95.4 VGG-16 IM-Loss, ESG, td BN Not Listed CIFAR-10 93.8 CIFARNet IM-Loss, ESG, td BN Not Listed CIFAR-10 92.2 VGG5 Not Listed Pytorch CIFAR-10 87.5 Res Net12 Not Listed Pytorch CIFAR-10 90.3 Res Net12 Not Listed Pytorch CIFAR-100 58.9 VGG11 Not Listed Pytorch CIFAR-100 65.1 Res Net-19 DSpike Not Listed CIFAR-100 72.2 VGG-16 IM-Loss, ESG, td BN Not Listed CIFAR-100 70.1",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_137",
    "chunk_position": 137
  },
  {
    "id": "2303.10780v2",
    "text": ". Summary of common architectures, learning methods, frameworks, datasets, and achieved accuracies.",
    "source": "arXiv",
    "chunk_id": "2303.10780v2_138",
    "chunk_position": 138
  },
  {
    "id": "2304.14152v1",
    "text": "ar Xiv:2304.14152v1 cs.NE 27 Apr 2023Spiking Neural Network Decision Feedback for IMDD Systems Alexander von Bank, Eike-Manuel Edelmann, and Laurent Schm alen Communications Engineering Lab, Karlsruhe Institute of Te chnology, 76187 Karlsruhe, Germany alexander.bankkit.edu, edelmannkit.edu Abstract: A spiking neural network (SNN) with a decision fee dback structure is applied to an IMDD link with various parameters. The SNN o utperforms linear and articial neural network (ANN) based . 2023 The Autho 1",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_139",
    "chunk_position": 139
  },
  {
    "id": "2304.14152v1",
    "text": ". The SNN o utperforms linear and articial neural network (ANN) based . 2023 The Autho 1. Introduction Machine-learning-based have proven to enhance system performance for non-linear optical channels. However, the performance of most depends on thei r complexity, leading to power-hungry receivers when implemented on digital hardware. Compared to conventi onal digital hardware, neuromorphic hardware can massively reduce energy consumption when solving the same t asks 1",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_140",
    "chunk_position": 140
  },
  {
    "id": "2304.14152v1",
    "text": ". Compared to conventi onal digital hardware, neuromorphic hardware can massively reduce energy consumption when solving the same t asks 1. Spiking neural networks (SNNs) implemented on neuromorphic hardware mimic the human brains beh avior and promise energy-efcient, low-latency processing 2. In 3, an SNN-based with a decision feedback structur e (SNN-DFE) has been proposed for and demapping based on future and cur rently received, and already decided symbols",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_141",
    "chunk_position": 141
  },
  {
    "id": "2304.14152v1",
    "text": ". For different multipath scenarios, i.e., linear channels, the SNN-DFE performs similarly to the classical decision feedback (CDFE) and articial neural networ k (ANN) based . For a 4-fold pulse amplitude modulation (PAM4) transmitted over an intensity modul ation direct detection (IMDD) link suffering from chromatic dispersion (CD) and non-linear impairments, 4 proposes an SNN that estimates the transmit symbols based on received symbols without feedback, no-feedba ck-SNN (NF-SNN)",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_142",
    "chunk_position": 142
  },
  {
    "id": "2304.14152v1",
    "text": ". In simulations and experiments using the neuromorphic Brain Scale S-2 system, 4 outperforms linear minimum mean square error (LMMSE) and draws level with ANN-based . Howe ver, the approach of 4 lacks feedback on already decided decisions, which can enhance . In this paper, we apply the SNN-DFE of 3 to the channel model of 4. By incorporating decisions, the SNN-DFE outperforms the approach proposed by 4. As chromatic dispersion grows, the SNN-DFE benets more from the decisio n feedback structure. The available at . 2",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_143",
    "chunk_position": 143
  },
  {
    "id": "2304.14152v1",
    "text": ". As chromatic dispersion grows, the SNN-DFE benets more from the decisio n feedback structure. The available at . 2. Spiking-Neural-Network-based Decision Feedback izer and Demapper Like ANNs, SNNs are networks of interconnected neurons; how ever, SNNs represent information using pulses (spikes) instead of numbers. SNN neurons have an inte rnal membrane state , which depends on the time-variant input spike trains s 0,1,j N, emitted by the j-th upstream connected neuron. The spike trains scause the synaptic current , which charges the neurons state",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_144",
    "chunk_position": 144
  },
  {
    "id": "2304.14152v1",
    "text": ". The spike trains scause the synaptic current , which charges the neurons state . Ifsurpasses a predened threshold vth, i.e., if vth, the neuron emits an output spike. Afterward, the neuron res ets its state to vr. A common neuron model is the leaky-integrate-and-re (LIF ) model. It is described by 2 d dvr) m(vth)(vrvth)andd dt sjwjs, where mandsare the time constants ofand,()denotes the Heaviside function, and wj Rare the input weights",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_145",
    "chunk_position": 145
  },
  {
    "id": "2304.14152v1",
    "text": ". The Py Torch-based SNN deep learning library Norse 5 provides the approximate solutions of the as wel l as the backpropagation through time algorithm (BPTT) with surrogate gradient s 2, which is used for updating the networks weights. SNNEncodingz1z1yyyencknargmax Bit-Mapper one-hot z1z1z1akmyk ak bbbk . 1: Proposed SNN-DFE structure Inspired by a DFE, we proposed an SNN-based and demapper (SNN-DFE) in 3, whose architecture can be seen in . 1",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_146",
    "chunk_position": 146
  },
  {
    "id": "2304.14152v1",
    "text": ". 1: Proposed SNN-DFE structure Inspired by a DFE, we proposed an SNN-based and demapper (SNN-DFE) in 3, whose architecture can be seen in . 1. The received symbol yk, where kdenotes the discrete time, is translated into a spike signal us ing ternary encoding 3 with Mt8 input neurons per sample. Based on the last nceilingleftbigntap 2ceilingrightbig received symbols and the lastmfloorleftbigntap 2floorrightbig estimated symbols, the transmit symbol class estimate akis obtained, where ntapis the number of significant channel taps when sampling the channel with symbol rate",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_147",
    "chunk_position": 147
  },
  {
    "id": "2304.14152v1",
    "text": ". Finally, a bit mapper translates the estimate akto a corresponding bit sequence bbbk. This work has received funding from the European Research Co uncil (ERC) under the European Unions Horizon 2020 researc h and innovation programme (grant agreement No. 101001899). Par ts of this work were carried out in the framework of the CELTIC -NEXT project AI-NET-ANTILLAS (C20193-3) funded by the German Federal M inistry of Education and Research (BMBF) (grant agreement 1 6KIS1316). 3",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_148",
    "chunk_position": 148
  },
  {
    "id": "2304.14152v1",
    "text": ". 3. Results and Conclusion 16 18 20 22105104103102 2(d B)BERLMMSE CDFE ref ANN1 4 NF-ANN ANN-DFE ref SNN 4 NF-SNN SNN-DFE 16 18 20 22105104103102 2(d B)BERLMMSE CDFE ref ANN1 4 NF-ANN ANN-DFE ref SNN 4 NF-SNN SNN-DFE16 18 20 22 2(d B)NF-ANN ANN-DFE NF-SNN SNN-DFE 16 18 20 22 2(d B)NF-ANN ANN-DFE NF-SNN SNN-DFE16 18 20 22 2(d B)NF-SNN t e d B NF-SNN t 17 d B SNN-DFE t e d B SNN-DFE t 17 d B 16 18 20 22 2(d B)NF-SNN t e d B NF-SNN t 17 d B SNN-DFE t e d B SNN-DFE t 17 d B: Comparison of ANNs and SNNs with NFand DFE-structur e",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_149",
    "chunk_position": 149
  },
  {
    "id": "2304.14152v1",
    "text": ". Left: Channel A, including CDFE, LMMSE, as well as the ANN a nd SNN references of 4. Middle: Channel B. Right: Channel B for networks trained a t217d B versus those trained at each 2. 1 2 3 4 5 6105104103102 Length (km)BERNF-ANN ANN-DFE NF-SNN SNN-DFE 1 2 3 4 5 6105104103102 Length (km)BERNF-ANN ANN-DFE NF-SNN SNN-DFE . 3: Channel B for different ber lengths.To benchmark the SNN-DFE against the approach of 4, we implemented the IMDD link as given by 4, . 2A with the same parameters (channel A). With altered parameters, a second r eference link (channel B) with a stronger CD is created",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_150",
    "chunk_position": 150
  },
  {
    "id": "2304.14152v1",
    "text": ". 2A with the same parameters (channel A). With altered parameters, a second r eference link (channel B) with a stronger CD is created. Shared parame ters are a ber length of 5km, a pulse shaping roll-off factor of 0.2, and a direct current block prior to . Channel-s pecic parameters for channel A are (100 GBd, 1270 nm, 5 psnm1km1, a bias of 2.25 after pulse shaping, ntap17,C3,1,1,3) and for channel B (50 GBd, 1550 nm, 17 psnm1km1, a bias of 0.25 after pulse shaping, ntap41,C0,1, 2, 3), where Cis the set of transmit symbols with Gray mapping",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_151",
    "chunk_position": 151
  },
  {
    "id": "2304.14152v1",
    "text": ". To benchmark our S NNDFE (using ternary encoding 3), we implemented the SNN and reference ANN of 4 (using log-scale encoding) with ntaptaps. Since both approaches lack decision feedback, they are referred t o as NF-SNN and NF-ANN. Furthermore, we benchmark against the CDFE and LMMSE with ntaptaps. To compare the SNN-DFE with an ANN-based approach, we replace the SNN of . 1with an ANN (ANN-DFE) and ignore the encoding (real-valued i nputs instead of encoded inputs)",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_152",
    "chunk_position": 152
  },
  {
    "id": "2304.14152v1",
    "text": ". 1with an ANN (ANN-DFE) and ignore the encoding (real-valued i nputs instead of encoded inputs). All networks have No4 output neurons, for channel A Nh40 hidden neurons, and channel B Nh80, since the task of and demapping is more dema nding for strong CD. The networks are trained with a learning rate of 103using 10000 independent batches with 200000 transmit symbo ls each. The noise power during training is experimentally chosen to be217d B, corresponding to an ANN-DFE training SER 102. For the left plot of",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_153",
    "chunk_position": 153
  },
  {
    "id": "2304.14152v1",
    "text": ". The noise power during training is experimentally chosen to be217d B, corresponding to an ANN-DFE training SER 102. For the left plot of . 2, we include the results of 4 for their proposed SNN (ref SNN) and reference ANN (ref ANN1). Since NF-SNN and NF-ANN outper form ref SNN and ref ANN1, which are of architecture, we conclude that our proposed training setup can boost performance. As shown in the left and middle plots of . 2, for both channels, the DFE-based structures outperform NF -structures; furthermore, the SNNs we studied performed better than the corresponding ANN s",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_154",
    "chunk_position": 154
  },
  {
    "id": "2304.14152v1",
    "text": ". The right plot of . 2reveals that training all networks at a specic noise power (t 17 d B) results in better performance than training the networks for each 2(t e d B). We conclude that for low 2, the error magnitude is insufcient for proper learning; ad trained networks can generalize for different 2. 3depicts the results for channel B at 221 d B and varying ber lengths. Again, the networks are trained indiv idually for each length at a 2corresponding to an ANN-DFE SER 102",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_155",
    "chunk_position": 155
  },
  {
    "id": "2304.14152v1",
    "text": ". Again, the networks are trained indiv idually for each length at a 2corresponding to an ANN-DFE SER 102. As the ber length increases, the impact of CD grows, and DFE -based structures (especially the SNN-DFE) outperform NF-structures. We conclude that for an IMDD link, the SNN-DFE can exploit the decision feedback to combat CD for the investigated 2. It should be noted that the SNN-DFE suffers from error propagation with increasing SER. The SNN-DFE outperf orms all ANN-based and the proposed in 4 for highly dispersive regions, enabling powerful and ener gy-efcient and demapping",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_156",
    "chunk_position": 156
  },
  {
    "id": "2304.14152v1",
    "text": ". The SNN-DFE outperf orms all ANN-based and the proposed in 4 for highly dispersive regions, enabling powerful and ener gy-efcient and demapping. References 1.T. Ferreira de Lima et al. Progress in neuromorphic photonics, Nanophotonics , 6, no. 3, 577-599, 2017. 2. E. O. Neftci, H. Mostafa and F. Zenke, Surrogate gradient learning in spiking neural networks: Bringing the power of g radient-based optimization to spiking neural networks, IEEE Signal Process. Mag. , 36, no. 6, 51-63, Nov. 2019 3. E.-M. Bansbach, A. von Bank and L. Schmalen, Spiking neur al network decision feedback , in Proc",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_157",
    "chunk_position": 157
  },
  {
    "id": "2304.14152v1",
    "text": ". Mag. , 36, no. 6, 51-63, Nov. 2019 3. E.-M. Bansbach, A. von Bank and L. Schmalen, Spiking neur al network decision feedback , in Proc. Int. ITG WSA-SCC , Braunschweig, Germany, Feb. 2023 4. E. Arnold et al. , Spiking neural network on neuromorphic har dware for IMDD optical communication, in Proc. Eur. Conf. Opt. Commun. (ECOC) , Basel, CH, Sep. 2022. 5. C. Pehle and J. Pedersen, Norse A deep learning library f or spiking neural networks, Jan. 2021, o.4422025, Documentation:",
    "source": "arXiv",
    "chunk_id": "2304.14152v1_158",
    "chunk_position": 158
  },
  {
    "id": "2304.00112v2",
    "text": "of Additive and Multiplicative Coupling in Spiking Neural Networks Georg Brner,1Fabio Schittler Neves,1and Marc Timme1 1Chair for Network Dynamics, Institute for Theoretical Physics and Center for Advancing Electronics Dresden (cfaed), TU Dresden (Dated: April 12, 2023) Spiking neural network models characterize the emergent collective dynamics of circuits of biological neurons and help engineer neuro-inspired solutions across elds",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_159",
    "chunk_position": 159
  },
  {
    "id": "2304.00112v2",
    "text": ". Most dynamical systems models of spiking neural networks typically exhibit one of two major types of interactions: First, the response of a neurons state variable to incoming pulse signals (spikes) may be additive and independent of its current state. Second, the response may depend on the current neurons state and multiply a function of the state variable. Here we reveal that spiking neural network models with additive coupling are to models with multiplicative coupling for simultaneously modied intrinsic neuron time evolution",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_160",
    "chunk_position": 160
  },
  {
    "id": "2304.00112v2",
    "text": ". As a consequence, the same collective dynamics can be attained by state-dependent multiplicative and constant (state-independent) additive coupling. Such a mapping enables the transfer of theoretical insights between spiking neural network models with dierent types of interaction mechanisms as well as simpler and more eective engineering applications. I. BACKGROUND Dierential model the time evolution of a broad range of natural and human-made systems with time-continuous intrinsic dynamics and interactions. 1 4",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_161",
    "chunk_position": 161
  },
  {
    "id": "2304.00112v2",
    "text": ". 1 4. The dynamics of systems with short-lasting interactions may be modeled by networks of articial pulsecoupled or spiking neurons. They constitute hybrid dynamical systems where the intrinsic continuous timeevolution of the units dynamics are interrupted by timediscrete events. At these events, the incoming pulses (spikes) deterministically change the state variables of the units in the receiving end. The event times, in turn, are determined by the state space trajectory passing certain subsets, e.g., crossing some manifold or hitting boundary points of state space",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_162",
    "chunk_position": 162
  },
  {
    "id": "2304.00112v2",
    "text": ". Since half a century, spiking and pulse-coupled network models play key roles in our understanding of natural phenomena such as the emergence of waves or synchrony 57. They also help to design appropriate collective dynamics for engineered systems implementing desired functionalities such as distributed sensing 810. Due to their hybrid nature, spiking systems exhibit, and thus capture, novel phenomena not present in systems of smooth, time-continuous dierential",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_163",
    "chunk_position": 163
  },
  {
    "id": "2304.00112v2",
    "text": ". Due to their hybrid nature, spiking systems exhibit, and thus capture, novel phenomena not present in systems of smooth, time-continuous dierential . Examples include the emergence of linearly unstable attractors (in the sense of Milnor) 1113, the existence of speed limits in the relaxation dynamics of networks of spiking units 14, 15, the possibility of identical oscillators overtaking each other even though they are symmetricallycoupled16, andtheemergenceofisochronous regions where multiple periodic orbits of the same period coexist 17",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_164",
    "chunk_position": 164
  },
  {
    "id": "2304.00112v2",
    "text": ". These phenomena have been demonstrated for models where the interactions are additive and the coupling strength is constant, i.e. independent of the current state of the unit that receives an interaction pulse signal. However, several models of biological neural circuits exhibit state-dependent, multiplicative coupling 1821 and recent conceptual works on articial computationaldevices demonstrated that networks of pulse-coupled oscillatory neurons with multiplicative coupling give rise to novel computational features such as recongurability and improved robustness, see, e.g., 22 and 23",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_165",
    "chunk_position": 165
  },
  {
    "id": "2304.00112v2",
    "text": ". In addition, certain types of multiplicative coupling may be simpler and more ecient to implement in hardware. Here we demonstrate that under certain conditions, network models with state-dependent, multiplicative coupling and models with constant additive coupling are in the sense that they exhibit identical spiking dynamics. The implications of this result are twofold. First, many theoretical and practical results on oscillator networks with additive coupling can be transferred to those with multiplicative coupling and vice versa, by transforming the neuron model accordingly",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_166",
    "chunk_position": 166
  },
  {
    "id": "2304.00112v2",
    "text": ". In particular, the extensive body of work on additive coupling systems can be carried over eectively to multiplicative coupling systems, while phenomena that have been described for systems with multiplicative coupling, such as robust and recongurable computation over a combinatorial number of inputs observed in 22 should also be expected for classes of additive-coupling systems",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_167",
    "chunk_position": 167
  },
  {
    "id": "2304.00112v2",
    "text": ". Second, for technical applications such as the growing eld of analogue and spiking computation, depending on the particular neuron model and the coupling network, usually one type of coupling can be designed and implemented more eectively. For example, systems with conductance-based leaky integrate-and-re (IF) neurons allow for straightforward multiplicative coupling. Using the connections established in this paper, one can choose the coupling type more freely by modifying the neuron model accordingly. II",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_168",
    "chunk_position": 168
  },
  {
    "id": "2304.00112v2",
    "text": ". Using the connections established in this paper, one can choose the coupling type more freely by modifying the neuron model accordingly. II. PULSE-COUPLED OSCILLATOR NETWORKS AND PHASE FORMALISM Consider an N-dimensional dynamical system with statesx,i1,...,N, with a dynamic dened byar Xiv:2304.00112v2 physics.comp-ph 11 Apr 2023 2 Ncoupled ordinary dierential dxi dtf S (1) with continuous functions f :RR, and interaction mediated by the terms S, where xjx1,...,x N",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_169",
    "chunk_position": 169
  },
  {
    "id": "2304.00112v2",
    "text": ". In the following, we consider oscillatory neurons as one core example 5, 12, 2429, where variables xiare typically interpreted as a potential",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_170",
    "chunk_position": 170
  },
  {
    "id": "2304.00112v2",
    "text": ". For implementing periodic free dynamics we require thatf :RRis positive and state a reset mechanism x :xreset0, (2) which sets the state variable xito a reset value xreset: 0 atthediscretetimes ti,mwhereitpassesathresholdvalue xthr i1for them-th time (from below), x xthr1,dx dtvextendsinglevextendsinglevextendsinglevextendsingle tti,m0.(3) Together with the intrinsic dynamics dened by (1) for Si 0, the reset mechanism (2) creates a free period Tifor each neuron. Note that the choice of xthr1andxreset0is made without loss of generality",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_171",
    "chunk_position": 171
  },
  {
    "id": "2304.00112v2",
    "text": ". Note that the choice of xthr1andxreset0is made without loss of generality. The interaction between dierent neurons, as described by the functions S, can take on many forms. Short-lasting, momentary interactions as found in biological systems such as populations of ashing reies or networks of spiking neurons in the brain, are often adequately captured in terms of pulse coupling. Within this context, a neuron that reaches its threshold is said to re or spike, and sends a stereotyped short-lasting signal, whichmediatesitseectonconnectedunits",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_172",
    "chunk_position": 172
  },
  {
    "id": "2304.00112v2",
    "text": ". Such pulse coupling between oscillatory neurons can generally be written as S Nsummationdisplay j1 jnegationslashisummationdisplay mepsilon1ij Ki(4) whereepsilon1ij Rquanties the strength of the coupling from neuron jto neuron iand the response kernels Kisatisfy Ki0,Ki 0fort 0, andintegraltext Kidt 1.tj,mis them-th time at which unit jreaches the threshold xreset 1, see (3) and ij0is the time it takes for a pulse to travel from neuronjto neuroni. Note that for ij0additional rules regarding the order of processing spiking events may be necessary",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_173",
    "chunk_position": 173
  },
  {
    "id": "2304.00112v2",
    "text": ". Note that for ij0additional rules regarding the order of processing spiking events may be necessary. Often, pulse-coupled systems exhibit a strong time scale separation between the interaction duration and the intrinsic time scales of the neurons so that the kernels Kijimaybeidealizedasa Diracdistribution Ki (t) (5) such that the coupling (4) becomes discontinuous. In this work, we focus on purely inhibitory coupling, i.e.,epsilon1ij0. For simplicity of presentation, in the following we setij 0(instantaneous interactions)",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_174",
    "chunk_position": 174
  },
  {
    "id": "2304.00112v2",
    "text": ". For simplicity of presentation, in the following we setij 0(instantaneous interactions). However, we point out that our ndings easily generalize to delayed systems. The eect of neuron jreaching threshold at time tj (dropping the index mfor sake of readability) can be summarized as instantaneous, discontinuous change of the other neurons states xi,inegationslashjby a constant epsilon1ij, which is independent of both the time and the state xi itself: x xepsilon1ij, (6) for illustration see gure 1a,b",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_175",
    "chunk_position": 175
  },
  {
    "id": "2304.00112v2",
    "text": ". Note that large coupling strengthsepsilon1ijmay lead to a negative potential xepsilon1 just after a reset, requiring (1) to provide solutions with xi0for the non-interaction case S 0. This may pose a modeling problem, as naturally occurring neurons typically have a nite lower boundary regarding their potential 29, 30. The resulting dynamical system is of hybrid type with continuous-time free (uncoupled) time evolution of neuronstatesinterruptedbyoneoftwotypesofeventsoccurring at discrete times, where maps are applied: i) reset (eq. (2)) or ii) spike reception (eq. (6))",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_176",
    "chunk_position": 176
  },
  {
    "id": "2304.00112v2",
    "text": ". (2)) or ii) spike reception (eq. (6)). The hybrid nature enables to transfer to new neuron state variables that are time-like (phases), in terms of the phase formalism introduced by Mirollo-Strogatz 5. Essentially a nonlinear transformation, it parametrizes the free time evolution of the oscillatory neurons in terms of periodic phasesthat evolve with a constant phase velocity didtiuntil they reach the threshold value thr i: 1 and are reset to 0. The interaction between dierent neurons is mediated via a so-called rise function or neuron potential U : 0 and U 1",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_177",
    "chunk_position": 177
  },
  {
    "id": "2304.00112v2",
    "text": ". The interaction between dierent neurons is mediated via a so-called rise function or neuron potential U : 0 and U 1. Then, the response of unit ion another unit jreaching the threshold valuej 1at timetjis dened as :Hepsilon1,), (7) with transfer function Hepsilon1,:U1 i Uepsilon1, (8) withepsilon1epsilon1ij. For a free dynamic of the form dxidt f, as in (1), it is always possible to choose Ux, (9) wherexthe free solution with period Ti. The phase description of the free time evolution is then given by d dti1 Ti, ,(10) whereis reset to 0after reaching 1,: 0. For illustration, see gure 1a-e",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_178",
    "chunk_position": 178
  },
  {
    "id": "2304.00112v2",
    "text": ". For illustration, see gure 1a-e. 3 - 1. Phase formalism for (a-e) additive and (fj) multiplicative inhibitory pulse coupling. (a,f) Reset event patterns of three-unit network as function of time. (b,g) Rise function of rst neuron x1(t). Att1, there is a threshold-induced reset, and at t2andt3resets of other neurons cause a discontinuous jump in . (c,h) The same dynamics represented in terms of the phase 1(t)(t). It evolves with constant velocity d1dt 1T1, interrupted by discontinuous reset and interaction events. (d,i) Rise functions,as functions of the phase",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_179",
    "chunk_position": 179
  },
  {
    "id": "2304.00112v2",
    "text": ". (d,i) Rise functions,as functions of the phase . The phase jumps in blue and red are mediated in terms of jumps in the rise functions ,, respectively. (e,j) Transfer functions Hepsilon1(),H()summarize the eect of an incoming pulse on the phase (t)of the receiving neuron.As a standard example, consider leaky integrate-andre neurons as described by dx dt Ix, (11) (dropping the index ithroughout, as we focus on one specic neuron at a time here) with positive constants I and I, giving the free time evolution I (1et) for 0 t and forn Z, with free period length T1l",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_180",
    "chunk_position": 180
  },
  {
    "id": "2304.00112v2",
    "text": ". According to (9) and (8) one nds the rise function UI I parenleftbig 1eTparenrightbig I parenleftbigg 1parenleft Big 1 Iparenright Bigparenrightbigg ,(13) and the transfer function HIF epsilon1() lnparenleftbig (1I)epsilon1Iparenrightbig l.(14) Another common model class has been proposed by Mirollo and Strogatz 5 directly in terms of a rise function UMS,1 bl), b 0.(15) It is chosen such that, while UMS,bitself is nonlinear and allows for realistic features such as down-concavity, the resulting transfer function is ane, HMS,b epsilon1() epsilon1epsilon1, (16) with constants",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_181",
    "chunk_position": 181
  },
  {
    "id": "2304.00112v2",
    "text": "allows for realistic features such as down-concavity, the resulting transfer function is ane, HMS,b epsilon1() epsilon1epsilon1, (16) with constants epsilon1ebepsilon1andepsilon1 (ebepsilon11)(eb1), allowing for easy analytical treatment",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_182",
    "chunk_position": 182
  },
  {
    "id": "2304.00112v2",
    "text": ". Note that UMS,b diverges already for a nite value 01(1eb): lim 0UMS, , whereas limUI for the integrate-and-re neuron. Hence, the eect of incoming pulses vanishes already at nite phases 0, and the permitted phases are bounded, (0,1, instead of(,1, as for leaky integrate-and-re neurons. We emphasize again that the coupling considered up to this point is additive in the original potential-like variablesxi. III",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_183",
    "chunk_position": 183
  },
  {
    "id": "2304.00112v2",
    "text": ". We emphasize again that the coupling considered up to this point is additive in the original potential-like variablesxi. III. MULTIPLICATIVE PULSE COUPLING Although in many situations the additive pulse coupling, as reviewed, in the last a tting description, other choices may capture underlying mechanisms in existing systems better or can be implemented more eectively when designing systems",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_184",
    "chunk_position": 184
  },
  {
    "id": "2304.00112v2",
    "text": ". In the following, we consider a type of inhibitory pulse coupling between oscillatoryneurons, inwhichtheeectofanincomingpulse 4 from neuron jat timetjijis linearly related to the state xof the receiving oscillator, S x Nsummationdisplay j1 jnegationslashisummationdisplay mij Ki,(17) wherei. Throughoutweuseatildetopointout multiplicative coupling, as opposed to additive coupling. Again setting (t), and assuming instantaneous interactions, ij 0, for simplicity of presentation, this leads to an update rule x (1ij) x, (18) instead of the additive-coupling update rule (6)",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_185",
    "chunk_position": 185
  },
  {
    "id": "2304.00112v2",
    "text": ". Inthefollowingwemodifythe Mirollo-Strogatzformalism to also describe inhibitory multiplicative coupling. Given the free neuron dynamics in terms of a rise function U ::U1 i U(1), (19) so that the eect of a neuron jresetting at time tjon the phase of a connected neuron iis expressed as :H,), (20) withij, analogous to (7) for standard additive coupling. Now, the free time evolution is again dened bydidtiand a reset to i 0upon reaching the threshold value thr i 1,: 0. As with additive coupling, for given free neuron dynamics x, one can set Uxandddt 1Ti, with the free period length Ti",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_186",
    "chunk_position": 186
  },
  {
    "id": "2304.00112v2",
    "text": ". As with additive coupling, for given free neuron dynamics x, one can set Uxandddt 1Ti, with the free period length Ti. For multiplicative inhibitory pulse coupling, only neuron potentials with U0are sensible, such that U(1ij)0, avoiding that the sign of the induced phase jump changes in a physically implausible way. Other than that, the requirements on Uiare the sameasforadditivecoupling, thatis, Uishouldbemonotonicallyincreasingandtwicecontinuouslydierentiable. Again, generalizationtosystemswithdelayedcouplingor inhomogeneous coupling strengths are straightforward",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_187",
    "chunk_position": 187
  },
  {
    "id": "2304.00112v2",
    "text": ". As an example, we again consider the common Integrate-and-Fire model UI I parenleftbig 1eTparenrightbig I parenleftbigg 1parenleft Big 1 Iparenright Bigparenrightbigg ,(21) (again dropping the neuron index ithroughout for sake of readability) and nd a multiplicative-coupling transfer function HIF () lnparenleftbig (1)(1I)parenrightbig l,(22) for illustration, see gure 1f-j.Note that the Mirollo-Strogatz neuron potential UMS,b epsilon1() 1 bl)as discussed in the last not lead to an ane transfer function anymore, if we apply multiplicative coupling: HMS,b () 1 eb1bracketleft Big (1",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_188",
    "chunk_position": 188
  },
  {
    "id": "2304.00112v2",
    "text": "1 bl)as discussed in the last not lead to an ane transfer function anymore, if we apply multiplicative coupling: HMS,b () 1 eb1bracketleft Big (1 (eb1))(1)1bracketright Big .(23) However, we propose an alternative neuron potential, UMS,:1c, c 0, (24) which indeed leads to an ane (and even linear) transfer function HMS,c () (1)c (25) for multiplicative coupling, see gure 2a,b",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_189",
    "chunk_position": 189
  },
  {
    "id": "2304.00112v2",
    "text": ". 2. Nonlinear neuron with linear transfer function for multiplicative coupling. (a) While the rise function UMS,(see eq. (24)) itself is nonlinear, its respective transfer function for multiplicative pulse coupling, H(), is linear and allows for easy analytical treatment. Shown for dierent values of c, for xed coupling strength 0.25. IV. OF MULTIPLICATIVE AND ADDITIVE COUPLING While with additive coupling (eq. (6), (7)) the state variablexiof a spike-receiving neuron iis shifted by a constant value epsilon1ij, with multiplicative coupling (eq",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_190",
    "chunk_position": 190
  },
  {
    "id": "2304.00112v2",
    "text": ". (6), (7)) the state variablexiof a spike-receiving neuron iis shifted by a constant value epsilon1ij, with multiplicative coupling (eq. (18), (16)) the state variable xiexperiences a relative shiftxiij. However, both interaction mechanism can be transferred into each other by simultaneously modifying the rise function accordingly. For sake of simplicity we focus only on a single neuron iat a time and drop indexithroughout, and assume that all pulses arriving at neuronihave the same strength: epsilon1ijepsilon1iepsilon1or iji",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_191",
    "chunk_position": 191
  },
  {
    "id": "2304.00112v2",
    "text": ". Below, we generalize to arbitrary coupling strength, and thus also sparse networks, see . Again writing and for rise functions for additive and multiplicative coupling, respectively, we demand that the corresponding transfer functions Hepsilon1() U1epsilon1 (26) and H() U1bracketleftbig(1)bracketrightbig (27) 5 are , i.e. Hepsilon1()H() (28) U1epsilon1 U1bracketleftbig(1)bracketrightbig ,(29) for all. Main claim",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_192",
    "chunk_position": 192
  },
  {
    "id": "2304.00112v2",
    "text": ". Hepsilon1()H() (28) U1epsilon1 U1bracketleftbig(1)bracketrightbig ,(29) for all. Main claim. As a main statement of this article we have the following: Given a rise function for additive inhibitory pulse coupling with coupling strength epsilon10, exactly the same spiking dynamics is realized by multiplicative inhibitory pulse coupling with coupling strength0,1)and rise function (1)parenleftbig1 epsilon1parenrightbig . (30) Todemonstratethisclaim, wetakeseveralsteps",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_193",
    "chunk_position": 193
  },
  {
    "id": "2304.00112v2",
    "text": ". (30) Todemonstratethisclaim, wetakeseveralsteps. First, we calculate the inverse U1of the rise function ) with regard to variable , expressed in terms of the inverse function U1and Uitself, U1U1parenleftbig 1epsilon1log1(U)parenrightbig , (31) where log1denotes the logarithm with base 1",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_194",
    "chunk_position": 194
  },
  {
    "id": "2304.00112v2",
    "text": ". We then use (30) and (31) to compute U1bracketleftbig(1)bracketrightbig U1parenleftbig 1epsilon1log1((1))parenrightbig U1parenleftbigg 1epsilon1log1parenleftbigg (1)parenleftbig1 epsilon11parenrightbigparenrightbiggparenrightbigg U1parenleftbigg 1epsilon1parenleftbigg1 epsilon1 1parenrightbiggparenrightbigg U1epsilon1. (32) Hence, indeedthetransferfunctions, andthusthespiking dynamics, are identical, Hepsilon1() H(), see (28). By solving (30) for , we get the inverse transformation",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_195",
    "chunk_position": 195
  },
  {
    "id": "2304.00112v2",
    "text": ". By solving (30) for , we get the inverse transformation. Given a multiplicative-coupling rise function , the same system can be described in terms of additive pulse coupling with rise function 1epsilon1log1(). (33) (30) and (33) can be interpreted as families of rise functions. For a given rise function for additive coupling and a given additive coupling strengthepsilon1, the coupling strength of the transformed dynamics can be chosen freely, when Uis chosen accordingly, so that eectively parametrizes the possible multiplicative-coupling rise functions U",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_196",
    "chunk_position": 196
  },
  {
    "id": "2304.00112v2",
    "text": ". Inversely, possible transformations to additive coupling are capturedbydierentchoicesofthenewcouplingstrength epsilon1, given a xed original multiplicative-coupling strength . For heterogeneous coupling, we refer to . While transformations (30), (33) are given in terms of the rise functions and , they are easily transferred to the actual neuron dynamics by letting ), ): (1)parenleftbig1 epsilon1parenrightbig , (34)and 1epsilon1log1(), (35) where we assume xthr 1. We point out that the period length Tand hence also the phase velocityddt 1Tis necessarily the same for rise functions. A",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_197",
    "chunk_position": 197
  },
  {
    "id": "2304.00112v2",
    "text": ". We point out that the period length Tand hence also the phase velocityddt 1Tis necessarily the same for rise functions. A. Additive to multiplicative coupling First we consider the transformation from additive to multiplicative coupling via (30), as illustrated by gure 3a-c for two standard neuron models. Note thatforthetransformedneurondynamics, , wehave 0. Indeed, from (30) we nd UR (1)1UR epsilon1,, (36) where we write URand URfor the reset potentials of and, respectively. According to (36) it is not possible to normalize both to 0 and 0 at the same time",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_198",
    "chunk_position": 198
  },
  {
    "id": "2304.00112v2",
    "text": ". According to (36) it is not possible to normalize both to 0 and 0 at the same time. In fact, it is a necessary condition for the between a multiplicative-coupling rise function Uand an additive-coupling rise function Uthat 0 and for the same phase 0, or in the limit. Illustratively speaking, 0and dene the phase 0at which the eect of multiplicative and additive coupling, respectively, vanishes, which must be the same for dynamics. If we require 0 , as is usually done for additive-coupling rise functions, the reset value UR (1)1epsilon10is always larger than the minimally allowed potential 0",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_199",
    "chunk_position": 199
  },
  {
    "id": "2304.00112v2",
    "text": ". From a practical perspective, the multiplicative coupling uses a baseline potential UG 0lower than the reset potential UR0 which can be considered a natural generalization of the standard multiplicative coupling as introduced in the last section. In this case, UR 0,UR0, we can also rewrite the transformation (30) in terms of the reset voltage URof the multiplicative coupling, ) R",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_200",
    "chunk_position": 200
  },
  {
    "id": "2304.00112v2",
    "text": ". In this case, UR 0,UR0, we can also rewrite the transformation (30) in terms of the reset voltage URof the multiplicative coupling, ) R. (37) While in (30) the family of multiplicative-coupling rise functions is parametrized in terms of the coupling strengths of the new coupling, (37) parametrizes the possible transformations in terms of UR, from which the new coupling strength is found as 1(UR)epsilon1. For example, consider the standard leaky integrateand-re potential (eq",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_201",
    "chunk_position": 201
  },
  {
    "id": "2304.00112v2",
    "text": ". For example, consider the standard leaky integrateand-re potential (eq. (13)) UIF,add.() I parenleftbig 1eTparenrightbig I parenleftbigg 1parenleft Big 1 Iparenright Bigparenrightbigg , (38) 6 where T1l, with an additive-coupling transfer function HIF,add. epsilon1 () lnparenleftbig (1I)epsilon1Iparenrightbig l.(39) As before, we use a tilde to dierentiate between rise functions for multiplicative or additive pulse coupling; the superset add. denotes that the original model (here integrate-and-re, IF) was formulated for additive coupling",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_202",
    "chunk_position": 202
  },
  {
    "id": "2304.00112v2",
    "text": ". denotes that the original model (here integrate-and-re, IF) was formulated for additive coupling. Transforming via (30), we get an rise function UIF,add.() (1)1 epsilon1parenleftbig 1I (1(1 I)parenrightbig (40) Uparenleftbig 1I (1(1 I)parenrightbig R (41) for multiplicative coupling. As another common example for additive coupling we consider the rise function UMS,b,add.() 1 blnparenleftbig 1 parenleftbig eb1parenrightbig parenrightbig , b 0,(42) (see eq. (15)) which diverges for a nite phase 0 1(1eb)(,0andcorrespondstoanadditivecoupling transfer function HMS,b,add",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_203",
    "chunk_position": 203
  },
  {
    "id": "2304.00112v2",
    "text": ". (15)) which diverges for a nite phase 0 1(1eb)(,0andcorrespondstoanadditivecoupling transfer function HMS,b,add. epsilon1 () ebepsilon11 eb1ebepsilon1, (43) (see eq",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_204",
    "chunk_position": 204
  },
  {
    "id": "2304.00112v2",
    "text": ". epsilon1 () ebepsilon11 eb1ebepsilon1, (43) (see eq. (16)) and an multiplicative coupling rise function UMS,b,add.() parenleftbig 1 parenleftbig eb1)parenrightbigparenrightbigln (1) bepsilon1(44) parenleftbig 1 parenleftbig eb1)parenrightbigparenrightbigb1ln UR We point out that for practical applications, the oset reset voltage, UR0, might be avoided by modifying the transformed rise function in an interval 0,arbitrary close to the reset phase 0, such that 0and is still strictly monotonous and continuous",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_205",
    "chunk_position": 205
  },
  {
    "id": "2304.00112v2",
    "text": ". While doing so might slightly change the collective dynamics during transients, for stable orbits in which neurons do not reach non-positive phases via inhibition, can be chosen such that the dynamics are completely to the original additive coupling. B. Multiplicative to additive coupling 3d-f illustrates the transfer of multiplicative to additive coupling via (33) in the case of UR0. This represents the most natural formulation for multiplicative coupling (as described in the last section, also see gure 2), where the reset potential URis to the ground potential UG 0",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_206",
    "chunk_position": 206
  },
  {
    "id": "2304.00112v2",
    "text": ". In this case, the eect of the coupling vanishes right at the reset phase 0. Hence, no negative phases are necessary,0,1. However, multiplicative coupling with UR 0 requires a divergence of the additive-coupling rise functionat 0,lim0 , forbidding normalization to UR0, or any other nite value. Note that the phase 1, above which the neuron potential for additive coupling becomes positive, 0, is connected to the coupling strength epsilon1via (1)1epsilon1, so that it can be chosen freely within (0,1)by adjusting epsilon1accordingly, in principle arbitrary close to 0",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_207",
    "chunk_position": 207
  },
  {
    "id": "2304.00112v2",
    "text": ". Instead of epsilon1, we can also use 1or to parametrize the family of additive-coupling transfer functions: 1log 1ln ln(45) Consider, for example, multiplicative coupling with a standard leaky integrate-and-re rise function UIF,mult.() I parenleftbigg 1parenleft Big 1 Iparenright Bigparenrightbigg ,(46) and a corresponding transfer function HIF,mult. () lnparenleftbig (1)(1I)parenrightbig l,(47) where we use the superscript mult. to point out that the original rise function is implemented for multiplicative coupling",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_208",
    "chunk_position": 208
  },
  {
    "id": "2304.00112v2",
    "text": ". to point out that the original rise function is implemented for multiplicative coupling. Via (33) we nd an additive-coupling rise function UIF,mult.() 1epsilon1log1parenleftbigg1(1I) Iparenrightbigg (48) 1logparenleftbigg1(1I) Iparenrightbigg , see gure 3g-i. As second example we give UMS,c,mult.() 1c, c 0,(49) with a linear multiplicative-coupling transfer function HMS,c,mult. () (1)c (50) and additive coupling rise function UMS,c,mult.() 1epsilon1 clog1()(51) 1ln () ln (1), which is the Mirollo-Strogatz neuron potential UMS,b (eq. (15)) shifted such that its divergence occurs at 0 0",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_209",
    "chunk_position": 209
  },
  {
    "id": "2304.00112v2",
    "text": ". (15)) shifted such that its divergence occurs at 0 0. Additive-coupling rise functions as created by the transformation (33) might be suciently approximated by rise function avoiding divergent behavior as 0, for0, allowing normalization to 0. For a practical example, see . 7 3. Transformation between additive and multiplicative pulse coupling. (a,d) Additive-coupling rise function . (b,e) Transfer function Hepsilon1() H(). (c,f) Multiplicative-coupling rise function . (red) Leaky integrate-and-re neuron model with additive coupling (eq. (14) with I 1, 0.9) ,epsilon1 0.2, 0.3",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_210",
    "chunk_position": 210
  },
  {
    "id": "2304.00112v2",
    "text": ". (red) Leaky integrate-and-re neuron model with additive coupling (eq. (14) with I 1, 0.9) ,epsilon1 0.2, 0.3. (blue) Mirollo-Strogatz neuron potential with additive coupling (eq. (15), b 2) ,epsilon1 0.4, 0.4. (brown): Leaky integrate-and-re neuron model with multiplicative coupling (eq. (21) with I 1, 0.9),epsilon1 0.2, 0.2. (green): Neuron potential UMS,cwith linear transfer function for multiplicative coupling (eq. (49) for c 2), withepsilon1 0.2, 0.3. C",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_211",
    "chunk_position": 211
  },
  {
    "id": "2304.00112v2",
    "text": ". (green): Neuron potential UMS,cwith linear transfer function for multiplicative coupling (eq. (49) for c 2), withepsilon1 0.2, 0.3. C. Inhomogeneous coupling strengths In the last we demonstrated a general approach to transfer multiplicative pulse-coupling to additive pulse-coupling and vice-versa. By setting epsilon1ijepsilon1 andij, we implicitly assumed that the coupling strengthsepsilon1ijdo not depend on the neuron jwhich is sending the pulse, but only on the receiving neuron i (whose index iwe suppressed throughout)",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_212",
    "chunk_position": 212
  },
  {
    "id": "2304.00112v2",
    "text": ". However, the between multiplicative and additive coupling holds also for inhomogenuous coupling strengths and coupling topologies dened on networks. In order to have dynamics for variable original coupling strengths, not only the rise functions, but also the coupling strengths itself are transformed via a mapping thatis uniquely determined by dening two values of epsilon1and as . Consider (30) again: (1)parenleftbig1 epsilon1parenrightbig",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_213",
    "chunk_position": 213
  },
  {
    "id": "2304.00112v2",
    "text": ". Consider (30) again: (1)parenleftbig1 epsilon1parenrightbig . (52) The corresponding transfer functions for additive and multiplicative coupling are not only for the specic choice ofepsilon1,used in the transformation (52) itself, but also for any combination of additive and multiplicative coupling strengths epsilon1primeandprimethat satises prime 1(1)epsilon1primeepsilon1, (53) as can be veried analogously to (26)-(32)",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_214",
    "chunk_position": 214
  },
  {
    "id": "2304.00112v2",
    "text": ". Hence, for each neuron i, a single choice of two couplingstrengthsdenesboththetransformedrisefunc8 tionsaswellasthemappingbetweenarbitraryequivalent coupling strengths epsilon1ij,ij. For transforming standard additive coupling with UR 0 to multiplicative coupling, the mapping between coupling strengths may also be dened in terms of the reset voltage URof the new rise function, not requiring an explicit choice of two coupling strengths",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_215",
    "chunk_position": 215
  },
  {
    "id": "2304.00112v2",
    "text": ". The new multiplicativecoupling rise function is then given in terms of ) R, (54) see (37), while the mapping between coupling strengths epsilon1ij,ijis given by ij 1 R. (55) Similarly, if the possible transformations of multiplicative coupling with UR 0to additive coupling are parametrized via the phase 1for which 0, 1log 1ln ln,(56) see (45)), the transformation between coupling strengths ij,epsilon1ijis dened via epsilon1ijlog (1ij) log ()",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_216",
    "chunk_position": 216
  },
  {
    "id": "2304.00112v2",
    "text": ". (57) Interaction topologies dened on a directed graph can be either implemented explicitly in terms of an adjacency matrix Aijthat denes which neurons ireceive a pulse from another neuron jor, implicitly, by setting epsilon1ij 0or ij 0for neurons that are not connected in the specic direction. For an example of delayed heterogeneous coupling on a directed network, see the next section. V. EXAMPLE APPLICATIONS A",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_217",
    "chunk_position": 217
  },
  {
    "id": "2304.00112v2",
    "text": ". For an example of delayed heterogeneous coupling on a directed network, see the next section. V. EXAMPLE APPLICATIONS A. Network computing To give a practical example for a transformation from multiplicative to additive pulse-coupling, we consider an implementation of a k-winners-take-all ( k-WTA) computation based on an network of Ninhibitorily coupled oscillatory neurons, as proposed in 22. The free dynamic of neuroni1,...,Nwith state variable xiis dened by dierential (Ix), (58) with constants I and external inputs :RR, and a reset rule, which sets x: 0for each time ti where x xthr1",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_218",
    "chunk_position": 218
  },
  {
    "id": "2304.00112v2",
    "text": ". Assuming that changes much slower than the inputs x, we get a free dynamic x I (1et) for 0 t T,(59)with free period length )1l. The interaction between dierent neurons is given in terms of all-to-all inhibitory multiplicative pulse-coupling x (1) x, (60) (cf. eq. (18)) with global coupling strength . Now, for performing k-WTA computations, the functions, which set the intrinsic frequencies i 1Tiof the individual neurons, are interpreted as the time-dependent input of the network, while the stream of reset events (i.e. the sent pulses) dene the output space, see gure 4a",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_219",
    "chunk_position": 219
  },
  {
    "id": "2304.00112v2",
    "text": ". the sent pulses) dene the output space, see gure 4a. Upon varying , the system after a typically short transient converges to a periodic orbit, in which only the k Nneurons with the largest intrinsic frequencies ispike, thereby revealing the subset of the klargest input signals i. The number of winners kis selected by adjusting the global coupling strength accordingly, allowing for easy recongurability",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_220",
    "chunk_position": 220
  },
  {
    "id": "2304.00112v2",
    "text": ". The number of winners kis selected by adjusting the global coupling strength accordingly, allowing for easy recongurability. The underlying mechanism is summarized as follows: Upon receiving a pulse from a neuroni, due to the multiplicative inhibitory coupling all other neurons jnegationslashiloose a certain share of their voltage, pushing their voltages closer together on an absolute scale. Depending on the inhibition strength, this allows faster neurons to overtake slower ones repeatedly, potentially keeping the latter from spiking altogether",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_221",
    "chunk_position": 221
  },
  {
    "id": "2304.00112v2",
    "text": ". 4d,e illustrates typical dynamics with k 2andk 3, for a specic choice of input signals in the original multiplicative-coupling formulation, with rise function U I parenleftbigg 1parenleft Big 1 Iparenright Bigiparenrightbigg , (61) (see g. 4c) and transfer function H, lnparenleftbigg (1)parenleft Big (1I parenright Bigiparenrightbigg lnparenleft Big 1I parenright Big.(62) The dynamics (g",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_222",
    "chunk_position": 222
  },
  {
    "id": "2304.00112v2",
    "text": ". 4g,h) for additive coupling are implemented via a free neuron dynamic given by x 1epsilon1logparenleft Big I (I (1)1epsilon1parenright Big log (1),(63) which corresponds to an additive-coupling rise function U 1epsilon1I parenleft Big 1parenleftbig 1 Iparenrightbigiparenright Big log (1),(64) see gure 4f. Note that neither Unor Udepend on itself because for the specic free dynamics as given by (58) the shape of the rise function itself does not change with , but the intrinsic neuron frequency 9 4",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_223",
    "chunk_position": 223
  },
  {
    "id": "2304.00112v2",
    "text": ". Recongurable k-WTA computation via symmetrical inhibitory coupling for multiplicative and additive coupling. (a) All-to-all network of N 5oscillatory neurons with time-dependent external inputs setting the intrinsic frequencies of the individual neurons. (b): Choice of ifor the illustrated dynamics. (c,f,i) Rise functions for multiplicative, additive, and approximated additive coupling with Uap 0. In (i), The phase below which Uap(solid line) deviates from (dotted line) is denoted by a blue line. The minimum phase that is reached via inhibition is marked by a cross",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_224",
    "chunk_position": 224
  },
  {
    "id": "2304.00112v2",
    "text": ". The minimum phase that is reached via inhibition is marked by a cross. (d,g,j) dynamics with k 2spiking neurons for multiplicative, additive, and approximated additive coupling. 2for multiplicative and epsilon1 0.1for additive coupling. (e,h,k) dynamics with k 3spiking neurons for multiplicative, additive, and approximated additive coupling. prime 1.9for multiplicative and epsilon1prime0.089for additive coupling. i 1Ti)",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_225",
    "chunk_position": 225
  },
  {
    "id": "2304.00112v2",
    "text": ". prime 1.9for multiplicative and epsilon1prime0.089for additive coupling. i 1Ti). For the illustrated transformation, we required that multiplicative coupling strength 0.21 (for the rst orbit, with k 2) is to additive coupling strength epsilon1 0.1, so thatprime 0.19 (for the second orbit, with k 3) is to epsilon1primeepsilon1log (1prime)log (1)0.089, (cf. eq. (53)). 4i-k illustrates how additive coupling with URcan be eectively approximated by a modied additional-coupling rise function Uapwhich satises Uap 0 for a more practical implementation",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_226",
    "chunk_position": 226
  },
  {
    "id": "2304.00112v2",
    "text": ". Here, we require that Uapfor phases 0.2and dene Uapfor in terms of cubic extrapolation which satises Uap 0. For our specic choice of 0.2, the illustrated spiking dynamics are in fact fully to both the original multiplicative and the exact additive transformation, because the minimal phase reached upon inhibition is larger than 0.2(see g. 4i). B",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_227",
    "chunk_position": 227
  },
  {
    "id": "2304.00112v2",
    "text": ". 4i). B. Topology-induced synchronization As a second example, we consider a network with delayed, heterogeneous coupling, implementing a phenomenon initially addressed in 31 which relates topological features of directed interaction graphs with the degree of synchronization in the resulting collective dynamics. Though originally described for continuously coupled Kuramoto oscillators, we here reproduce the ef-fect for leaky-integrate and re-dynamics with delayed additive inhibitory pulse coupling and then exemplarily transfer the dynamics to multiplicative coupling",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_228",
    "chunk_position": 228
  },
  {
    "id": "2304.00112v2",
    "text": ". Consider a network of Nidentical oscillatory neurons with a leaky integrate-and-re dynamic xi Ixi, (65) with I 1, 0.9, and a reset rule which sets x 0for each time tiwherex xthr1, leading to a free time evolution x I (1et) for 0 t T,(66) with period length T1l. The interaction between dierent units is given in terms of delayed inhibitory additive pulse-coupling with x xepsilon1ij, (67) (cf. eq. (6)), where 0",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_229",
    "chunk_position": 229
  },
  {
    "id": "2304.00112v2",
    "text": ". eq. (6)), where 0. The coupling strengths epsilon1ijare given in terms of the adjacency matrix Aof an directed graph with matrix elements Aij0,1as epsilon1ijepsilon1Aijgi, (68) wheregisummationtext j Aijis the in-degree of node iandepsilon1 0 is a global constant. 5a,b depicts two interaction graphs,Gand Gprime, which are identical except for an additional directed link between neurons 1and5in the second graph. 10 5. Topology-induced phase synchronization via inhibitory coupling with delay, for multiplicative and additive coupling",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_230",
    "chunk_position": 230
  },
  {
    "id": "2304.00112v2",
    "text": ". 10 5. Topology-induced phase synchronization via inhibitory coupling with delay, for multiplicative and additive coupling. (a) Network of N 10identical integrate-and-re neurons, with interaction topology dened by graph G. Strongly connected components Aand Baect Cindependently from each other. (b) After addition of a directed link from node 1to5,Aaects both Band Cand acts as a single source of the new interaction graph Gprime. (c,g) rise functions and for additive and multiplicative pulse coupling. For additive coupling, we use standard integrate-and-re dynamics as described by eq",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_231",
    "chunk_position": 231
  },
  {
    "id": "2304.00112v2",
    "text": ". For additive coupling, we use standard integrate-and-re dynamics as described by eq. (13), with I 1, 0.9. (d,h) coupling strengths epsilon1ijandijfor additive and multiplicative coupling, respectively, as given by eq. (73), for the interaction graph G. The matrix element corresponding to the added link is marked by a blue square. For the delay between all connected units we take 1. (e,j) Phase lag of the individual units i relative to the period length collective dynamic after a transient",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_232",
    "chunk_position": 232
  },
  {
    "id": "2304.00112v2",
    "text": ". (e,j) Phase lag of the individual units i relative to the period length collective dynamic after a transient. Strongly connected components Aand Bact as individual sources of the directed interaction graph G, not allowing for global phase synchronization. (f,g) Adding a single link makes Athe only source in the new interaction graph G, leading to full phase synchronization. 5e,f shows for both network topologies the relative time lag between reset events of the neurons after a periodic orbit is reached, from the same random initial condition",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_233",
    "chunk_position": 233
  },
  {
    "id": "2304.00112v2",
    "text": ". While the rst interaction topology leads to only partial phase synchronization (g. 5e), adding a single link, A5,1: 1, leads to full synchronization with no phase lag at all (g. 5f). This is directly connected to the strongly-connected components of the graphs Gand Gprime, referred to as A,B, and C. While in the rst interaction topology, Aand Bindependently aect C, with no feedback from Cback to Aor Cand no interaction at all between Aand B, in the second topology, component A additionally aects B",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_234",
    "chunk_position": 234
  },
  {
    "id": "2304.00112v2",
    "text": ". While for G,Aand Bsynchronize independently to dierent phases depending on the initial conditions and do not allow for Cto synchronize to a single phase, for Gprime,Aacts as a single source of the system, which synchronizes autonomously and forces its phase on the rest of the network via directed connections to all other nodes.In terms of the phase formalism, for the original additive coupling (eq",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_235",
    "chunk_position": 235
  },
  {
    "id": "2304.00112v2",
    "text": ". (67)), we have rise functions U I parenleftbig 1ei Tparenrightbig I parenleftbigg 1parenleft Big 1 Iparenright Bigiparenrightbigg ,(69) and transfer functions HIF i,epsilon1i lnparenleftbig (1I)iepsilon1ijIparenrightbig l.(70) Note that the neurons itself are identical in their free dynamics, and only dier in their coupling strengths, and hence their transfer functions, see gure 5c. Although it is not strictly necessary, it is reasonable to maintain identical free dynamics also when using multiplicative coupling, resulting in dierent coupling strengths ijfor each pulse-receiving neuron i",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_236",
    "chunk_position": 236
  },
  {
    "id": "2304.00112v2",
    "text": ". Hence, we set the reset voltage UR0to be the same for each neuron, UR0.1and use it to dene the transformation to multiplicative coupling (see eq. (37)), leading to free neuron 11 dynamics x ) R (71) or, , rise functions U Uparenleftbig 1I (1(1 I)iparenrightbig R , (72) see gure 5g with corresponding multiplicative coupling strengths ij 1 R 1Uepsilon1Aijgi R, (73) see gure 5d,h. Starting from initial conditions, the resulting spiking dynamic and hence also the degree of phase locking (see g. 5j,i) is exactly the same as for the original additive-coupling formulation. VI",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_237",
    "chunk_position": 237
  },
  {
    "id": "2304.00112v2",
    "text": ". 5j,i) is exactly the same as for the original additive-coupling formulation. VI. CONCLUSION Basic standard models of spiking neural networks such as networks of leaky integrate-and-re neurons exhibit additive interactions where the postsynaptic response of a neuron is parametrized in terms of a coupling strength. Other and more advanced models feature multiplicative coupling where the state change of the postsynaptic neuron depends on the state that neuron is in at the time of pulse reception",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_238",
    "chunk_position": 238
  },
  {
    "id": "2304.00112v2",
    "text": ". Here we have demonstrated that under certain conditions, additiveandmultiplicativecouplingmaybeviewed as two mathematically options for modeling. In particular, the phase representation originally introduced by Mirollo and Strogatz for additive coupling 5 is readily modied to characterize dynamics of pulse-coupled systems with multiplicative coupling. To explicate the most clearly, we analyze a simple class of systems with instantaneous (delta-coupled)postsynaptic responses and homogeneous inhibitory coupling",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_239",
    "chunk_position": 239
  },
  {
    "id": "2304.00112v2",
    "text": ". Wendinparticularthatinhibitorymultiplicative coupling can be transferred to additive coupling and vice versa by simultaneously modifying the neuron potential, that is, the free neuron dynamics, so that the resulting spiking dynamics are identical. The coupling parameter of the new coupling may be chosen freely, if the neurons rise function is selected accordingly. We discuss some peculiarities of the transformed neuron models in detail, such as non-zero reset voltages, and suggest approximations to avoid these",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_240",
    "chunk_position": 240
  },
  {
    "id": "2304.00112v2",
    "text": ". We discuss some peculiarities of the transformed neuron models in detail, such as non-zero reset voltages, and suggest approximations to avoid these. The between additive and multiplicative coupling holds also for inhomogeneous coupling as well as networked systems with intricate interaction topology, if all coupling strengths are transformed by the same mapping (for every receiving neuron). To illustrate the range of applicability, we simulate two dierent collective phenomena both for models of additive and multiplicative coupling",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_241",
    "chunk_position": 241
  },
  {
    "id": "2304.00112v2",
    "text": ". To illustrate the range of applicability, we simulate two dierent collective phenomena both for models of additive and multiplicative coupling. The rst example supportsk-winner-takes-all computations via symmetrically all-to-all coupled neuron networks; the second topologydependent synchronization for inhibitory pulse coupling with delays. The ndings explicate that indeed exactly the same collective dynamics can be generated by models with either type of coupling",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_242",
    "chunk_position": 242
  },
  {
    "id": "2304.00112v2",
    "text": ". The ndings explicate that indeed exactly the same collective dynamics can be generated by models with either type of coupling. Also, we exemplify that slight manipulations of the transformed neuron potentials can allow more eective realization, often without changing the resulting collective dynamics at all. These results not only advance the theoretical foundations for modeling spiking neural networks but may also help transfering knowledge about systems with additive to systems with multiplicative coupling and vice versa",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_243",
    "chunk_position": 243
  },
  {
    "id": "2304.00112v2",
    "text": ". Such complementary results may in particular ease software or hardware implementations of spiking neural networks for dierent purposes. 1 G. D. Birkho, Dynamical systems , 9 (American Mathematical Soc., 1927). 2 S. H. Strogatz, Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering (CRC press, 2018). 3 D. Hansel, G. Mato, and C. Meunier, Synchrony in Excitatory Neural Networks, Neural Comput. 7, 307 (1995). 4 P. Ashwin and J. Borresen, Encoding via conjugate symmetries of slow oscillations for globally coupled oscillators, Phys. Rev",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_244",
    "chunk_position": 244
  },
  {
    "id": "2304.00112v2",
    "text": ". 7, 307 (1995). 4 P. Ashwin and J. Borresen, Encoding via conjugate symmetries of slow oscillations for globally coupled oscillators, Phys. Rev. E 70, 026203 (2004). 5 R. E. Mirollo and S. H. Strogatz, Synchronization of Pulse-Coupled Biological Oscillators, SIAM J. Appl. Math.50, 1645 (1990). 6 W. Maass, Networks of spiking neurons: The third generation of neural network models, Neural Netw. 10, 1659 (1997). 7 Z. Wang, Y. Ma, F. Cheng, and L. Yang, Review ofpulse-coupled neural networks, Image Vis. Comput. 28, 5 (2010). 8 E. Yanmaz, S. Yahyanejad, B. Rinner, H. Hellwagner, and C",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_245",
    "chunk_position": 245
  },
  {
    "id": "2304.00112v2",
    "text": ". Yang, Review ofpulse-coupled neural networks, Image Vis. Comput. 28, 5 (2010). 8 E. Yanmaz, S. Yahyanejad, B. Rinner, H. Hellwagner, and C. Bettstetter, Drone networks: Communications, coordination, and sensing, Ad Hoc Netw. 68, 1 (2018). 9 J. Klinglmayr, C. Kirst, C. Bettstetter, and M. Timme, Guaranteeing global synchronization in networks with stochastic interactions, New J. Phys. 14, 073031 (2012). 10 J. Klinglmayr, C. Bettstetter, M. Timme, and C. Kirst, Convergence of self-organizing pulse-coupled oscillator synchronization in dynamic networks, IEEE Trans. Automat. Contr",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_246",
    "chunk_position": 246
  },
  {
    "id": "2304.00112v2",
    "text": ". Timme, and C. Kirst, Convergence of self-organizing pulse-coupled oscillator synchronization in dynamic networks, IEEE Trans. Automat. Contr. 62, 1606 (2016). 11 J. Milnor, On the concept of attractor, Commun. Math. Phys.99, 177 (1985). 12 M.Timme, F.Wolf,and T.Geisel,Prevalenceofunstable attractors in networks of pulse-coupled oscillators, Phys. Rev. Lett. 89, 154105 (2002). 12 13 P. Ashwin and M. Timme, Unstable attractors: Existence and robustness in networks of oscillators with delayed pulse coupling, Nonlinearity 18, 2035 (2005). 14 M. Timme, F. Wolf, and T",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_247",
    "chunk_position": 247
  },
  {
    "id": "2304.00112v2",
    "text": ". 14 M. Timme, F. Wolf, and T. Geisel, Topological speed limits to network synchronization, Phys. Rev. Lett. 92, 074101 (2004). 15 T. Van Vu and K. Saito, Topological speed limit, Phys. Rev. Lett. 130, 010402 (2023). 16 H. Kielblock, C. Kirst, and M. Timme, Breakdown of order preservation in symmetric oscillator networks with pulse-coupling, Chaos 21, 025113 (2011). 17 P. Li, W. Lin, and K. Efstathiou, Isochronous dynamics in pulse coupled oscillator networks with delay, Chaos 27, 053103 (2017). 18 A. L. Hodgkin and A. F",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_248",
    "chunk_position": 248
  },
  {
    "id": "2304.00112v2",
    "text": ". Lin, and K. Efstathiou, Isochronous dynamics in pulse coupled oscillator networks with delay, Chaos 27, 053103 (2017). 18 A. L. Hodgkin and A. F. Huxley, A quantitative description of membrane current and its application to conduction and excitation in nerve, J. Physiol. (Lond.) 117, 500 (1952). 19 A. T. Winfree, Biological rhythms and the behavior of populations of coupled oscillators, J. Theor. Biol. 16, 15 (1967). 20 B. Ermentrout, Type i membranes, phase resetting curves, and synchrony, Neural Comput. 8, 979 (1996). 21 R. Brette, M. Rudolph, T. Carnevale, M. Hines, D. Beeman, J. M",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_249",
    "chunk_position": 249
  },
  {
    "id": "2304.00112v2",
    "text": ". 8, 979 (1996). 21 R. Brette, M. Rudolph, T. Carnevale, M. Hines, D. Beeman, J. M. Bower, M. Diesmann, A. Morrison, P. H. Goodman, F. C. Harris, M. Zirpe, T. Natschlger, D. Pecevski, B. Ermentrout, M. Djurfeldt, A. Lansner, O. Rochel, T. Vieville, E. Muller, A. P. Davison, S. El Boustani, and A. Destexhe, Simulation of networks of spiking neurons: A review of tools and strategies, J. Comput. Neurosci. 23, 349 (2007). 22 F. S. Neves and M. Timme, Recongurable Computation in Spiking Neural Networks, IEEE Access 8, 179648(2020). 23 L. Wang, X. Fang, D. Liu, and S",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_250",
    "chunk_position": 250
  },
  {
    "id": "2304.00112v2",
    "text": ". 22 F. S. Neves and M. Timme, Recongurable Computation in Spiking Neural Networks, IEEE Access 8, 179648(2020). 23 L. Wang, X. Fang, D. Liu, and S. Duan, Memristive lif spiking neuron model and its application in morse code, Front. Neurosci. , 374 (2022). 24 U.Ernst, K.Pawelzik,and T.Geisel,Synchronization Induced by Temporal Delays in Pulse-Coupled Oscillators, Phys. Rev. Lett. 74, 1570 (1995). 25 U. Ernst, K. Pawelzik, and T. Geisel, Delay-induced multistable synchronization of biological oscillators, Phys. Rev. E57, 2150 (1998). 26 R.-M. Memmesheimer and M",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_251",
    "chunk_position": 251
  },
  {
    "id": "2304.00112v2",
    "text": ". Geisel, Delay-induced multistable synchronization of biological oscillators, Phys. Rev. E57, 2150 (1998). 26 R.-M. Memmesheimer and M. Timme, Designing the Dynamics of Spiking Neural Networks, Phys. Rev. Lett. 97, 188101 (2006). 27 L. F. Abbott and C. van Vreeswijk, Asynchronous states in networks of pulse-coupled oscillators, Phys. Rev. E 48, 1483 (1993). 28 R.-M. Memmesheimer and M. a. Timme, Stable and unstableperiodicorbitsincomplexnetworksofspikingneuronswithdelays,Discrete Contin.Dyn.Syst.-A 28,1555 (2010). 29 M. Timme and F",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_252",
    "chunk_position": 252
  },
  {
    "id": "2304.00112v2",
    "text": ". a. Timme, Stable and unstableperiodicorbitsincomplexnetworksofspikingneuronswithdelays,Discrete Contin.Dyn.Syst.-A 28,1555 (2010). 29 M. Timme and F. Wolf, The simplest problem in the collective dynamics of neural networks: Is synchrony stable?, Nonlinearity 21, 1579 (2008), 0810.4472. 30 M. Timme, F. Wolf, and T. Geisel, Coexistence of Regular and Irregular Dynamics in Complex Networks of Pulse-Coupled Oscillators, Phys. Rev. Lett. 89, 258701 (2002). 31 M. Timme, Does dynamics reect topology in directed networks?, Europhysics Letters 76, 367 (2006).",
    "source": "arXiv",
    "chunk_id": "2304.00112v2_253",
    "chunk_position": 253
  },
  {
    "id": "2308.08218v2",
    "text": "Expressivity of Spiking Neural Networks Manjot Singh SINGH MATH .LMU .DE Adalbert Fono FONO MATH .LMU .DE Gitta Kutyniok KUTYNIOK MATH .LMU .DE Ludwig-Maximilians-Universit at M unchen Abstract The synergy between spiking neural networks and neuromorphic hardware holds promise for the development of energy-efficient AI applications. Inspired by this potential, we revisit the foundational aspects to study the capabilities of spiking neural networks where information is encoded in the firing time of neurons",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_254",
    "chunk_position": 254
  },
  {
    "id": "2308.08218v2",
    "text": ". Under the Spike Response Model as a mathematical model of a spiking neuron with a linear response function, we compare the expressive power of artificial and spiking neural networks, where we initially show that they realize piecewise linear mappings. In contrast to Re LU networks, we prove that spiking neural networks can realize both continuous and discontinuous functions. Moreover, we provide complexity bounds on the size of spiking neural networks to emulate multi-layer (Re LU) neural networks",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_255",
    "chunk_position": 255
  },
  {
    "id": "2308.08218v2",
    "text": ". Moreover, we provide complexity bounds on the size of spiking neural networks to emulate multi-layer (Re LU) neural networks. Restricting to the continuous setting, we also establish complexity bounds in the reverse direction for one-layer spiking neural networks. Keywords: Expressivity, Approximation Theory, Spiking Neural Networks, Deep (Re LU) Neural Networks, Temporal Coding, Linear Regions 1. Introduction Spiking Neural Networks (SNNs), sometimes considered as the third generation of neural networks, have recently emerged as a notable paradigm in neural computing",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_256",
    "chunk_position": 256
  },
  {
    "id": "2308.08218v2",
    "text": ". In traditional artificial neural networks (ANNs), information is propagated synchronously through the network, whereas SNNs are based on asynchronous information transmission in the form of an action-potential or a spike . Spikes can be considered as point-like events in time, where incoming spikes received via a neurons synapses trigger new spikes in the outgoing synapses. Hence, a key difference between ANNs and SNNs lies in the significance of timing in the operation of SNNs",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_257",
    "chunk_position": 257
  },
  {
    "id": "2308.08218v2",
    "text": ". Hence, a key difference between ANNs and SNNs lies in the significance of timing in the operation of SNNs. Moreover, the (typically) real-valued input information to an SNN needs to be encoded in the form of spikes, necessitating a spike-based encoding scheme. Different encoding schemes enable spiking neurons to represent real-valued inputs, broadly categorized into rate and temporal coding (Gerstner and van Hemmen, 1993). Rate coding refers to the number of spikes in a given time period whereas in temporal coding, the precise timing of spikes matters (Maass, 2001)",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_258",
    "chunk_position": 258
  },
  {
    "id": "2308.08218v2",
    "text": ". Rate coding refers to the number of spikes in a given time period whereas in temporal coding, the precise timing of spikes matters (Maass, 2001). The notion of firing rate adheres to neurobiological experiments where it was observed that some neurons fire frequently in response to some external stimuli (Stein, 1967; Gerstner et al., 2014). However, the firing rate results in high latency and is computationally expensive due to an overhead related to temporal averaging",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_259",
    "chunk_position": 259
  },
  {
    "id": "2308.08218v2",
    "text": ". However, the firing rate results in high latency and is computationally expensive due to an overhead related to temporal averaging. The latest experimental results indicate that the firing time of a neuron is essential for the system to respond fast to more complex sensory stimuli (Hopfield, 1995; Thorpe et al., 1996; Abeles, 1991). With the firing time, each spike carries a significant amount of information, thus the resulting signal can be quite sparse.",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_260",
    "chunk_position": 260
  },
  {
    "id": "2308.08218v2",
    "text": ". With the firing time, each spike carries a significant amount of information, thus the resulting signal can be quite sparse. . Contribution 1ar Xiv:2308.08218v2 cs.NE 15 Mar 2024 While there is no general consensus on the description of neural coding, in this work, we assume that the information is encoded exclusively in the firing time of a neuron",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_261",
    "chunk_position": 261
  },
  {
    "id": "2308.08218v2",
    "text": ". The event-driven, sparse information propagation, as seen in time-to-first-spike encoding (Gerstner and Kistler, 2002), facilitates system efficiency in terms of reduced computational power and improved energy efficiency in comparison to the substantial time and energy consumption associated with training and inferring on ANNs . This concept is particularly relevant in the context of neuromorphic computing , where a hardware architecture based on SNNs is designed to mimic the human brains structure and functioning to achieve efficient information processing",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_262",
    "chunk_position": 262
  },
  {
    "id": "2308.08218v2",
    "text": ". It is clear that the differences in the processing of information between ANNs and SNNs should also lead to differences in the computations performed by these models. Several groups have analyzed the expressive power of ANNs (Yarotsky, 2017; Cybenko, 1989; G uhring et al., 2022; Petersen and V oigtlaender, 2018), and in particular provided explanations for the superior performance of deep networks over shallow ones (Daubechies et al., 2022; Yarotsky, 2017)",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_263",
    "chunk_position": 263
  },
  {
    "id": "2308.08218v2",
    "text": ". In the case of ANNs with Re LU activation function, the number of linear regions into which the input space is partitioned is another property that highlights the advantages of deep networks over shallow ones. Unlike shallow networks, deep networks divide the input space into exponentially more linear regions (Goujon et al., 2024; Mont ufar et al., 2014) enabling them to express more complex functions",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_264",
    "chunk_position": 264
  },
  {
    "id": "2308.08218v2",
    "text": ". There exist further approaches to characterize the expressiveness of ANNs, e.g., the concept of VC-dimension in the context of classification problems (Bartlett et al., 1999; Goldberg and Jerrum, 1995; Bartlett et al., 2019). Few attempts have been made to understand the computational power of SNNs. The works by Maass (1996a,b) demonstrate the capability of spiking neurons to emulate Turing machines, arbitrary threshold circuits, and sigmoidal neurons in temporal coding",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_265",
    "chunk_position": 265
  },
  {
    "id": "2308.08218v2",
    "text": ". In Maass (1997), biologically relevant functions are depicted that can be emulated by a single spiking neuron but require complex ANNs to achieve the same task. Comsa et al. (2020), Maass (1995) showed that continuous functions can be approximated to arbitrary precision in temporal coding. A connection between SNNs and piecewise linear functions was noted in Mostafa (2018). The author showed that an SNN consisting of non-leaky integrate and fire neurons and temporal coding exhibits a piecewise linear input-output relation after a transformation of the time variable",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_266",
    "chunk_position": 266
  },
  {
    "id": "2308.08218v2",
    "text": ". A common theme is that the model of spiking neurons and the description of their dynamics varies, i.e., they are chosen and adjusted for a specific goal or task. St ockl and Maass (2021) aims at generating high-performance SNNs for image classification using a modified spiking neuron model that limits the number of spikes emitted by each neuron while considering precise spike timing. In Zhang and Zhou (2022), the authors investigate self-connection SNNs, demonstrating their capacity to efficiently approximate discrete dynamical systems. Moraitis et al",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_267",
    "chunk_position": 267
  },
  {
    "id": "2308.08218v2",
    "text": ". Moraitis et al. (2021) showcases the ability of SNNs using short-term spike-timing-dependent-plasticity mechanism to model certain dynamic environments. The primary challenge in advancing the field of SNNs has revolved around devising training methodologies. The typical approach is to either train from scratch (Lee et al., 2020; Wu et al., 2018; Comsa et al., 2020; G oltz et al., 2021) or convert trained ANNs into SNNs performing the same tasks (Rueckauer et al., 2017; Kim et al., 2018; Rueckauer and Liu, 2021, 2018; Stanojevic et al., 2022, 2023; Zhang et al., 2019; Yan et al., 2021)",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_268",
    "chunk_position": 268
  },
  {
    "id": "2308.08218v2",
    "text": ". The latter works concentrate on the construction of SNNs approximating or emulating given ANNs for various spike patterns, encoding schemes, and spiking neuron models. Therefore, we aim to extend the theoretical understanding of the differences and similarities in the expressive power between a network of spiking and artificial neurons employing a piecewise-linear activation function. 2 Contributions In this paper, to analyze SNNs, we employ the noise-free of the Spike Response Model (SRM) (Gerstner, 1995)",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_269",
    "chunk_position": 269
  },
  {
    "id": "2308.08218v2",
    "text": ". 2 Contributions In this paper, to analyze SNNs, we employ the noise-free of the Spike Response Model (SRM) (Gerstner, 1995). It describes the state of a neuron as a weighted sum of response and threshold functions. We assume a linear response function, where additionally each neuron spikes at most once to encode information through precise spike timing. The spiking networks based on the linear SRM are succinctly referred to as LSNNs",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_270",
    "chunk_position": 270
  },
  {
    "id": "2308.08218v2",
    "text": ". The spiking networks based on the linear SRM are succinctly referred to as LSNNs. The main results are centered around the comparison of expressive power between LSNNs and ANNs: Similarities between LSNNs and Re LU-ANNs: We show that LSNNs are as expressive as ANNs with piecewise activation when expressing various functions. We prove that the mapping generated by an LSNN is piecewise linear and under certain settings continuous, concave, and increasing. We show that there exists an LSNN that emulates the Re LU non-linearity",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_271",
    "chunk_position": 271
  },
  {
    "id": "2308.08218v2",
    "text": ". We show that there exists an LSNN that emulates the Re LU non-linearity. Then, we extend the result to multi-layer neural networks and show that LSNNs have the capacity to effectively emulate any (Re LU) ANN. Furthermore, we present explicit complexity bounds for constructing an LSNN capable of realizing an ANN. We also provide insights into the influence of the encoding scheme and the impact of different parameters on the above expressivity results. These findings imply that LSNNs can approximate any function as accurately as deep ANNs with a piecewise linear activation function",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_272",
    "chunk_position": 272
  },
  {
    "id": "2308.08218v2",
    "text": ". These findings imply that LSNNs can approximate any function as accurately as deep ANNs with a piecewise linear activation function. Differences between LSNNs and Re LU-ANNs: We prove distinctive characteristics of LSNNs that distinguish them from Re LU-ANNs, thus illustrating differences in the structure of computations between LSNNs and ANNs. We show that the mapping generated by LSNNs may be discontinuous which is in contrast to a Re LU-ANN, which outputs a continuous piecewise linear mapping",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_273",
    "chunk_position": 273
  },
  {
    "id": "2308.08218v2",
    "text": ". This suggests that LSNNs might be better suited for approximating realizing discontinuous piecewise functions. We demonstrate that the maximum number of linear regions that a one-layer LSNN generates scales exponentially with input dimension. Consequently, a shallow LSNN can be as expressive as a deep Re LU network in terms of the number of linear regions required to express certain types of continuous piecewise linear functions. Additionally, we give upper bounds on the size of Re LU-ANNs to emulate one-layer LSNNs",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_274",
    "chunk_position": 274
  },
  {
    "id": "2308.08218v2",
    "text": ". Additionally, we give upper bounds on the size of Re LU-ANNs to emulate one-layer LSNNs. Broader impact The findings presented herein deepen our understanding of the theoretical capabilities of SNNs and their differences from ANNs. Although we consider a simplified model of spiking dynamics within the LSNN framework, we obtain insights into event-driven computations where time plays a critical role",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_275",
    "chunk_position": 275
  },
  {
    "id": "2308.08218v2",
    "text": ". Moreover, our results further extend the understanding of the approximation properties of spiking neural networks, emphasizing their potential as an alternative computational model for handling complex tasks. By studying the theoretical power of SNNs, we aim to contribute to the realization of energy-efficient and low-power AI on neuromorphic hardware, providing viable options in contrast to established deep learning models. 3 Outline In , we introduce necessary definitions, including spiking neural networks and their realization under the Spike Response Model",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_276",
    "chunk_position": 276
  },
  {
    "id": "2308.08218v2",
    "text": ". 3 Outline In , we introduce necessary definitions, including spiking neural networks and their realization under the Spike Response Model. We present our main results in . We conclude in by summarizing the limitations and implications of our results. The proofs of all the results are provided in . 2. Spiking neural networks In neuroscience literature, several mathematical models exist that describe the generation and propagation of action-potentials. Action-potentials or spikes are short electrical pulses that are the result of electrical and biochemical properties of a biological neuron",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_277",
    "chunk_position": 277
  },
  {
    "id": "2308.08218v2",
    "text": ". Action-potentials or spikes are short electrical pulses that are the result of electrical and biochemical properties of a biological neuron. We refer to Gerstner et al. (2014) for a comprehensive and detailed introduction to the dynamics of spiking neurons. To study the expressivity of SNNs, the main principles of a spiking neuron are condensed into a (simplified) mathematical model, where certain details about the biophysics of a biological neuron are neglected. 2.1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_278",
    "chunk_position": 278
  },
  {
    "id": "2308.08218v2",
    "text": ". 2.1. Spiking neurons under Spike Response Model Following Maass (1996a), we consider the Spike Response Model (SRM) (Gerstner, 1995) as a formal model for a spiking neuron. It effectively captures the dynamics of the Hodgkin-Huxley model (Kistler et al., 1997; Gerstner et al., 2014), the most accurate model in describing neuronal dynamics, and is a generalized of the leaky integrate and fire model (Gerstner, 1995). The SRM leads to the subsequent definition of an SNN (Maass, 1996b)",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_279",
    "chunk_position": 279
  },
  {
    "id": "2308.08218v2",
    "text": ". The SRM leads to the subsequent definition of an SNN (Maass, 1996b). Definition 1 Aspiking neural network under the SRM is a (simple) finite directed graph (V, E) and consists of a finite set Vof spiking neurons, a subset Vin Vof input neurons, a subset Vout Vof output neurons, and a set EVVof synapses. Each synapse (u, v)Eis associated with a synaptic weight wuv0, asynaptic delay duv0, and a response function uv:RR, which depends on the synaptic delay",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_280",
    "chunk_position": 280
  },
  {
    "id": "2308.08218v2",
    "text": ". Each neuron v VVinis associated with a firing threshold v0, and a membrane potential Pv:RR, which is given by P X (u,v)EX tf u Fuwuvu, (1) where Futf u: 1fnfor some n Ndenotes the set of firing times of a neuron u, i.e., times twhenever Preaches ufrom below. In general, the membrane potential also includes the threshold function v:R0R0, that models the refractoriness effect. That is, if a neuron vemits a spike at time tf v,vcannot fire again for some time interval immediately after tf v, regardless of how large its potential might be",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_281",
    "chunk_position": 281
  },
  {
    "id": "2308.08218v2",
    "text": ". However, we assume that each neuron fires at most once, i.e., information is encoded in the firing time of single spikes. Thus, in Definition 1, the refractoriness effect can be ignored, and the contribution of vis modeled by the constant v. Moreover, the single spike condition simplifies (1) to P X (u,v)Ewuvu,where tuis the firing time of presynaptic neuron u. (2) The response function uvmodels the impact of a spike from a presynaptic neuron uon the membrane potential of a postsynaptic neuron",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_282",
    "chunk_position": 282
  },
  {
    "id": "2308.08218v2",
    "text": ". (2) The response function uvmodels the impact of a spike from a presynaptic neuron uon the membrane potential of a postsynaptic neuron . A biologically realistic approximation 4 ofuvis a delayed function (Gerstner, 1995), which is non-linear and leads to intractable problems when analyzing the propagation of spikes through an SNN. Hence, following Maass (1996a), we consider a simplified response and only require uvto satisfy the following condition: u ( 0, ift duv, duv, ,iftduv, duv,where s 1,1and 0",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_283",
    "chunk_position": 283
  },
  {
    "id": "2308.08218v2",
    "text": ". (3) The parameter is some constant assumed to be the length of a linear segment of the response function. The variable sreflects the fact that biological synapses are either excitatory orinhibitory and the synaptic delay duvis the time required for a spike to travel from utov",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_284",
    "chunk_position": 284
  },
  {
    "id": "2308.08218v2",
    "text": ". Inserting condition (3) in (2) and setting wuv:swuv, i.e., allowing wuvto take arbitrary values in R, yields P X (u,v)E10ttuduvwu (4) Using (4) enables us to iteratively compute the firing time tvof each neuron v VVinif we know the firing time tuof each neuron u Vwit Eby solving for tin inf t RP inf t RX (u,v)E10ttuduvwu v, (5) i.e., tvv P (u,v)E10tvtuduvwu P (u,v)E10tvtuduvwuv. Observe that tvis a weighted sum (up to a positive constant) of the firing times of neurons u,(u, v)E, actually contributing to the firing of v",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_285",
    "chunk_position": 285
  },
  {
    "id": "2308.08218v2",
    "text": ". Observe that tvis a weighted sum (up to a positive constant) of the firing times of neurons u,(u, v)E, actually contributing to the firing of v. For instance, if tzdzv tvfor some synapse (z, v)E, then zdid not influence the firing of vsince the spike from zarrived after v already fired. Depending on the firing time of the presynaptic neurons and the associated parameters (weights, delays, threshold), a specific subset of presynaptic neurons triggers the firing in vso that tvchanges accordingly. The dynamics of a neuron in this model are depicted in 1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_286",
    "chunk_position": 286
  },
  {
    "id": "2308.08218v2",
    "text": ". The dynamics of a neuron in this model are depicted in 1. Definition 2 We call an SNN based on the SRM with the additional assumptions (3)anan LSNN and the corresponding spiking neurons LSNN neurons . 2.2. Realizations of LSNNs A common representation of feedforward ANNs is based on a sequence of matrix-vector tuples , (Petersen and V oigtlaender, 2018), whereby a distinction between the network and the target function it realizes is established. Definition 3 Let L, N 0, . . . , N LN. Anartificial neural network is a sequence of matrix-vector tuples (( W1, B1),(W2, B2), . .",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_287",
    "chunk_position": 287
  },
  {
    "id": "2308.08218v2",
    "text": ". Definition 3 Let L, N 0, . . . , N LN. Anartificial neural network is a sequence of matrix-vector tuples (( W1, B1),(W2, B2), . . . , (WL, BL)), where each WRN1Nand BRN.N0and NLare the input and output dimension of . We call :PL j0Njthe number of neurons of the network , : Lthe number of layers ofand Nthe width of in layer . The realization ofwith component-wise activation function :RRis defined as the map R:RN0RNL,R(x) y L, where y Lresults from y0x, y ((W)Ty1B),for 1, . . . , L 1,andy L (WL)Ty L1BL.(6) 5 tu1 tu2. . . . .",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_288",
    "chunk_position": 288
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , L 1,andy L (WL)Ty L1BL.(6) 5 tu1 tu2. . . . . . tu5tv input neuronsoutput neuron (a) (b) 1: (a) An LSNN neuron vwith five input neurons u1, . . . , u 5that fire at times tu1, . . . , t u5, respectively. (b) The trajectory in black shows the evolution of the membrane potential Pof vas a result of incoming spikes (vertical arrows). Neurons u1andu2generate positive responses, whereas neurons u3andu5trigger negative responses, with the response magnitudes denoted by wuiv. The spike from neuron u4does not influence the firing time tvofvsince tv tu4du4v",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_289",
    "chunk_position": 289
  },
  {
    "id": "2308.08218v2",
    "text": ". The spike from neuron u4does not influence the firing time tvofvsince tv tu4du4v. Remark 4 Henceforth, (x) madenotes the Re LU activation . An analogous framework can be derived for LSNNs by arranging the underlying graph in layers and representing LSNNs by a sequence of their parameters. Definition 5 Let L, N 0, . . . , N LN. An LSNN associated to the acyclic graph (V, E)is a sequence of matrix-matrix-vector tuples (( W1, D1,1),(W2, D2,2), . . . , (WL, DL,L)) where each Wl RN1N,DRN1N 0, and RN 0",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_290",
    "chunk_position": 290
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , (WL, DL,L)) where each Wl RN1N,DRN1N 0, and RN 0. The matrix entries W uvand D uv represent the weight and delay value associated with the synapse (u, v)E, respectively, and the entry vis the firing threshold associated with node v Vin layer .N0is the input dimension and NLis the output dimension of . We call :PL j0Njthenumber of neurons and : Ldenotes the number of layers of",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_291",
    "chunk_position": 291
  },
  {
    "id": "2308.08218v2",
    "text": ". We call :PL j0Njthenumber of neurons and : Ldenotes the number of layers of. Before turning to the realization of LSNNs, we highlight two assumptions we will rely on, which allow us to analyze the LSNN framework in the most basic setting: Assumption I : The parameter describing the length of the linear segment of the response function introduced in (3) is assumed to be large or even infinite",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_292",
    "chunk_position": 292
  },
  {
    "id": "2308.08218v2",
    "text": ". Then the minimization problem in (5) to obtain the firing time tvof a neuron vsimplifies to inf t RP inf t RX (u,v)E1ttuduvwu inf t RX (u,v)Ewuv(ttuduv) 6 so that tvv P (u,v)E:tvtuduvwu P (u,v)E:tvtuduvwuv. (8) Informally, a large linear segment entails that spikes have a constant effect on postsynaptic neurons so that spikes do not act as point-like events in time",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_293",
    "chunk_position": 293
  },
  {
    "id": "2308.08218v2",
    "text": ". The obtained framework, which exhibits similarities to the integrate and fire model, enables us to assess the spiking dynamics of LSNN neurons and gain insights into what we can expect from more generalized models incorporating multi-spike responses and refractoriness effects. In contrast, a biologically more realistic small linear segment requires incoming spikes to have a correspondingly small time delay to jointly affect the potential of a neuron. Otherwise, the impact of the earlier spikes on the potential may already have vanished before the subsequent spikes arrive",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_294",
    "chunk_position": 294
  },
  {
    "id": "2308.08218v2",
    "text": ". Otherwise, the impact of the earlier spikes on the potential may already have vanished before the subsequent spikes arrive. The resulting model resembles the leaky integrate and fire model. In conclusion, incorporating as an additional parameter in the LSNN framework leads to additional complexity since the same firing patterns may result in different outcomes. However, an in-depth analysis of this effect is left as future work. Assumption II : The sum of incoming weights of each neuron vin an LSNN is assumed to be positive",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_295",
    "chunk_position": 295
  },
  {
    "id": "2308.08218v2",
    "text": ". Assumption II : The sum of incoming weights of each neuron vin an LSNN is assumed to be positive. The positivity ensures that each neuron in emits a spike, in particular, it is a sufficient but not a necessary condition to guarantee that spikes are emitted by the output neurons. One can certainly treat LSNNs without requiring that neurons have to spike, which again leads to augmented complexity due to increased flexibility in the model. Under the introduced conditions, the firing time of LSNN neurons can be considered as welldefined mappings in the following sense",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_296",
    "chunk_position": 296
  },
  {
    "id": "2308.08218v2",
    "text": ". Under the introduced conditions, the firing time of LSNN neurons can be considered as welldefined mappings in the following sense. Definition 6 Letbe an LSNN with input neurons u1, . . . , u dand output neurons v1, . . . , v n. For any firing time of the input neurons (tu1, . . . , t ud)TRdand the corresponding firing times of the output neurons (tv1, . . . , t vn)TRndetermined via (7), we denote by t:Rd Rn, (tu1, . . . , t ud)7t(tu1, . . . , t ud) (tv1, . . . , t vn)Tthefiring mapping of",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_297",
    "chunk_position": 297
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , t vn)TRndetermined via (7), we denote by t:Rd Rn, (tu1, . . . , t ud)7t(tu1, . . . , t ud) (tv1, . . . , t vn)Tthefiring mapping of. The key feature of any SNN is the asynchronous information propagation in the spiking domain due to variable firing times among neurons. Hence, to employ SNNs, the (typically real-valued) input information needs to be encoded in the firing times of the neurons in the input layer, and similarly, the firing times of the output neurons need to be translated back to an appropriate target domain",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_298",
    "chunk_position": 298
  },
  {
    "id": "2308.08218v2",
    "text": ". We will refer to this process as input encoding and output decoding. The applied encoding scheme certainly depends on the specific task at hand and the potential power and suitability of different encoding schemes is a topic that warrants separate investigation on its own. Our focus in this work lies on exploring the intrinsic capabilities of LSNNs, rather than the specifics of the encoding scheme. Thus, we can formulate some guiding principles for establishing a reasonable encoding scheme",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_299",
    "chunk_position": 299
  },
  {
    "id": "2308.08218v2",
    "text": ". Thus, we can formulate some guiding principles for establishing a reasonable encoding scheme. First, the firing times of input and output neurons should encode real-valued information in a consistent way so that different networks can be concatenated in a well-defined manner. This enables us to construct suitable subnetworks and combine them appropriately to solve more complex tasks. One can perform basic actions on neural networks such as concatenation and parallelization to construct larger networks from existing ones. Adapting a general approach for ANNs as defined in Berner et al",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_300",
    "chunk_position": 300
  },
  {
    "id": "2308.08218v2",
    "text": ". Adapting a general approach for ANNs as defined in Berner et al. (2022); Petersen and V oigtlaender (2018), we formally introduce the concatenation and parallelization of networks of spiking neurons in the .1. Second, in the extreme case, the encoding scheme might directly contain the solution to a problem, underscoring the need for a sufficiently simple and broadly applicable encoding scheme to avoid this. 7 Definition 7 Leta, bd Rdandbe an LSNN with input neurons u1, . . . , u dandnoutput neurons",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_301",
    "chunk_position": 301
  },
  {
    "id": "2308.08218v2",
    "text": ". 7 Definition 7 Leta, bd Rdandbe an LSNN with input neurons u1, . . . , u dandnoutput neurons. Fix reference times Tin Rdand Tout Rnvia Tinti Tand Tout tou T, respectively, where tin, tout Rwithtout t in. For any xa, bd, we set the firing times of the input neurons to (tu1, . . . , t ud)TTinx. The corresponding firing times of the output neurons t(tu1, . . . , t ud) Toutyencode the target y Rn. The realization of is defined as the map R:Rd Rn, R(x) Toutt(tu1, . . . , t ud) y. Remark 8 A bounded input range ensures that appropriate reference times can be fixed",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_302",
    "chunk_position": 302
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , t ud) y. Remark 8 A bounded input range ensures that appropriate reference times can be fixed. Note that the introduced encoding scheme translates real-valued information into input firing times in a continuous manner. Occasionally, we will point out the effect of adjusting the scheme. 3. Main results Subsequently, we will employ the framework introduced in to analyze the properties of LSNNs. First, we prove that LSNNs generate Continuous Piecewise Linear (CPWL) mappings under certain conditions on the weights",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_303",
    "chunk_position": 303
  },
  {
    "id": "2308.08218v2",
    "text": ". First, we prove that LSNNs generate Continuous Piecewise Linear (CPWL) mappings under certain conditions on the weights. Next, we show that LSNNs can emulate the realization of any multi-layer ANN employing Re LU as an activation function. We analyze the number of linear regions generated by LSNNs and compare the arising pattern to the well-studied case of Re LU-ANNs. Lastly, our findings show that LSNNs can efficiently realize certain CPWL functions using fewer computational units and layers compared to Re LU-ANNs",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_304",
    "chunk_position": 304
  },
  {
    "id": "2308.08218v2",
    "text": ". If not stated otherwise, the encoding scheme introduced in Definition 7 is applied and the results need to be understood concerning this specific encoding. 3.1. Characterization of functions expressed by LSNNs A broad class of ANNs based on a wide range of activation functions such as Re LU generate CPWL mappings (Dym et al., 2020; De V ore et al., 2021). In other words, these ANNs partition the input domain into regions, the so-called linear regions, on which an affine function represents the neural networks realization",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_305",
    "chunk_position": 305
  },
  {
    "id": "2308.08218v2",
    "text": ". Analyzing the firing mapping introduced in Definition 6, we find that LSNNs exhibit a similar behaviour although the continuity is not necessarily maintained. The proof of the statement can be found in .2. Theorem 9 Let (( W1, D1,1),(W2, D2,2), . . . , (WL, DL,L))be an LSNN. The firing mapping tis PWL. If additionally W uv X z:Wzv0W zv0for all andu, vwith W uv0 (9) holds, i.e., each incoming positive synaptic weight of any neuron vis larger than the absolute value of the sum of its incoming negative synaptic weights, then tis a CPWL mapping",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_306",
    "chunk_position": 306
  },
  {
    "id": "2308.08218v2",
    "text": ". Sketch of proof First, via (8) one can derive that the firing mapping of an LSNN neuron with arbitrarily many presynaptic neurons is PWL. Since consists of LSNN neurons arranged in layers it immediately follows that the firing map of each layer is PWL. Thus, as a composition of PWL mappings titself is PWL. Moreover, if all weights in are positive, then the continuity of tat 8 the breakpoints of the linear regions can be directly verified",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_307",
    "chunk_position": 307
  },
  {
    "id": "2308.08218v2",
    "text": ". Moreover, if all weights in are positive, then the continuity of tat 8 the breakpoints of the linear regions can be directly verified. In contrast, negative weights can under certain circumstances create a plateau or decrease the potential, causing a delay in firing, hence, resulting in a (jump-)discontinuity in t. However, this effect can be excluded via (9) so that tis continuous if (9) holds. The condition given in (9) is sufficient but not necessary to generate CPWL mappings; a corresponding example is provided in .2",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_308",
    "chunk_position": 308
  },
  {
    "id": "2308.08218v2",
    "text": ". The condition given in (9) is sufficient but not necessary to generate CPWL mappings; a corresponding example is provided in .2. Under stronger assumptions, we can further characterize the properties of LSNNs. The properties can be verified mainly by repeated application of (7) and (8); the detailed computations are presented in .2. Proposition 10 Letbe an LSNN with only positive weights. Then tis an increasing and concave function. Additionally, the firing time of a neuron vinwith corresponding parameter (w, d, )Rd Rd 0R0and firing times tu1, . .",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_309",
    "chunk_position": 309
  },
  {
    "id": "2308.08218v2",
    "text": ". Additionally, the firing time of a neuron vinwith corresponding parameter (w, d, )Rd Rd 0R0and firing times tu1, . . . , t ud Rin the previous layer is given by t inf Idn s I1P i Iwi X i Iw :s Imax i Ituio . The properties described in Proposition 10 are in general not true if the positive weights assumption is dropped. An immediate follow-up question is if the above findings apply to the realization of LSNNs via input and output encoding on a bounded domain",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_310",
    "chunk_position": 310
  },
  {
    "id": "2308.08218v2",
    "text": ". An immediate follow-up question is if the above findings apply to the realization of LSNNs via input and output encoding on a bounded domain. It is immediate to verify that the realization of an LSNN is CPWL, increasing or concave if the encoding scheme and tare CPWL, increasing or concave, respectively, which certainly holds for the encoding presented in Definition 7. However, even if tis not CPWL, increasing or concave, the corresponding feature can still arise in the realization due to the bounded input domain as the constructions in the subsequent results indicate",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_311",
    "chunk_position": 311
  },
  {
    "id": "2308.08218v2",
    "text": ". In particular, we show that LSNNs can realize the Re LU activation and as a consequence any multi-layer Re LU ANN. For the proof, please refer to A.3 and A.4 in the Appendix. Theorem 11 Let L, d N,a, bd Rdand let be an arbitrary ANN of depth Land fixed width demploying a Re LU non-linearity. Then, there exists an LSNN with (2d 2) and 3 L2that realizes Rona, bd. Sketch of proof Any multi-layer ANN with Re LU activation is simply an alternating composition of affine functions Adetermined by the weights Wand biases Bin layer and a non-linear function represented by",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_312",
    "chunk_position": 312
  },
  {
    "id": "2308.08218v2",
    "text": ". Thus, to construct an LSNN that realizes R, we first construct LSNNs that realize affine-linear functions and the Re LU non-linearity. Subsequently, we compose these subnetworks to obtain . Thereby, we realize Aby an LSNN with the same weights W, which may be negative. Therefore, in the LSNN construction, we rely on the value of the threshold parameter, which may depend on a, b, and auxiliary neurons with appropriate weights that ensure the firing of the output neurons and the desired (continuous) realization",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_313",
    "chunk_position": 313
  },
  {
    "id": "2308.08218v2",
    "text": ". Remark 12 The result can be generalized to ANNs with varying widths that employ any type of PWL activation function. We note that the encoding scheme that converts the analog values into the time domain plays a crucial role. We construct a two-layer LSNN that realizes via the encoding scheme ( Tin) and ( Tout). At the same time, the encoding scheme ( Tin ) and ( Tout ) fails in the two-layer case, whereas utilizing an inconsistent input and output encoding enables us to construct a one-layer LSNN that realizes",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_314",
    "chunk_position": 314
  },
  {
    "id": "2308.08218v2",
    "text": ". This shows that not only the network but also the applied encoding scheme is highly relevant. For details, we refer to .3. 9 It is well known that Re LU-ANNs not only realize CPWL mappings but that every CPWL function can be represented by Re LU-ANNs . Theorem 11 implies that LSNNs are as expressive as any Re LU-ANN, i.e., LSNNs can represent every Re LU-ANN and thereby every CPWL function with similar complexity. However, in a hypothetical real-world implementation, which certainly includes some noise, the constructed LSNN is not necessarily robust with respect to input perturbation",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_315",
    "chunk_position": 315
  },
  {
    "id": "2308.08218v2",
    "text": ". Additionally, the complexity of an LSNN can be captured in other ways than in terms of the number of computational units and layers, e.g., the total number of spikes emitted in LSNNs is related to its energy consumption since emitting spikes consumes energy. Hence, the minimum number of spikes needed to realize a given function class may be a reasonable complexity measure with regard to energy efficiency for SNNs",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_316",
    "chunk_position": 316
  },
  {
    "id": "2308.08218v2",
    "text": ". Further research in these directions is necessary to analyze the behaviour under noise and provide error estimations as well as to evaluate the complexity of LSNNs via different measures with their benefits and drawbacks. 3.2. Bounds on the complexity of Re LU-ANNs for expressing LSNNs In this section, we further explore the differences in the computational structure between LSNNs and Re LU-ANNs. An already observed major distinction is the ability of LSNNs to realize discontinuous functions",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_317",
    "chunk_position": 317
  },
  {
    "id": "2308.08218v2",
    "text": ". An already observed major distinction is the ability of LSNNs to realize discontinuous functions. Aside from this fact, can we establish dissimilarities when restricted to continuous realizations? Since Re LU-ANNs can represent any CPWL mapping, they can realize any LSNN with a CPWL realization, in particular, LSNNs with positive weights and a CPWL encoding scheme. Hence, the key difference in the realization of arbitrary CPWL mappings may be the necessary size and complexity of the respective ANN and LSNN",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_318",
    "chunk_position": 318
  },
  {
    "id": "2308.08218v2",
    "text": ". Hence, the key difference in the realization of arbitrary CPWL mappings may be the necessary size and complexity of the respective ANN and LSNN. To that end, we give upper bounds on the complexity of Re LU-ANNs needed to realize corresponding LSNNs. The first step in establishing the result is the study of the number of linear regions that LSNNs generate. The number of linear regions can be seen as a measure of the flexibility and expressivity of the associated CPWL function. Similarly, we can measure the expressivity of an ANN by the number of linear regions of its realization",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_319",
    "chunk_position": 319
  },
  {
    "id": "2308.08218v2",
    "text": ". Similarly, we can measure the expressivity of an ANN by the number of linear regions of its realization. The connection of the depth, width, and activation function of an ANN to the maximum number of its linear regions is well-established, e.g., with increasing depth the number of linear regions can grow exponentially in the number of parameters of an ANN (Mont ufar et al., 2014; Arora et al., 2018; Goujon et al., 2024). In the following, we observe a distinct scaling behaviour for LSNNs. Lemma 13 Letbe a one-layer LSNN with a single output neuron vand input neurons u1, . . . , u d",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_320",
    "chunk_position": 320
  },
  {
    "id": "2308.08218v2",
    "text": ". Lemma 13 Letbe a one-layer LSNN with a single output neuron vand input neurons u1, . . . , u d. Then tpartitions the input domain into at most 2d1linear regions. The maximal number of linear regions is attained if and only if all synaptic weights are positive. Proof By Theorem 9, we observe that tis a PWL mapping. It can be inferred via (8) that each linear region corresponds to a subset of input neurons responsible for the firing of von that specific domain. Hence, the number of regions is bounded by the number of non-empty subsets ofu1, . . . , u d, i.e., 2d1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_321",
    "chunk_position": 321
  },
  {
    "id": "2308.08218v2",
    "text": ". Hence, the number of regions is bounded by the number of non-empty subsets ofu1, . . . , u d, i.e., 2d1. Now, observe that any subset of input neurons causes a spike in vif and only if the sum of their weights is positive. Otherwise, inputs from the corresponding region cannot trigger a spike in vsince their net contribution is negative, i.e., the potential does not reach the threshold v. Hence, the maximal number of regions is attained if and only if all weights are positive, and thereby the sum of weights of any subset of input neurons is positive as well",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_322",
    "chunk_position": 322
  },
  {
    "id": "2308.08218v2",
    "text": ". A one-layer Re LU-ANN with one output neuron will partition the input domain into at most two linear regions, independent of the dimension of the input. In contrast, for a one-layer LSNN 10 with one output neuron, the maximum number of linear regions scales exponentially in the input dimension. In the case of LSNNs, non-linearity is an intrinsic property of the model and emerges from the subset of neurons that affect the firing time of the output neuron, whereas in ANNs a nonlinear activation is directly applied to the output neuron",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_323",
    "chunk_position": 323
  },
  {
    "id": "2308.08218v2",
    "text": ". By shifting the non-linearity and applying it to the input, ANNs could exhibit the same exponential scaling of the linear regions as LSNNs. However, this change has rather a detrimental effect on the expressivity since the partitioning of the input domain is fixed and independent of the parameters of the ANN. The flexibility of LSNNs to generate arbitrary linear regions is to a certain extent limited, albeit not entirely restricted as in the adjusted ANN; this is exemplarily demonstrated for a two-dimensional input space in .2",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_324",
    "chunk_position": 324
  },
  {
    "id": "2308.08218v2",
    "text": ". Analogously to the result in Theorem 11, a natural question is: what is the complexity of the Re LU ANN needed to emulate a given LSNN? The full power of ANN comes into play with large numbers of layers, however, the result in Lemma 13 suggests that a shallow LSNN can be as expressive as a deep Re LU network in terms of the number of linear regions. In the following, we give upper bounds on depth and number of computational units required for a Re LU-ANN to express a one-layer LSNN with ddimensional input. Theorem 14 Ford2,:log2(d 1) 1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_325",
    "chunk_position": 325
  },
  {
    "id": "2308.08218v2",
    "text": ". Theorem 14 Ford2,:log2(d 1) 1. Let be a one-layer LSNN with one output neuron vanddinput neurons u1, . . . , u dwithwuiv R0forid. Then, (a)tcan be realized by a Re LU-ANN with and . (b)tcan be realized by a Re LU-ANN with and . Proof Since consists of only positive weights, the firing map tis via Theorem 9 and Lemma 13 a CPWL mapping with 2d1linear regions. Using upper bounds on the size of Re LU-ANNs to realize CPWL mappings with a fixed number of linear regions, we obtain the given bounds. Thereby, the result (a) follows from Theorem 9 in Hertrich et al",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_326",
    "chunk_position": 326
  },
  {
    "id": "2308.08218v2",
    "text": ". Thereby, the result (a) follows from Theorem 9 in Hertrich et al. (2021), and (b) follows from Theorem 1 in Chen et al. (2022). The complexity bounds in (a) and (b) of Theorem 14 are connected to the number of linear regions of the CPWL function that the given LSNN realizes. In (a), to represent this CPWL function with lower depth, a substantial number of units in each layer might be necessary. Conversely, in (b), deep but less wide networks can achieve the same function",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_327",
    "chunk_position": 327
  },
  {
    "id": "2308.08218v2",
    "text": ". Conversely, in (b), deep but less wide networks can achieve the same function. To the best of our knowledge, we are not aware of any lower bounds on the depth of the Re LU-ANN in terms of the number of linear regions when expressing any CPWL function. Extending the bounds to multi-layer LSNNs is an important step and is left for future investigation. Via Theorem 11 and Theorem 14, we provide upper bounds on the size of LSNNs to realize Re LU networks and vice versa",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_328",
    "chunk_position": 328
  },
  {
    "id": "2308.08218v2",
    "text": ". Via Theorem 11 and Theorem 14, we provide upper bounds on the size of LSNNs to realize Re LU networks and vice versa. Although the bounds presented in Theorem 11 and Theorem 14 are not optimal, however, note that the bound on the size of Re LU-ANNs to emulate one-layer LSNNs grows exponentially in the input dimension whereas for LSNNs to emulate L-layer Re LUANNs, the bound scales linearly in the input dimension. These bounds (and their derivation) suggest that LSNNs and Re LU-ANNs offer distinct benefits for realizing certain types of CPWL functions",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_329",
    "chunk_position": 329
  },
  {
    "id": "2308.08218v2",
    "text": ". These bounds (and their derivation) suggest that LSNNs and Re LU-ANNs offer distinct benefits for realizing certain types of CPWL functions. These observations highlight the need to establish the associated lower bounds. This will not only shed light on the types of functions for which LSNNs are better suited in terms of emulation approximation capabilities than Re LU-ANNs but also provide valuable insights into their computational power. 11 4. Discussion The central aim of this paper is to study and compare the expressive power of SNNs and ANNs employing any PWL activation function",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_330",
    "chunk_position": 330
  },
  {
    "id": "2308.08218v2",
    "text": ". 11 4. Discussion The central aim of this paper is to study and compare the expressive power of SNNs and ANNs employing any PWL activation function. Our expressivity result in Theorem 11 implies that LSNNs can approximate any function with the same accuracy and a certain complexity overhead as (deep) ANNs employing a piecewise linear activation function, given the response function satisfies some basic assumptions. Most related to Theorem 11 are the results in Stanojevic et al. (2023)",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_331",
    "chunk_position": 331
  },
  {
    "id": "2308.08218v2",
    "text": ". Most related to Theorem 11 are the results in Stanojevic et al. (2023). Under certain assumptions, the authors define a one-to-one neuron mapping that converts a trained Re LU network to a corresponding SNN consisting of integrate and fire neurons by a non-linear transformation of parameters. However, significant distinctions exist between the approaches, particularly in terms of the chosen model, for instance, with the handling of the threshold parameter",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_332",
    "chunk_position": 332
  },
  {
    "id": "2308.08218v2",
    "text": ". In terms of methodology, we introduce an auxiliary neuron to ensure the firing of neurons even when a corresponding Re LU neuron exhibits zero activity. This diverges from their approach, which employs external current and a special parameter to achieve similar outcomes. We study the differences in the structure of computations between ANNs and SNNs, whereas in Stanojevic et al. (2023), only the conversion of ANNs to SNNs is examined and not vice versa",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_333",
    "chunk_position": 333
  },
  {
    "id": "2308.08218v2",
    "text": ". (2023), only the conversion of ANNs to SNNs is examined and not vice versa. Rather than approximating some function space by emulating a known construction for Re LU networks, one could also achieve optimal approximations by leveraging the intrinsic capabilities of LSNNs instead. The findings in Lemma 13 and Theorem 14 indicate that the latter approach may indeed be beneficial in terms of the complexity of the architecture in certain circumstances. However, we point out that finding optimal architectures for approximating different classes of functions is not the focal point of our work",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_334",
    "chunk_position": 334
  },
  {
    "id": "2308.08218v2",
    "text": ". However, we point out that finding optimal architectures for approximating different classes of functions is not the focal point of our work. The significance of our results lies in investigating theoretically the approximation and expressivity capabilities of SNNs, highlighting their potential as an alternative computational model for complex tasks. Extending the model of an LSNN neuron by incorporating, e.g., multiple spikes of a neuron, may yield an improvement in our results. However, by increasing the complexity of the model the analysis also tends to be more elaborate",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_335",
    "chunk_position": 335
  },
  {
    "id": "2308.08218v2",
    "text": ". However, by increasing the complexity of the model the analysis also tends to be more elaborate. In the aforementioned case of multiple spikes the threshold function becomes important so that additional complexity when approximating some target function is introduced since one would have to consider the coupled effect of response and threshold functions. Similarly, the choice of the response function and the frequency of neuron firings will surely influence the approximation results and we leave this for future work",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_336",
    "chunk_position": 336
  },
  {
    "id": "2308.08218v2",
    "text": ". Limitations We study similarities and differences in the structure of computations between ANNs and LSNNs and theoretically show that LSNNs can be as expressive as Re LU-ANNs. However, achieving similar results in practice heavily relies on the effectiveness of the employed training . The implementation of efficient learning with weights, delays, and thresholds as programmable parameters is left for future work. In this work, our choice of model resides on theoretical considerations and not on practical considerations regarding implementation",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_337",
    "chunk_position": 337
  },
  {
    "id": "2308.08218v2",
    "text": ". In this work, our choice of model resides on theoretical considerations and not on practical considerations regarding implementation. However, there might be other models of spiking neurons that are more apt for implementation purposes see e.g. Stanojevic et al. (2023) and Comsa et al. (2020). Furthermore, in reality, due to the ubiquitous sources of noise in the spiking neurons, the firing activity of a neuron is not deterministic. For mathematical simplicity, we perform our analysis in a noise-free case",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_338",
    "chunk_position": 338
  },
  {
    "id": "2308.08218v2",
    "text": ". For mathematical simplicity, we perform our analysis in a noise-free case. Generalizing to the case of noisy spiking neurons is important (for instance concerning the aforementioned implementation in noisy environments) and may lead to further insights into the model. 12 Acknowledgments The authors would like to thank Philipp Petersen for helpful discussion and feedback. M. Singh is supported by the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research. G",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_339",
    "chunk_position": 339
  },
  {
    "id": "2308.08218v2",
    "text": ". G. Kutyniok acknowledges support from LMUexcellent, funded by the Federal Ministry of Education and Research (BMBF) and the Free State of Bavaria under the Excellence Strategy of the Federal Government and the L ander as well as by the Hightech Agenda Bavaria. Further, G. Kutyniok was supported in part by the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research. G",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_340",
    "chunk_position": 340
  },
  {
    "id": "2308.08218v2",
    "text": ". G. Kutyniok also acknowledges support from the Munich Center for Machine Learning (MCML) as well as the German Research Foundation under Grants DFG-SPP-2298, KU 144631-1 and KU 144632-1 and under Grant DFG-SFBTR 109, Project C09 and the German Federal Ministry of Education and Research (BMBF) under Grant Ma Gri Do. References M. Abeles. Corticonics: Neural Circuits of the Cerebral Cortex . Cambridge University Press, 1991. Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural networks with rectified linear units. In ICLR 2018 , 2018",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_341",
    "chunk_position": 341
  },
  {
    "id": "2308.08218v2",
    "text": ". Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural networks with rectified linear units. In ICLR 2018 , 2018. Peter Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC dimension bounds for piecewise polynomial networks. Neural Computation , 10, 1999. Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VCdimension and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning Research , 20(63):117, 2019. Julius Berner, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_342",
    "chunk_position": 342
  },
  {
    "id": "2308.08218v2",
    "text": ". Journal of Machine Learning Research , 20(63):117, 2019. Julius Berner, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. The Modern Mathematics of Deep Learning. In Mathematical Aspects of Deep Learning , 1111. Cambridge University Press, 2022. 10.10179781009025096.002. Kuan-Lin Chen, Harinath Garudadri, and Bhaskar D. Rao. Improved bounds on neural complexity for representing piecewise linear functions. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Neur IPS 2022 , volume 35, 71677180. Curran Associates, Inc., 2022. Iulia M",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_343",
    "chunk_position": 343
  },
  {
    "id": "2308.08218v2",
    "text": ". Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Neur IPS 2022 , volume 35, 71677180. Curran Associates, Inc., 2022. Iulia M. Comsa, Krzysztof Potempa, Luca Versari, Thomas Fischbacher, Andrea Gesmundo, and Jyrki Alakuijala. Temporal coding in spiking neural networks with alpha synaptic function. In ICASSP 2020 , 85298533, 2020. 10.1109ICASSP40776.2020.9053856. George V . Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems , 2:303314, 1989. I. Daubechies, R. De V ore, S. Foucart, B. Hanin, and G. Petrova",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_344",
    "chunk_position": 344
  },
  {
    "id": "2308.08218v2",
    "text": ". Mathematics of Control, Signals and Systems , 2:303314, 1989. I. Daubechies, R. De V ore, S. Foucart, B. Hanin, and G. Petrova. Nonlinear approximation and (deep) Re LU networks. Constructive Approximation , 55(1):127172, 2022. 10.1007 s00365-021-09548-z. 13 Ronald De V ore, Boris Hanin, and Guergana Petrova. Neural network approximation. Acta Numerica, 30:327444, 2021. 10.1017S0962492921000052. Nadav Dym, Barak Sober, and Ingrid Daubechies. Expression of fractals through neural network functions. IEEE Journal on Selected Areas in Information Theory , 1(1):5766, 2020. 10",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_345",
    "chunk_position": 345
  },
  {
    "id": "2308.08218v2",
    "text": ". Expression of fractals through neural network functions. IEEE Journal on Selected Areas in Information Theory , 1(1):5766, 2020. 10. 1109JSAIT.2020.2991422. Wulfram Gerstner. Time structure of the activity in neural network models. Physical Review E , 51: 738758, 1995. Wulfram Gerstner and Werner M. Kistler. Spiking Neuron Models: Single Neurons, Populations, Plasticity . Cambridge University Press, 2002. Wulfram Gerstner and J. van Hemmen. How to describe neuronal activity: Spikes, rates, or assemblies? In J. Cowan, G. Tesauro, and J. Alspector, editors, NIPS 1993 , volume 6",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_346",
    "chunk_position": 346
  },
  {
    "id": "2308.08218v2",
    "text": ". How to describe neuronal activity: Spikes, rates, or assemblies? In J. Cowan, G. Tesauro, and J. Alspector, editors, NIPS 1993 , volume 6. MorganKaufmann, 1993. Wulfram Gerstner, Werner M. Kistler, Richard Naud, and Liam Paninski. Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition . Cambridge University Press, 2014. Paul W. Goldberg and Mark R. Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. Machine Learning , 18(2):131148, 1995. 10.1007 BF00993408. Alexis Goujon, Arian Etemadi, and Michael Unser",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_347",
    "chunk_position": 347
  },
  {
    "id": "2308.08218v2",
    "text": ". Machine Learning , 18(2):131148, 1995. 10.1007 BF00993408. Alexis Goujon, Arian Etemadi, and Michael Unser. On the number of regions of piecewise linear neural networks. Journal of Computational and Applied Mathematics , 441, 2024. 10.1016 j.cam.2023.115667. J. Goltz, L. Kriener, Andreas Baumbach, S. Billaudelle, O. Breitwieser, B. Cramer, Dominik Dold, Akos Kungl, Walter Senn, Johannes Schemmel, Karlheinz Meier, and M. Petrovici. Fast and energy-efficient neuromorphic deep learning with first-spike times. Nature Machine Intelligence , 3:823835, 2021. 10.1038s42256-021-00388-x",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_348",
    "chunk_position": 348
  },
  {
    "id": "2308.08218v2",
    "text": ". Fast and energy-efficient neuromorphic deep learning with first-spike times. Nature Machine Intelligence , 3:823835, 2021. 10.1038s42256-021-00388-x. Ingo G uhring, Mones Raslan, and Gitta Kutyniok. Expressivity of deep neural networks. In Philipp Grohs and Gitta Kutyniok, editors, Mathematical Aspects of Deep Learning , 149199. Cambridge University Press, 2022. Christoph Hertrich, Amitabh Basu, Marco Di Summa, and Martin Skutella. Towards lower bounds on the depth of relu neural networks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_349",
    "chunk_position": 349
  },
  {
    "id": "2308.08218v2",
    "text": ". Towards lower bounds on the depth of relu neural networks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Neur IPS 2021 , volume 34, 33363348. Curran Associates, Inc., 2021. John Hopfield. Pattern recognition computation using action potential timing for stimulus representation. Nature , 376:336, 1995. 10.1038376033a0. Jaehyun Kim, Heesu Kim, Subin Huh, Jinho Lee, and Kiyoung Choi. Deep neural networks with weighted spikes. Neurocomputing , 311:373386, 2018. Werner M. Kistler, Wulfram Gerstner, and J. Leo van Hemmen",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_350",
    "chunk_position": 350
  },
  {
    "id": "2308.08218v2",
    "text": ". Deep neural networks with weighted spikes. Neurocomputing , 311:373386, 2018. Werner M. Kistler, Wulfram Gerstner, and J. Leo van Hemmen. Reduction of the Hodgkin-Huxley to a single-variable threshold model. Neural Computation , 9(5):10151045, 1997. 10.1162neco.1997.9.5.1015. 14 Chankyu Lee, Syed Shakib Sarwar, Priyadarshini Panda, Gopalakrishnan Srinivasan, and Kaushik Roy. Enabling spike-based backpropagation for training deep neural network architectures. Frontiers in Neuroscience , 14, 2020. 10.3389fnins.2020.00119. Wolfgang Maass",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_351",
    "chunk_position": 351
  },
  {
    "id": "2308.08218v2",
    "text": ". Frontiers in Neuroscience , 14, 2020. 10.3389fnins.2020.00119. Wolfgang Maass. An efficient implementation of sigmoidal neural nets in temporal coding with noisy spiking neurons. Technical report, Technische Universit at Graz, 1995. Wolfgang Maass. Noisy spiking neurons with temporal coding have more computational power than sigmoidal neurons. In NIPS 1996 , volume 9. MIT Press, 1996a. Wolfgang Maass. Lower bounds for the computational power of networks of spiking neurons. Neural Computation , 8(1):140, 1996b. 10.1162neco.1996.8.1.1. Wolfgang Maass",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_352",
    "chunk_position": 352
  },
  {
    "id": "2308.08218v2",
    "text": ". Neural Computation , 8(1):140, 1996b. 10.1162neco.1996.8.1.1. Wolfgang Maass. Networks of spiking neurons: The third generation of neural network models. Neural Networks , 10(9):16591671, 1997. 10.1016S0893-6080(97)00011-7. Wolfgang Maass. On the relevance of time in neural computation and learning. Theoretical Computer Science , 261(1):157178, 2001. 10.1016S0304-3975(00)00137-7. Guido Mont ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In NIPS 2014 , 29242932, Cambridge, MA, USA, 2014. MIT Press",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_353",
    "chunk_position": 353
  },
  {
    "id": "2308.08218v2",
    "text": ". On the number of linear regions of deep neural networks. In NIPS 2014 , 29242932, Cambridge, MA, USA, 2014. MIT Press. Timoleon Moraitis, Abu Sebastian, and Evangelos Eleftheriou. Optimality of short-term synaptic plasticity in modelling certain dynamic environments. ar Xiv:2009.06808 , 2021. Hesham Mostafa. Supervised learning based on temporal coding in spiking neural networks. IEEE Transactions on Neural Networks and Learning Systems , 29(7):32273235, 2018. 10.1109 TNNLS.2017.2726060. Philipp Petersen and Felix V oigtlaender",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_354",
    "chunk_position": 354
  },
  {
    "id": "2308.08218v2",
    "text": ". 10.1109 TNNLS.2017.2726060. Philipp Petersen and Felix V oigtlaender. Optimal approximation of piecewise smooth functions using deep relu neural networks. Neural Networks , 108:296330, 2018. 10.1016j.neunet. 2018.08.019. Bodo Rueckauer and Shih-Chii Liu. Conversion of analog to spiking neural networks using sparse temporal coding. In ISCAS 2018 , 15, 2018. 10.1109ISCAS.2018.8351295. Bodo Rueckauer and Shih-Chii Liu. Temporal pattern coding in deep spiking neural networks. In IJCNN 2021 , 18, 2021. 10.1109IJCNN52387.2021.9533837",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_355",
    "chunk_position": 355
  },
  {
    "id": "2308.08218v2",
    "text": ". Bodo Rueckauer and Shih-Chii Liu. Temporal pattern coding in deep spiking neural networks. In IJCNN 2021 , 18, 2021. 10.1109IJCNN52387.2021.9533837. Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conversion of continuous-valued deep networks to efficient event-driven networks for image classification. Frontiers in Neuroscience , 11, 2017. 10.3389fnins.2017.00682. Catherine D. Schuman, Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, and Bill Kay. Opportunities for neuromorphic computing and applications",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_356",
    "chunk_position": 356
  },
  {
    "id": "2308.08218v2",
    "text": ". Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, and Bill Kay. Opportunities for neuromorphic computing and applications. Nature Computational Science , 2(1):1019, 2022. Ana Stanojevic, Evangelos Eleftheriou, Giovanni Cherubini, Stanisaw Wo zniak, Angeliki Pantazi, and Wulfram Gerstner. Approximating Re LU networks by single-spike computation. In ICIP 2022 , 19011905, 2022. 10.1109ICIP46576.2022.9897692. 15 Ana Stanojevic, Stanisaw Wo zniak, Guillaume Bellec, Giovanni Cherubini, Angeliki Pantazi, and Wulfram Gerstner. An exact mapping from Re LU networks to spiking neural networks",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_357",
    "chunk_position": 357
  },
  {
    "id": "2308.08218v2",
    "text": ". An exact mapping from Re LU networks to spiking neural networks. Neural Networks , 168:7488, 2023. 10.1016j.neunet.2023.09.011. R. B. Stein. The information capacity of nerve cells using a frequency code. Biophysical journal , 7 (6):797826, 1967. Christoph St ockl and Wolfgang Maass. Optimized spiking neurons can classify images with high accuracy through temporal coding with two spikes. Nature Machine Intelligence , 3:230238, 2021. 10.1038s42256-021-00311-4. Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_358",
    "chunk_position": 358
  },
  {
    "id": "2308.08218v2",
    "text": ". Nature Machine Intelligence , 3:230238, 2021. 10.1038s42256-021-00311-4. Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. Deep learnings diminishing returns: The cost of improvement is becoming unsustainable. IEEE Spectrum , 58 (10):5055, 2021. Simon Thorpe, Denis Fize, and Catherine Marlot. Speed of processing in the human visual system. Nature , 381(6582):520522, 1996. Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for training high-performance spiking neural networks. Frontiers in Neuroscience , 12, 2018",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_359",
    "chunk_position": 359
  },
  {
    "id": "2308.08218v2",
    "text": ". Spatio-temporal backpropagation for training high-performance spiking neural networks. Frontiers in Neuroscience , 12, 2018. 10.3389fnins.2018.00331. Zhanglu Yan, Jun Zhou, and Weng-Fai Wong. Near lossless transfer learning for spiking neural networks. In AAAI 2021 , volume 35, 1057710584, 2021. Dmitry Yarotsky. Error bounds for approximations with deep Re LU networks. Neural Networks , 94:103114, 2017. Lei Zhang, Shengyuan Zhou, Tian Zhi, Zidong Du, and Yunji Chen. TDSNN: From deep neural networks to deep spike neural networks with temporal-coding. In AAAI 2019 , volume 33, 13191326, 2019",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_360",
    "chunk_position": 360
  },
  {
    "id": "2308.08218v2",
    "text": ". TDSNN: From deep neural networks to deep spike neural networks with temporal-coding. In AAAI 2019 , volume 33, 13191326, 2019. 10.1609aaai.v33i01.33011319. Shao-Qun Zhang and Zhi-Hua Zhou. Theoretically provable spiking neural networks. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Neur IPS 2022 , volume 35, 1934519356. Curran Associates, Inc., 2022. 16 . Proofs Outline We start by introducing the spiking network calculus in .1 to compose and parallelize different networks. In .2, we characterize the firing maps of LSNNs",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_361",
    "chunk_position": 361
  },
  {
    "id": "2308.08218v2",
    "text": ". In .2, we characterize the firing maps of LSNNs. In particular, we show that the firing maps of LSNNs are PWL and under stronger assumptions continuous, increasing, and concave. In .3, we construct an LSNN that emulates the Re LU non-linearity, and subsequently in .4, we prove that an LSNN can realize the output of any Re LU network and simultaneously provide bounds on the required size of the LSNN. A.1. Spiking neural network calculus It can be observed from Definition 7 that both inputs and outputs of LSNNs are encoded in a unified format",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_362",
    "chunk_position": 362
  },
  {
    "id": "2308.08218v2",
    "text": ". A.1. Spiking neural network calculus It can be observed from Definition 7 that both inputs and outputs of LSNNs are encoded in a unified format. This characteristic is crucial for concatenatingparallelizing two spiking network architectures that further enable us to attain compositions of network realizations. We operate in the following setting: Let L1,L2, d1, d2, d 1, d 2N. Consider two LSNNs 1, 2given by i ((Wi 1, Di 1,i 1), . . . , (Wi Li, Di Li,i Li)), i 1,2, with input domains a1, b1d1Rd1,a2, b2d2Rd2and output dimension d 1, d 2, respectively. Denote the input neurons by u1, . .",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_363",
    "chunk_position": 363
  },
  {
    "id": "2308.08218v2",
    "text": ". Denote the input neurons by u1, . . . , u diwith respective firing times ti uj. By Definition 6 and 7, we can express the firing times of the input neurons as t1 : ( t1 u1, . . . , t1 ud1)TT1 inxforxa1, b1d1, t2 : ( t2 u1, . . . , t2 ud2)TT2 inxforxa2, b2d2(10) and the realization of the networks as R1(x) T1 outt1(t1 ) forxa1, b1d1, R2(x) T2 outt2(t2 ) forxa2, b2d2(11) for some constants T1 in Rd1,T2 in Rd2,T1 out Rd 1,T2 out Rd 2. We define the concatenation of the two networks in the following way",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_364",
    "chunk_position": 364
  },
  {
    "id": "2308.08218v2",
    "text": ". We define the concatenation of the two networks in the following way. Definition 15 (Concatenation) Let 1and2be such that the input layer of 1has the same dimension as the output layer of 2, i.e., d 2d1. Then, the concatenation of 1and2, denoted as12, represents the (L1L2)-layer network 12: ((W2 1, D2 1,2 1), . . . , (W2 L2, D2 L2,2 L2),(W1 1, D1 1,1 1), . . . , (W1 L1, D1 L1,1 L1)). Lemma 16 Letd 2d1and fix Tin T2 inand Tout T1 out. If T2 out T1 inand R2(a2, b2d2) a1, b1d1, then R12(x) R1(R2(x))for all xa, bd2 with respect to the reference times Tin, Tout",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_365",
    "chunk_position": 365
  },
  {
    "id": "2308.08218v2",
    "text": ". If T2 out T1 inand R2(a2, b2d2) a1, b1d1, then R12(x) R1(R2(x))for all xa, bd2 with respect to the reference times Tin, Tout. Moreover, 12is composed of d1 computational units. 17 Proof It is straightforward to verify via the construction that the network 12is composed of d1computational units. Moreover, under the given assumptions R1 R2is well-defined so that (10) and (11) imply R12(x) Toutt1(t2(Tinx)) T1 outt1(t2(T2 inx)) T1 outt1(t2(t2 )) T1 outt1(T2 out R2(x)) T1 outt1(T1 in R2(x)) T1 outt1(t1 )) R1(R2(x)) forxa2, b2d2",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_366",
    "chunk_position": 366
  },
  {
    "id": "2308.08218v2",
    "text": ". In addition to concatenating networks, we also perform parallelization operation on LSNNs. Definition 17 (Parallelization) Let 1and2be such that they have the same depth and input dimension, i.e., L1L2:Landd1d2:d. Then, the parallelization of 1and2, denoted as, represents the L-layer network with d-dimensional input : (( W1,D1,1), . . . , (WL,DL,L)), where W1 W1 1W2 1 ,D1 D1 1D2 1 ,1 1 1 2 1! and Wl W1 l0 0W2 l ,Dl D1 l0 0D2 l ,l 1 l 2 l! ,for1 l L. Lemma 18 Letd:d2d1and fix Tin:T1 in,Tout: (T1 out, T2 out),a:a1andb:b1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_367",
    "chunk_position": 367
  },
  {
    "id": "2308.08218v2",
    "text": ". Lemma 18 Letd:d2d1and fix Tin:T1 in,Tout: (T1 out, T2 out),a:a1andb:b1. If T2 in T1 in,T2 out T1 outanda1a2,b1b2, then R(x) (R1(x),R2(x)) forxa, bd with respect to the reference times Tin, Tout. Moreover, is composed of dcomputational units. Proof The number of computational units is an immediate consequence of the construction. Since the input domains of 1and2agree, (10) and (11) show that R(x) Tout (t1(Tinx), t2(Tinx)) (T1 outt1(T1 inx),T2 outt2(T2 inx)) (T1 outt1(t1 ),T2 outt2(t2 )) (R1(x),R2(x)) forxa, bd",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_368",
    "chunk_position": 368
  },
  {
    "id": "2308.08218v2",
    "text": ". Remark 19 Note that parallelization and concatenation can be straightforwardly extended (recursively) to a finite number of networks. Additionally, more general forms of parallelization and concatenations of networks, e.g., parallelization of networks with different depths, can be established. However, for the constructions presented in this work, the introduced notions suffice. 18 A.2. Characterization of functions expressed by LSNNs A.2.1. S PIKING NEURON WITH TWO INPUTS First, we provide a simple toy example to demonstrate the dynamics of an LSNN neuron",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_369",
    "chunk_position": 369
  },
  {
    "id": "2308.08218v2",
    "text": ". S PIKING NEURON WITH TWO INPUTS First, we provide a simple toy example to demonstrate the dynamics of an LSNN neuron. Let v be an LSNN neuron with two input neurons u1, u2. Denote the associated weights and delays by wuiv Randduiv0, respectively, and the threshold of vbyv0. A spike emitted from v could then be caused by either u1oru2or a combination of both. Each possibility corresponds to a linear region in the input space R2",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_370",
    "chunk_position": 370
  },
  {
    "id": "2308.08218v2",
    "text": ". Each possibility corresponds to a linear region in the input space R2. We consider each case separately under Assumption I, i.e., in (3) is arbitrarily large, and we discuss the implications of this assumption in more detail after presenting the different cases. Case 1 :u1causes vto spike before a potential effect from u2reaches v. Note that this can only happen if wu1v0and tu2du2vtvv wu1vtu1du1v, where we applied (7) and (8), and tzrepresents the firing time of a neuron z. Solving for tu2leads to tu2v wu1vtu1du1vdu2v",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_371",
    "chunk_position": 371
  },
  {
    "id": "2308.08218v2",
    "text": ". Solving for tu2leads to tu2v wu1vtu1du1vdu2v. Case 2 : An analogous calculation shows that tu2 v wu2vtu1du1vdu2v, whenever u2causes vto spike before a potential effect from u1reaches v. Case 3 : The remaining possibility is that spikes from u1andu2influence the firing time of v. Then, the following needs to hold: wu1vwu2v0and tu1du1v tvv wu1vwu2v X iwuiv wu1vwu2and tu2du2v tvv wu1vwu2v X iwuiv wu1vwu2. This yields tu2 v wu2vtu1du1vdu2v,ifwu2v wu1vwu2v0 v wu2vtu1du1vdu2v,ifwu2v wu1vwu2v0, respectively tu2 v wu1vtu1du1vdu2v,ifwu1v wu1vwu2v0 v wu1vtu1du1vdu2v,ifwu1v wu1vwu2v0",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_372",
    "chunk_position": 372
  },
  {
    "id": "2308.08218v2",
    "text": ". 19 Example 1 In a simple setting with vwuivdu2v 1anddu1v 2, the above considerations imply the following firing time of von the corresponding linear regions (see 2): tv tu1 3, iftu2tu1 2 tu2 2, iftu2tu1 1 2(tu1tu2) 2,iftu1 tu2 tu1 2. (a) (b) 2: Illustration of Example 1. It shows that the output firing time tas a function of inputs tu1, tu2is a CPWL mapping. (a) An illustration of the partitioning of the input space into three different regions. (b) Each region is associated with an affine-linear mapping. Already this simple setting with two-dimensional inputs provides crucial insights",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_373",
    "chunk_position": 373
  },
  {
    "id": "2308.08218v2",
    "text": ". (b) Each region is associated with an affine-linear mapping. Already this simple setting with two-dimensional inputs provides crucial insights. The actual number of linear regions in the input domain corresponds to the parameter of the LSNN neuron v. In particular, the maximum number of linear regions, i.e. three, can only occur if both weights wuiv are positive. Similarly, vdoes not fire at all if both weights are non-positive, which motivates the condition in Assumption II. The exact number of linear regions depends on the sign and magnitude of the weights",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_374",
    "chunk_position": 374
  },
  {
    "id": "2308.08218v2",
    "text": ". The exact number of linear regions depends on the sign and magnitude of the weights. Furthermore, note that the linear regions are described by hyperplanes of the form tu2tu1Cp,u, (12) where Cp,uis a constant depending on the parameter pcorresponding to v, i.e., threshold, delays and weights, and the actual input neuro causing vto spike. Hence, phas only a limited effect on the boundary of a linear region; depending on their exact value, the parameter only introduces an additive constant shift",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_375",
    "chunk_position": 375
  },
  {
    "id": "2308.08218v2",
    "text": ". Remark 20 Dropping the assumption that is arbitrarily large in (3)yields an evolved model that is also biologically more realistic. The magnitude of describes the duration in which an incoming spike influences the membrane potential of a neuron. By setting arbitrarily large, we 20 generally consider an incoming spike to have a lasting effect on the membrane potential. Specifying a fixed increases the importance of the timing of the individual spikes as well as the choice of the parameter",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_376",
    "chunk_position": 376
  },
  {
    "id": "2308.08218v2",
    "text": ". Specifying a fixed increases the importance of the timing of the individual spikes as well as the choice of the parameter. For instance, inputs from certain regions in the input domain may not trigger a spike since the combined effect of multiple delayed incoming spikes is neglected. An in-depth analysis of the influence of is left as future work and we continue our analysis under the assumption that is arbitrarily large. A.2.2",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_377",
    "chunk_position": 377
  },
  {
    "id": "2308.08218v2",
    "text": ". An in-depth analysis of the influence of is left as future work and we continue our analysis under the assumption that is arbitrarily large. A.2.2. S PIKING NEURON WITH ARBITRARILY MANY INPUTS A significant observation in the two-dimensional case is that the firing time tas a function of the input tu1, tu2is a CPWL mapping. Indeed, each linear region is associated with an affine linear mapping and crucially these affine mappings agree at the breakpoints",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_378",
    "chunk_position": 378
  },
  {
    "id": "2308.08218v2",
    "text": ". Indeed, each linear region is associated with an affine linear mapping and crucially these affine mappings agree at the breakpoints. This intuitively makes sense since a breakpoint marks the event when the effect of an additional neuron on the firing time of vneeds to be taken into consideration or, , a neuron does not contribute to the firing of v anymore",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_379",
    "chunk_position": 379
  },
  {
    "id": "2308.08218v2",
    "text": ". However, in both circumstances, the effective contribution of this specific neuron is zero (and the contribution of the other neuron remains unchanged) at the breakpoint so that the crossing of a breakpoint and the associated change of a linear region does not result in a discontinuity. Formally, the class of CPWL functions describes functions that are globally continuous and locally linear on each polytope in a given finite decomposition of Rdinto polytopes. We refer to the polytopes as linear regions",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_380",
    "chunk_position": 380
  },
  {
    "id": "2308.08218v2",
    "text": ". We refer to the polytopes as linear regions. The insights obtained in the two-dimensional case do not straightforwardly extend to a d-dimensional input domain for d 2. Crucially, continuity may be lost as the following simple example shows. Example 2 Let v be an LSNN neuron with threshold v 1 and presynaptic neurons u1, u2, u3 with corresponding weights wu1v 1, wu2v1, wu3v 1, delays du1vdu2vdu3v 0, and firing times tu1 0, tu2 1 , tu3 2, respectively. One easily verifies via (7)that t ( 1, for0 2, for 0. Hence, tis not continuous since lim 0t lim 02 2 1 lim 0t",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_381",
    "chunk_position": 381
  },
  {
    "id": "2308.08218v2",
    "text": ". One easily verifies via (7)that t ( 1, for0 2, for 0. Hence, tis not continuous since lim 0t lim 02 2 1 lim 0t. However, we can show that an LSNN neuron generates a PWL mapping, which under certain conditions on its weights is in fact continuous. Lemma 21 Letvbe a LSNN neuron with with threshold v0and presynaptic neurons u1, . . . , u d with corresponding weights wuiv R, delays duiv0, and firing times tui R, respectively. Then the firing time tas a function of the firing times tu1, . . . , t udis a PWL mapping, and additionally continuous provided that wuiv X j:wujv0wujv0for all iwithwuiv0",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_382",
    "chunk_position": 382
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , t udis a PWL mapping, and additionally continuous provided that wuiv X j:wujv0wujv0for all iwithwuiv0. (13) Proof Recall that we operate under Assumption II, i.e., we presuppose that Pd i1wuiv0so that any input firing time (tu1, . . . , t ud)Rdnecessarily triggers a firing in v. In particular, the notion 21 oftvas a PWL mapping on Rdis well-defined. Moreover, for given tu1, . . . , t udwe can identify a subset I 1, . . . , d such that all uiwithi Icontribute to the firing of vwhereas spikes from ujwithj Ic1, . . . , d Ido not influence the firing of",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_383",
    "chunk_position": 383
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , d such that all uiwithi Icontribute to the firing of vwhereas spikes from ujwithj Ic1, . . . , d Ido not influence the firing of . Then P i Iwuivis required to be positive, so that by (7) and (8) the following holds: tukdukvtvv P i Iwuiv X i Iwuiv P j Iwujfor all k I and tukdukv tvv P i Iwuiv X i Iwuiv P j Iwujfor all k I. (15) Rewriting yields tukv P i Iwuiv X i Iwuiv P j Iwujdukvfor all k I and tuk v P j Ikwujv P i Ikwuiv P j Ikwujdukv,if P i Ikwuiv P i Iwuiv0 v P j Ikwujv P i Ikwuiv P j Ikwujdukv,if P i Ikwuiv P i Iwuiv0k I",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_384",
    "chunk_position": 384
  },
  {
    "id": "2308.08218v2",
    "text": ". It is now clear that the firing time tas a function of the input tu1, . . . , t udis a piecewise linear mapping on polytopes decomposing Rd. To show that the mapping is additionally continuous if (13) holds, we need to assess tvon the breakpoints. Let I, J 1, . . . , d be index sets corresponding to input neurons ui:i I,uj:j Jthat cause vto fire on the input region RIRd,RJRdrespectively. Assume that it is possible to transition from RIto RJthrough a breakpoint t I,J (t I,J u1, . . . , t I,J ud)Rdwithout leaving RIRJ",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_385",
    "chunk_position": 385
  },
  {
    "id": "2308.08218v2",
    "text": ". Assume that it is possible to transition from RIto RJthrough a breakpoint t I,J (t I,J u1, . . . , t I,J ud)Rdwithout leaving RIRJ. Crossing the breakpoint is to the fact that the input neurons ui:i IJdo not contribute to the firing of v anymore and the input neurons ui:i JIbegin to contribute to the firing of v. Subsequently, we consider different cases concerning the relation of Iand J. Thereby, we first require that all weights are positive. Now, assume that JI",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_386",
    "chunk_position": 386
  },
  {
    "id": "2308.08218v2",
    "text": ". Thereby, we first require that all weights are positive. Now, assume that JI. Then, we observe that the breakpoint t I,Jis necessarily an element of the linear region corresponding to the index set with smaller cardinality, i.e., t I,JRJ. This is an immediate consequence of (15) and the fact that t I,Jis characterized by t I,J ukdukvtfor all k IJ",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_387",
    "chunk_position": 387
  },
  {
    "id": "2308.08218v2",
    "text": ". This is an immediate consequence of (15) and the fact that t I,Jis characterized by t I,J ukdukvtfor all k IJ. (17) Indeed, if t I,J ukdukv t, then there exists k0such that (16) also holds for t I,J uk, where 0 k, i.e., a small change in t I,J ukis not sufficient to change the corresponding linear region, contradicting our assumption that t I,Jis a breakpoint. The firing time tis explicitly given by t v P i Jwuiv X i Jwuiv P j Jwuj 22 Using (17), we obtain 0 wukv P j Jwuj(t I,J ukdukv)) for all k IJ so that t v P i Jwuiv X i Jwuiv P j Jwuj X i IJwuiv P j Jwuj(t I,J uiduiv))",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_388",
    "chunk_position": 388
  },
  {
    "id": "2308.08218v2",
    "text": ". Solving for tyields t 1 X i IJwuiv P j Jwujv1 v P i Jwuiv X i Iwuiv P j Jwuj X i Jwuiv P j Iwujvv P i Jwuiv X i Iwuiv P j Jwuj v P i Iwuiv X i Iwuiv P j Iwuj, which is exactly the expression for the firing time on RI. This shows that tis continuous in t I,J. Since the breakpoint t I,Jwas chosen arbitrarily, tis continuous at any breakpoint. The case IJfollows analogously. It remains to check the case when neither IJnor JI. To that end, let i IJandj JI. Assume without loss of generality that t I,JRI so that (14) and (15) imply t I,J uiduiv tt I,J ujdujv",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_389",
    "chunk_position": 389
  },
  {
    "id": "2308.08218v2",
    "text": ". To that end, let i IJandj JI. Assume without loss of generality that t I,JRI so that (14) and (15) imply t I,J uiduiv tt I,J ujdujv. Hence, there exists 0such that t I,J uiduiv t I,J ujdujv. (18) Moreover, due to the fact that t I,Jis a breakpoint we can find t JRJ, where denotes the open ball with radius 3centered at t I,J. In particular, this entails that 3(t J uit I,J ui),(t I,J ujt J uj) 3, and therefore together with (18) t J uidui (t J uit I,J ui) (t I,J uidui) ( t I,J ujt J uj) 0,i.e.,t J uiduiv t J ujdujv. However, (14) and (15) require that t J ujdujv tt J uiduiv since t JRJ",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_390",
    "chunk_position": 390
  },
  {
    "id": "2308.08218v2",
    "text": ". However, (14) and (15) require that t J ujdujv tt J uiduiv since t JRJ. Thus, t I,Jcan not exist, and the case when neither IJnor JIcan not arise. 23 Finally, we observe that the previous analysis remains valid when dropping the positivity assumption on the weights, with the only exception being (17). In particular, for negative weights the behaviour demonstrated in Example 2 may arise so that (17) is not valid in general anymore",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_391",
    "chunk_position": 391
  },
  {
    "id": "2308.08218v2",
    "text": ". In particular, for negative weights the behaviour demonstrated in Example 2 may arise so that (17) is not valid in general anymore. However, by restricting the feasible weights, i.e., prohibiting negative weights of large magnitude via the condition in (13), we ensure that (17) holds and the statement follows. Remark 22 The observations about the parameter in Remark 20 directly transfer from the twoto thed-dimensional setting. Additionally, note that (13) is a necessary and sufficient condition for the firing time of an LSNN neuron to be continuous in the input firing times",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_392",
    "chunk_position": 392
  },
  {
    "id": "2308.08218v2",
    "text": ". However, the corresponding condition for a whole multi-layer LSNN as provided in (9)is sufficient but not necessary. We want to highlight some similarities and differences between twoand d-dimensional inputs. In both cases, the actual number of linear regions depends on the choice of parameter, in particular, the synaptic weights. Thereby, the number of linear regions scales at most as 2d1in the input dimension dof an LSNN neuron, and the number is indeed attained if all weights are positive (as observed in Lemma 13)",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_393",
    "chunk_position": 393
  },
  {
    "id": "2308.08218v2",
    "text": ". However, the d-dimensional case allows for more flexibility in the structure of the linear regions. Recall that in the two-dimensional case, the boundary of any linear region is described by hyperplanes of the form (12). This does not hold if d 2, see e.g. (16). Here, the weights also affect the shape of the linear region. Refining the connection between the boundaries of a linear region, its response function, and the specific choice of parameter requires further considerations",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_394",
    "chunk_position": 394
  },
  {
    "id": "2308.08218v2",
    "text": ". Similarly, obtaining a non-trivial upper bound on the number of linear regions for networks of LSNN neurons is not straightforward as the following example shows. Example 3 Letbe a one-layer LSNN with dininput neurons and doutoutput neurons. Via Proposition 13, we certainly can upper bound the number of linear regions generated by bdout, i.e., the product of the number of linear regions generated by each output neuron. Unfortunately, the bound is far from optimal. Consider the case when dindout 2",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_395",
    "chunk_position": 395
  },
  {
    "id": "2308.08218v2",
    "text": ". Unfortunately, the bound is far from optimal. Consider the case when dindout 2. Then, the structure of the linear regions generated by the individual output neurons is given in (12). In particular, the boundary of the linear regions is described by a set of specific hyperplanes with a common normal vector, where the parameters of only induce a shift of the hyperplanes. In other words, the hyperplanes separating the linear regions are parallel. Hence, each output neuron generates at most two parallel hyperplanes yielding three linear regions (see 2)",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_396",
    "chunk_position": 396
  },
  {
    "id": "2308.08218v2",
    "text": ". Hence, each output neuron generates at most two parallel hyperplanes yielding three linear regions (see 2). The number of linear regions generated byis therefore given by the number of regions four parallel hyperplanes can decompose the input domain into, i.e., at most 59 (2din1)dout. Finally, under even stronger conditions, i.e., all weights are positive, we can further characterize the firing map of an LSNN neuron. Lemma 23 Letvbe a LSNN neuron with with threshold v0and presynaptic neurons u1, . . . , u d with corresponding weights wuiv0, delays duiv0, and firing times tui R, respectively",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_397",
    "chunk_position": 397
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , u d with corresponding weights wuiv0, delays duiv0, and firing times tui R, respectively. Then the firing time tas a function of the firing times tu1, . . . , t udis a increasing and concave function. Moreover, the firing time of vis given by t inf Idn s I1P i Iwuiv v X i Iwui :s Imax i Ituio .(19) 24 Proof First, we show that the expression inf Idn s I1P i Iwuiv v X i Iwui :s Imax i Ituio is to the firing time tof an LSNN neuron v. By (8) we immediately observe that t 1P i Fwuiv v X i Fwui for some Fdsuch that max i Ftui tmin id Ftui, i.e.,t",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_398",
    "chunk_position": 398
  },
  {
    "id": "2308.08218v2",
    "text": ". By (8) we immediately observe that t 1P i Fwuiv v X i Fwui for some Fdsuch that max i Ftui tmin id Ftui, i.e.,t. Moreover, set s J1P i Jwuiv v X i Jwui for Jd and assume that s J tujfor all j J. If JFC, then for some k JFCwe have s J t ukt. Hence, assume that JFC, i.e., JF. Then, either s Jt(if FJ) or (for FJ) we get X i Jwui X i Fwuituiduiv) X i Jwuituiduiv) X i FJwuituiduiv) X i Jwuituiduiv),i.e.,s J t. Therefore s Jtso that tand (19) follows. Next, we show that tvis an increasing function. Let Rdsuch that i0for all i 1, . . . , d . Denote the potential of vfor inputs (tu1, . .",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_399",
    "chunk_position": 399
  },
  {
    "id": "2308.08218v2",
    "text": ". Next, we show that tvis an increasing function. Let Rdsuch that i0for all i 1, . . . , d . Denote the potential of vfor inputs (tu1, . . . , t ud)anby Pvand P v, respectively. Due to (7), we know that v P X idwuiv(ttuiduiv)X idwuiv(ttuiiduiv) P for any t t . Hence, the potential P does not reach vfort t . In other words, ttso that tvis indeed an increasing function. Finally, we prove the concavity of tv. Let x, y Rddenote two distinct firing times of u1, . . . , u d. Our goal is to show that ty)pt (1 p)tfor all 0 p 1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_400",
    "chunk_position": 400
  },
  {
    "id": "2308.08218v2",
    "text": ". Let x, y Rddenote two distinct firing times of u1, . . . , u d. Our goal is to show that ty)pt (1 p)tfor all 0 p 1. (20) We already know that tvis a CPWL function, i.e., tvpartitions Rdinto linear regions. Therefore, we consider the following possibilities: Either xandyare in the same or different linear regions. Since 25 the linear regions are defined by intersections of halfspaces (see (14) and (15)), they are convex so that in the former case it is immediate to verify that (20) holds. In the latter case, we need to further distinguish between two cases",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_401",
    "chunk_position": 401
  },
  {
    "id": "2308.08218v2",
    "text": ". In the latter case, we need to further distinguish between two cases. Assume xandyare located in distinct linear regions RIand RJ, respectively, whereby I, Jdindicate the subsets of neurons u1, . . . , u dthat trigger the firing of v on the given linear region. Then, either px (1p)y RIRJorpx (1p)C for0 p 1. We will explicitly show the concavity of tvforpx (1p)y RIRJ, the other case follows along similar lines. Without loss of generality, we assume that px (1p)y RI. Moreover, we denote the restriction of tvto RIand RJby AIand AJ, respectively",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_402",
    "chunk_position": 402
  },
  {
    "id": "2308.08218v2",
    "text": ". Without loss of generality, we assume that px (1p)y RI. Moreover, we denote the restriction of tvto RIand RJby AIand AJ, respectively. Since tvis a CPWL function, AIand AJare affine functions so that A ( m I)Tz Ib Iand A (m J)Tz Jb Jforz IRI,z JRJand suitable parameter m I, m JRd,b I, b JR. Using that AIand AJare affine, one derives that Ay)p A (1 p)A is to (m Im J)Ty (b Ib J)0, (21) i.e., verifying (21) suffices to obtain concavity of tv",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_403",
    "chunk_position": 403
  },
  {
    "id": "2308.08218v2",
    "text": ". To that end, observe that m I, b Iandm J, b J are determined by Iand J, respectively, i.e., by the weights and delays associated with Iand Jvia the firing time of vin (8). In particular, we have m I . A.2.3. N ETWORKS OF SPIKING NEURONS Next, we want to extend the properties of an LSNN neuron to a network of LSNN neurons. Due to the layer-wise arrangement of neurons in an LSNN, this task is rather straightforward. Thus, the results in Theorem 9 and Proposition 10 follow",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_404",
    "chunk_position": 404
  },
  {
    "id": "2308.08218v2",
    "text": ". Thus, the results in Theorem 9 and Proposition 10 follow. Proof of Theorem 9 In Lemma 21, we showed that the firing time of an LSNN neuron with arbitrarily many input neurons is a PWL function in the input firing times. Since consists of LSNN neurons arranged in layers it immediately follows that the firing map of each layer is PWL. Thus, as a composition of PWL mappings titself is PWL. Furthermore, (9) guarantees that (13) holds for each neuron in , i.e., the firing time of each LSNN neuron continuously depends on its input firing times",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_405",
    "chunk_position": 405
  },
  {
    "id": "2308.08218v2",
    "text": ". Therefore, we conclude analogously as for the PWL property that tis continuous, i.e., tis a CPWL mapping, if (9) is satisfied. As already indicated, the condition in Theorem 9 is not necessary to obtain a continuous firing map. Next, we specifically construct an LSNN demonstrating this observation. Example 4 Consider a two-layer LSNN defined by W11 0 0 0 11 2 , W2 1 1 1 ,1 1 1 1 ,2 1. 26 Without loss of generality, we assume the delays to be zero. We show that despite the negative weight, which violates (9),t(t1, t2)is a CPWL function for the input firing times tu1, tu2R",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_406",
    "chunk_position": 406
  },
  {
    "id": "2308.08218v2",
    "text": ". We show that despite the negative weight, which violates (9),t(t1, t2)is a CPWL function for the input firing times tu1, tu2R. Denote the input neurons by u1, u2and the neurons in the hidden layer by v1, v2, v3. Note that the firing times of the neurons in the hidden layer depend on either u1oru2. Via (8), the firing times of the neurons in the hidden layer are tv1 1 tu1, t v2 1 tu2, t v3 2 tu2. Similarly, via (8), the firing time of the output neuron is t(tu1, tu2) 2 tu1, iftv1 tv2 2 tu2, iftv2 tv1 3 21 2(tu1tu2),iftv1tv2 1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_407",
    "chunk_position": 407
  },
  {
    "id": "2308.08218v2",
    "text": ". Similarly, via (8), the firing time of the output neuron is t(tu1, tu2) 2 tu1, iftv1 tv2 2 tu2, iftv2 tv1 3 21 2(tu1tu2),iftv1tv2 1. Hence, the breakpoints (tb u1tb u2)R2of the linear regions are determined by tb u1tb u2 1tb u1( tb u2 1,iftb u1 tb u2 tb u21,iftb u1 tb u2. Evaluating the firing time of the output neuron at the breakpoints gives t(tb u1, tb u2) ( 2 tb u2,iftb u1 tb u2 2 tb u1,iftb u1 tb u2, which shows that tis indeed continuous. With the same arguments as in the proof of Theorem 9, we also show Proposition 10",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_408",
    "chunk_position": 408
  },
  {
    "id": "2308.08218v2",
    "text": ". With the same arguments as in the proof of Theorem 9, we also show Proposition 10. Proof of Proposition 10 First, the alternative expression for the firing time of an LSNN neuron with only positive weights is proven in Lemma 23. Hence, it is left to show that tis increasing and concave under the given condition. However, this is a direct consequence of Lemma 23 and the structure of . In particular, one establishes that tis increasing as a composition of strictly increasing maps and concludes that tis also concave since the composition of non-decreasing and concave functions is again concave",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_409",
    "chunk_position": 409
  },
  {
    "id": "2308.08218v2",
    "text": ". Finally, we construct specific LSNNs with locally decreasing and non-concave firing maps, highlighting the need for the positive weights condition. Example 5 Letvbe an LSNN neuron with threshold v 1 and presynaptic neurons u1, u2, u3 with corresponding weights wu1v1 2, wu2v 1, wu3v 1, delays du1vdu2vdu3v 0. Additionally, let ta (0,1,1)Tandtb (1 2,1,1)Tbe two data points corresponding to the firing times of input neurons. Via (8), one can directly verify that t twhen ta tb, i.e., tvis not increasing",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_410",
    "chunk_position": 410
  },
  {
    "id": "2308.08218v2",
    "text": ". Via (8), one can directly verify that t twhen ta tb, i.e., tvis not increasing. For the non-concave part, let tx (1 2,0,0)Tandty (1 2,1,1)Tas two data points corresponding to the firing times of input neurons. Setting p1 2and again via (8), one can directly verify that pt (1 p)ttty). 27 A.3. Realizing Re LU with LSNNs In this section, we show that LSNNs can realize the Re LU function. To that end, we first analyze the ability of an LSNN neuron to express certain (simple) piecewise functions via its firing map",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_411",
    "chunk_position": 411
  },
  {
    "id": "2308.08218v2",
    "text": ". To that end, we first analyze the ability of an LSNN neuron to express certain (simple) piecewise functions via its firing map. Proposition 24 Letc1, c2R,c3(a, b)Rand consider f1, f2: a, b Rgiven by f1(x) ( xc1,ifx c 3 c2 ,ifx c 3orf2(x) ( xc1,ifx c 3 c2 ,ifx c 3, where we do not fix the value at the breakpoint xc3. There does not exist an LSNN neuron vwith input neuron u1such that t ona, b, where f f1,f1, f2andtdenotes the firing time of von input tu1x",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_412",
    "chunk_position": 412
  },
  {
    "id": "2308.08218v2",
    "text": ". There does not exist an LSNN neuron vwith input neuron u1such that t ona, b, where f f1,f1, f2andtdenotes the firing time of von input tu1x. Proof Ifu1is the only input neuron, then vfires if and only if wu1v0and by (8) the firing time is given by t wu1vxdu1vfor all xa, b, i.e.,tvf. Therefore, we introduce auxiliary input neurons u2, . . . , u nand assume without loss of generality that tuiduiv tujdujvforj i . Here, the firing times tui,i 2, . . . , n , are suitable constants. We will show that even in this extended setting tvfstill holds and thereby also the claim",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_413",
    "chunk_position": 413
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , n , are suitable constants. We will show that even in this extended setting tvfstill holds and thereby also the claim. For the sake of contradiction, assume that t f1(x)for all xa, b. This implies that there exists an index set J 1, . . . , n with P j Jwujv0and a corresponding non-empty interval (a1, c3)a, bsuch that c2t 1P i Jwuiv v X i Jwui for all , (22) where we have applied (8). Note that Jis of the form J1, . . . , for some 2, . . . , n because (tuiduiv)n i2is in ascending order, i.e., if the spike from uhas reached vbefore vfired, then so did the spikes from ui,2i",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_414",
    "chunk_position": 414
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , n because (tuiduiv)n i2is in ascending order, i.e., if the spike from uhas reached vbefore vfired, then so did the spikes from ui,2i . In particular, we immediately observe that 1Jsince otherwise, due to the contribution from u1,tvis non-constant on (a1, c3), i.e., tvc3o. Hence, the spike from u1with firing time necessarily reaches vafter the subset of neurons specified by Jalready caused vto fire. Therefore, we have xdu1vt c2for all . However, it immediately follows for any yc3that ydu1v xdu1vc2for all",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_415",
    "chunk_position": 415
  },
  {
    "id": "2308.08218v2",
    "text": ". Therefore, we have xdu1vt c2for all . However, it immediately follows for any yc3that ydu1v xdu1vc2for all . Thus, we know that a spike from u1withtu1 c3does not reach vbefore it emits a spike unless there are additional incoming spikes in vfrom neurons ui:i I 2, . . . , n J 1, . . . , n , which delay its firing. By the same reasoning as before, we conclude that tu1 du1vc2because otherwise (22) would be violated since spikes from neurons ui:i I contribute to the firing of vo. Consequently, the firing of vcan not be delayed, i.e., 28 t c2for all f2(x)for all xa, b",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_416",
    "chunk_position": 416
  },
  {
    "id": "2308.08218v2",
    "text": ". Consequently, the firing of vcan not be delayed, i.e., 28 t c2for all f2(x)for all xa, b. This implies that there exists an index set I 1, . . . , n with P i Iwuiv0and a corresponding non-empty interval (a1, c3)a, bsuch that xc1t 1P i Iwuiv vwu1X i I1wui for, (23) where we have applied (8). Note that 1Inecessarily needs to hold, since otherwise tvis constant on (a1, c3). Hence, Iis of the form I1, . . . , for some 1, . . . , n . To satisfy t f2(x)for all xa, b, there additionally needs to exist an index set J 1, . .",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_417",
    "chunk_position": 417
  },
  {
    "id": "2308.08218v2",
    "text": ". Hence, Iis of the form I1, . . . , for some 1, . . . , n . To satisfy t f2(x)for all xa, b, there additionally needs to exist an index set J 1, . . . , n with P j Jwujv0and a corresponding non-empty interval (c3, b1)a, bsuch that tvc2on (c3, b1). We conclude that J1, . . . , m or J2, . . . , m for some m 1, . . . , n . In the former case, tvis non-constant on (c3, b1)(due to the contribution from u1), i.e., tvc2o. Hence, it remains to consider the latter case",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_418",
    "chunk_position": 418
  },
  {
    "id": "2308.08218v2",
    "text": ". , n . In the former case, tvis non-constant on (c3, b1)(due to the contribution from u1), i.e., tvc2o. Hence, it remains to consider the latter case. First, note that c10is not a admissible choice since (23) implies that for t xc1x, i.e., the spike from u1does not reach vbefore its firing, which contradicts the construction. Thus, there exists some 0 c 1such that c3(a1, c3)and t c3c1 c3. This particularly entails that no spikes from neurons uj:j JIreach vbefore time c3c1, i.e.,tu1du1c3c1 c3",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_419",
    "chunk_position": 419
  },
  {
    "id": "2308.08218v2",
    "text": ". This particularly entails that no spikes from neurons uj:j JIreach vbefore time c3c1, i.e.,tu1du1c3c1 c3. Therefore, we derive that mneeds to hold since otherwise u1withtu1contributes to the firing of v, contradicting the fact that tvis constant on (c3, b1). Moreover, m, i.e., JI 1, is not valid because either Jis empty or by comparing the coefficients of xin (23) we find that wu1v P i Iwuiv 1X i I1wuiv 0, (24) which implies the contradiction 0 X i I1wuiv X j Jwujv0. Finally, m is also not feasible",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_420",
    "chunk_position": 420
  },
  {
    "id": "2308.08218v2",
    "text": ". Finally, m is also not feasible. This follows from (24) via the observation that wu1v0, i.e., the contribution from u1to the potential of vis necessarily positive if the associated spike arrives in time. Consequently, when u1stops contributing to the firing of vothe firing time tvdid not decrease (since by the assumption m also no further incoming spikes arrive). Hence, the spikes from u2, . . . , u contribute to the firing of von any input contradicting m . Thus, Jcan not exist and thereby tvf2ona, bcan not be achieved",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_421",
    "chunk_position": 421
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , u contribute to the firing of von any input contradicting m . Thus, Jcan not exist and thereby tvf2ona, bcan not be achieved. Next, we show that f2as defined in Proposition 24 can indeed be emulated by the firing map of an LSNN neuron under suitable conditions. 29 Proposition 25 Letc1, c2R,c3(a, b)Rand consider f: a, b R ( xc1,ifx c 3 c2 ,ifx c 3, where we do not fix the value at the breakpoint xc3. There exists a one-layer LSNN with output neuron vand input neuron u1such that t ona, b, where tdenotes the firing time ofvon input tu1x, if and only if c1c3c2as well as c12c3",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_422",
    "chunk_position": 422
  },
  {
    "id": "2308.08218v2",
    "text": ". Proof First, we show that tvcan not emulate fif the conditions c1c3c2andc12c3are not met. The argument is essentially the same as in the proof of Proposition 24 and we only sketch the main steps. Assuming that t for all xa, bimplies that there exists an index set I12, . . . , for some 2, . . . , n with P i Iwuiv0and a corresponding non-empty interval (a1, c3)a, bsuch that xc1t 1P i Iwuiv vwu1X i I1wui for, (25) where we have applied (8). Due to 1I, we obtain xc1t xdu1vc12xdu1vfor, (26) and in particular c12c3needs to be satisfied",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_423",
    "chunk_position": 423
  },
  {
    "id": "2308.08218v2",
    "text": ". Due to 1I, we obtain xc1t xdu1vc12xdu1vfor, (26) and in particular c12c3needs to be satisfied. Hence, for 0such that c3(a1, c3)we find via (25) that t c3c1c3 c 3. (27) Moreover, to satisfy t f2(x)for all xa, bto hold, there additionally needs to exist an index set J2, . . . , m for some m 2, . . . , n with P j Jwujv0and a corresponding non-empty interval (c3, b1)a, bsuch that tvc2o. Due to (27) we conclude that no spikes from neurons uj:j JIreach vbefore time c3so that mneeds to be satisfied. Finally, assuming that c1c3c2entails that there is a jump at c3inf, i.e.,fis discontinuous at c3",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_424",
    "chunk_position": 424
  },
  {
    "id": "2308.08218v2",
    "text": ". Finally, assuming that c1c3c2entails that there is a jump at c3inf, i.e.,fis discontinuous at c3. However, a jump discontinuity requires an incoming spike that delays the firing of vfortu1c3, which we already excluded. Thus, t for all xa, bcan not be achieved given that c1c3c2. For the reverse direction, we will explicitly construct an LSNN neuron vthat emulates fif the conditions c1c3c2andc12c3are fulfilled",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_425",
    "chunk_position": 425
  },
  {
    "id": "2308.08218v2",
    "text": ". For the reverse direction, we will explicitly construct an LSNN neuron vthat emulates fif the conditions c1c3c2andc12c3are fulfilled. We introduce an auxiliary input neuron with constant firing time tu2Rand specify the parameter of (( W, D, ))in the following manner (see 3a): W1 2 1 , Dd1 d2 , , where , d1, d20are to be specified. Note that either u2oru1together with u2can trigger a spike invsince wu1v0. Therefore, applying (8) yields that u2triggers a spike in vunder the following circumstances: t tu2d2ifttu1d1xd1. Hence, this case only arises when tu2d2xd1tu2d2d1x",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_426",
    "chunk_position": 426
  },
  {
    "id": "2308.08218v2",
    "text": ". Hence, this case only arises when tu2d2xd1tu2d2d1x. 30 To emulate fthe parameters need to satisfy tu2d2d1xfor all which simplifies to tu2d2d1c3. (28) If the additional condition tu2d2c2 (29) is met, we can infer that for d1c2c3(which is a valid choice due to c2c3c12c3 2c32c3 0) t ( 2(tu2d2)(xd1),ifx c 3 tu2d2 ,ifxc3( xc1,ifx 0 c2 ,ifx0. Finally, it is immediate to verify that the conditions (28) and (29) can be satisfied simultaneously by choosing d2d1andtu2c3",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_427",
    "chunk_position": 427
  },
  {
    "id": "2308.08218v2",
    "text": ". Finally, it is immediate to verify that the conditions (28) and (29) can be satisfied simultaneously by choosing d2d1andtu2c3. Having established the capability or inability to emulate certain simple affine functions by the firing map of an LSNN neuron, we now turn to their realizations. To that end, recall that to realize a function f: a, b Rwe focus on encoding schemes of the type Tin Tout",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_428",
    "chunk_position": 428
  },
  {
    "id": "2308.08218v2",
    "text": ". To that end, recall that to realize a function f: a, b Rwe focus on encoding schemes of the type Tin Tout . Therefore, for an LSNN neuron vwith input neuron u1andxa, bwe distinguish the following encoding schemes: a)( tu1 Tinx:z t Tout R R Toutt b)( tu1 Tinx:z t Tout R R Toutt c)( tu1 Tinx:z t Tout R R Toutt d)( tu1 Tinx:z t Tout R R Toutt Note that cases a) and b) describe consistent input and output encoding schemes, whereas c) and d) provide inconsistent in the sense that input and output formalism do not match",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_429",
    "chunk_position": 429
  },
  {
    "id": "2308.08218v2",
    "text": ". Consistency is an important property when stacking LSNNs because otherwise, the realization of the stacked network does not match the composition of the subnetworks in general; see .1. Now, consider the function f1: a, b Rgiven by f1(x) ( xc1,ifx c 3 c2 ,ifx c 3 for some constants c1, c2Randc3(a, b). To realize f1by an LSNN neuron vvia scheme a) we need f1(x) R Touttfor all xa, b, 31 i.e., t ( z Tout Tinc1,ifz Tin c3 c2Tout ,ifz Tin c3t ( zc 1,ifz c 3 c 2 ,ifz c 3, (30) where c 1Tout Tinc1,c 2c2Tout, and c 3c3Tin",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_430",
    "chunk_position": 430
  },
  {
    "id": "2308.08218v2",
    "text": ". However, Proposition 24 implies that the sought function in (30) can not be expressed as the firing map of v. Hence, f1can not be realized by an LSNN neuron with encoding scheme a). Similar observations can be made for the other encoding schemes and the realization of the functions in Proposition 24 and 25. Our goal is to realize the Re LU activation function and it is now straightforward to verify that Re LU can not be realized by an LSNN neuron with consistent encoding",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_431",
    "chunk_position": 431
  },
  {
    "id": "2308.08218v2",
    "text": ". In contrast, for the inconsistent scheme c) we obtain for a given input domain a, b (x) R Toutt, i.e., t ( z Tout Tin,ifz Tin Tout ,ifz T in, which can be expressed by the firing map of vfor a suitable choice of Toutand Tin; see Proposition 25. We summarize these observations in our next result. Proposition 26 An LSNN neuron can not realize Re LU on a given domain with consistent encoding, whereas Re LU can be realized when applying an inconsistent encoding",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_432",
    "chunk_position": 432
  },
  {
    "id": "2308.08218v2",
    "text": ". Is it possible to realize Re LU with a consistent encoding? To realize by an LSNN via scheme a) we need (x) R(x) Touttfor all xa, b, (31) i.e., t ( z Tout Tin,ifz Tin0 Tout ,ifz Tin0t ( z Tout Tin,ifz T in Tout ,ifz Tin.(32) We proceed by constructing a two-layer LSNN that indeed achieves this goal. Proposition 27 Letc1, c2Rbe such that c2 c1andc1c22b. Consider f: a, b R defined as ( xc2c1,ifx c 1 c2 ,ifxc1. There exists a two-layer LSNN with output neuron vand input neuron u1such that t ona, b, where tdenotes the firing time of von input tu1x",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_433",
    "chunk_position": 433
  },
  {
    "id": "2308.08218v2",
    "text": ". Proof We introduce an auxiliary input neuron u2with constant firing time tu2Rand specify the parameter of (( W1, D1,1),(W2, D2,2))in the following manner: W11 20 1 1 , D1d0 d d ,1 , W21 2 1 , D2d d ,2, (33) 32 tu1 tu2tv (912, du1v) (1, du2v) input neuronsoutput neuron (a)tu1 tu2tz1 tz2tv (912, du1z1) (1, du2z1) (1, du2z2)(912, dz1v) (1, dz2v) input neuronsoutput neuron (b) 3: a) Computation graph associated with an LSNN with two input neurons and one output neuron that realizes fas defined in Proposition 25",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_434",
    "chunk_position": 434
  },
  {
    "id": "2308.08218v2",
    "text": ". (b) Stacking the network in (a) twice results in an LSNN that realizes the Re LU activation function. where d0and 0is chosen such that tu2 b. We denote the input neurons by u1, u2, the neurons in the hidden layer by z1, z2, and the output neuron by v. Note that the firing time of z1depends on u1andu2. In particular, either u2oru1together with u2can trigger a spike in z1 since wu1z10. Therefore, applying (7) yields that u2triggers a spike in z1under the following circumstances: tz1(x) tu2diftz1(x)tu1dxd. Hence, this case only arises when tu2dxdtu2x",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_435",
    "chunk_position": 435
  },
  {
    "id": "2308.08218v2",
    "text": ". Hence, this case only arises when tu2dxdtu2x. (34) However, by construction tu2 b, so that (34) does not hold for any xa, b. Thus, we conclude via (8) that tz1(x) 2( tu2d)(xd) 2( tu2) dx. By construction, the firing time tz2tu2dofz2is a constant which depends on the input only via u2. A similar analysis as in the first layer shows that t tz2difttz1d 2(tu2) dxd 2(tu2d)x. Hence, z2triggers a spike in vwhen tz2dtu2dd2(tu2d)x xtu2",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_436",
    "chunk_position": 436
  },
  {
    "id": "2308.08218v2",
    "text": ". A similar analysis as in the first layer shows that t tz2difttz1d 2(tu2) dxd 2(tu2d)x. Hence, z2triggers a spike in vwhen tz2dtu2dd2(tu2d)x xtu2. If the additional condition tz2dc2 2(d) tu2c2 (35) 33 is met, we can infer that t ( 2(tz2d)(tz1(x) d),ifx t u2 tz2d , ifxtu2 ( 2c2(2(tu2) dxd),ifx t u2 c2 ,ifxtu2 ( xc2tu2,ifx t u2 c2 ,ifxtu2. Setting tu2c1gives t ( xc2c1,ifx c 1 c2 ,ifxc1, and we observe that for 1 2(c2c1)2d, which is a valid threshold provided that dis small, (35) as well as tu2c1 b2d b is satisfied for a suitable choice of d. Hence, emulates fas desired",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_437",
    "chunk_position": 437
  },
  {
    "id": "2308.08218v2",
    "text": ". Hence, emulates fas desired. Finally, we state the implication of this proposition for the Re LU realization, which is a direct implication of (31) and (32). Proposition 28 There exists a two-layer LSNN that realizes Re LU on a given bounded domain with a consistent encoding scheme. A.4. Realizing Re LU networks by LSNNs In this section, we show that an LSNN has the capability to reproduce the output of any Re LU network. Specifically, given access to the weights and biases of an ANN, we construct an LSNN and set the parameter values based on the weights and biases of the given ANN",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_438",
    "chunk_position": 438
  },
  {
    "id": "2308.08218v2",
    "text": ". This leads us to the desired result. The essential part of our proof revolves around choosing the parameters of an LSNN such that it effectively realizes the composition of an affine-linear map and the non-linearity represented by the Re LU activation. The realization of Re LU with LSNNs is proved in the previous .3. To realize an affine-linear function using an LSNN neuron, it is necessary to ensure that the spikes from all the input neurons together result in the firing of an output neuron instead of any subset of the input neurons",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_439",
    "chunk_position": 439
  },
  {
    "id": "2308.08218v2",
    "text": ". We achieve that by appropriately adjusting the value of the threshold parameter. As a result, an LSNN neuron, which implements an affine-linear map, avoids partitioning of the input space. Setup for the proof of Theorem 11 Letd, LNbe the width and the depth of an ANN , respectively, i.e., (( A1, B1),(A2, B2), . . . , (AL, BL)),where (A, B)Rdd Rd,1 L, (AL, BL)Rdd Rd",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_440",
    "chunk_position": 440
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , (AL, BL)),where (A, B)Rdd Rd,1 L, (AL, BL)Rdd Rd. For a given input domain a, bd Rd, we denote by ((A, B))the-th layer, where y0 a, bdand yl R ((Al)Tyl1Bl),1 L, y LR (AL)Ty L1B 34 Rf R tu1 tu2 tu3tv Removal of auxiliary neurons (a)tu1 tu2 tu3tv Parallelization (b)tu1 tu2 tu3tv1 tv2 (c) 4: (a) Computation graph associated with an LSNN fresulting from the concatenation of andfthat realizes (), where fis an affine function and is the Re LU non-linearity. The auxiliary neurons are shown in red",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_441",
    "chunk_position": 441
  },
  {
    "id": "2308.08218v2",
    "text": ". The auxiliary neurons are shown in red. (b) Same computation graph as in (a); when parallelizing two identical networks, the dotted auxiliary neurons can be removed and auxiliary neurons from (a) can be used for each network instead. (c) Computation graph associated with an LSNN as a result of the parallelization of two subnetworks f1andf2. The auxiliary neuron in the output layer serves the same purpose as the auxiliary neuron in the input layer and is needed when concatenating two such subnetworks f. so that RRL R 1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_442",
    "chunk_position": 442
  },
  {
    "id": "2308.08218v2",
    "text": ". so that RRL R 1. For the construction of the corresponding LSNN, we refer to the associated weights and delays between two LSNN neurons uandvbywuvandduv, respectively. Proof of Theorem 11 Any multi-layer ANN with Re LU activation is simply an alternating composition of affine-linear functions (Al)Tyl1Bland a non-linear function represented by . To generate the mapping realized by , it suffices to realize the composition of affine-linear functions and the Re LU non-linearity and then extend the construction to the whole network using concatenation and parallelization operations",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_443",
    "chunk_position": 443
  },
  {
    "id": "2308.08218v2",
    "text": ". We prove the result via the following steps; see also 4 for a depiction of the intermediate constructions. Step 1: Realizing Re LU non-linearity. Proposition 28 gives the desired result. Step 2: Realizing affine-linear functions with one-dimensional range. Letf: a, bd Rbe an affine-linear function CTxs, CT (c1, . . . , c d)Rd, s R. (37) Consider a one-layer LSNN that consists of an output neuron vand d input units u1, . . . , u d. Via (8) the firing time of vas a function of the input firing times on the linear region RIcorresponding to the index set I1, . .",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_444",
    "chunk_position": 444
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , u d. Via (8) the firing time of vas a function of the input firing times on the linear region RIcorresponding to the index set I1, . . . , d is given by t v P i Iwuiv P i Iwui P i Iwuivprovided that X i Iwuiv0. 35 Introducing an auxiliary input neuron ud1with weight wud1v 1P i Iwuivensures that P i Id1wuiv0and leads to the firing time t v X i Id1wuion RId1. Setting wuivcifori Ianddujvd0forj I d 1yields t vwud1vtud1d X i Icituion RId1a, bd. Therefore, an LSNN f (W, D, )with parameters W c1 ... cd1 , D d ..",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_445",
    "chunk_position": 445
  },
  {
    "id": "2308.08218v2",
    "text": ". Therefore, an LSNN f (W, D, )with parameters W c1 ... cd1 , D d ... d , 0,where cd1 1X i Ici, and the usual encoding scheme Tin Toutand fixed firing time tud1tin, whereby we employ the notation from Definition 7, realizes R toutt touttind X i Icixi (38) touttindson RId1a, bd. (39) Choosing a large enough threshold ensures that a spike in vis necessarily triggered after all the spikes from u1, . . . , u d1reached vso that a, bd RId1holds. It suffices to set sup xa,bdsup xminttindxmax P, where xmin min x1, . . . , x d,0andxmax max x1, . .",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_446",
    "chunk_position": 446
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , u d1reached vso that a, bd RId1holds. It suffices to set sup xa,bdsup xminttindxmax P, where xmin min x1, . . . , x d,0andxmax max x1, . . . , x d,0, since this implies that the potential Pis smaller than the threshold to trigger a spike in von the time interval associated to feasible input spikes, i.e., vemits a spike after the last spike from an input neuron arrived at v. Applying (7) shows that for xa, bdandtxmintind, xmaxtind P X i Iwuiduiv) wud1 tdtin X i Icixi xmaxd C maxa,b. Hence, we set (1 d C) maxa,bssand toutstind to obtain via (38) that R toutt forxa, bd",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_447",
    "chunk_position": 447
  },
  {
    "id": "2308.08218v2",
    "text": ". Hence, we set (1 d C) maxa,bssand toutstind to obtain via (38) that R toutt forxa, bd. (40) Note that the reference time tout (1 d C) maxa,bstindis independent of the specific parameters of fin the sense that only upper bounds C,son the parameters 36 are relevant. Therefore, tou can be applied for different affine linear functions as long as the upper bounds remain valid. This is necessary for the composition and parallelization of subnetworks in the subsequent construction. Step 3: Realizing compositions of affine-linear functions with one-dimensional range and Re LU",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_448",
    "chunk_position": 448
  },
  {
    "id": "2308.08218v2",
    "text": ". Step 3: Realizing compositions of affine-linear functions with one-dimensional range and Re LU. The next step is to realize the composition of Re LU with an affine linear mapping fdefined in (37). To that end, we want to concatenate the networks andfconstructed in Step 1 and Step 2, respectively, via Lemma 16. To employ the concatenation operation we need to perform the following steps: 1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_449",
    "chunk_position": 449
  },
  {
    "id": "2308.08218v2",
    "text": ". To employ the concatenation operation we need to perform the following steps: 1. Find an appropriate input domain a, b R, that contains the image so that parameters and reference times of can be fixed appropriately (see Proposition 28 for the detailed conditions on how to choose the parameter). 2. Ensure that the output reference time tf outoffequals the input reference time t inof. 3. Ensure that the number of neurons in the output layer of fis the same as the number of input neurons in . For the first point, note that CTxs d C xs d Cmaxa,bsfor all xa, bd",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_450",
    "chunk_position": 450
  },
  {
    "id": "2308.08218v2",
    "text": ". For the first point, note that CTxs d C xs d Cmaxa,bsfor all xa, bd. Hence, we can use the input domain a, b d Cmaxa,bs, d Cmaxa,bs and specify the parameters of accordingly. Additionally, recall from Proposition 28 that t in can be chosen freely, so we may fix t intf out, where tf outis established in Step 2. It remains to consider the third point. To realize Re LU, an additional auxiliary neuron in the input layer of with constant input t inwas introduced",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_451",
    "chunk_position": 451
  },
  {
    "id": "2308.08218v2",
    "text": ". To realize Re LU, an additional auxiliary neuron in the input layer of with constant input t inwas introduced. Hence, we also need to add an auxiliary output neuron in f with (constant) firing time tf outt inso that the corresponding output and input dimension and their specification match. This is achieved by introducing a single synapse from the auxiliary neuron in the input layer of fto the newly added output neuron and by specifying the parameters of the newly introduced synapse and neuron suitably. Formally, the adapted network f (W, D, )is given by W c10 ...... cd0 cd11 , D d0 .....",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_452",
    "chunk_position": 452
  },
  {
    "id": "2308.08218v2",
    "text": ". Formally, the adapted network f (W, D, )is given by W c10 ...... cd0 cd11 , D d0 ...... d0 dd , tf outtf ind , where the values of the parameters are specified in Step 2. Then the realization of the concatenated network fis the composition of the individual realizations. This is exemplarily demonstrated in 4a for the two-dimensional input case. By analyzing f, we conclude that a three-layer LSNN with N0() 52 d 3 d 6 37 computational units can realize fona, bd, where N0()denotes the number of neurons in the input layer of . Step 4: Realizing layer-wise computation of",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_453",
    "chunk_position": 453
  },
  {
    "id": "2308.08218v2",
    "text": ". Step 4: Realizing layer-wise computation of . The computations performed in a layer ofare described in (6). Hence, for 1 L the computation can be expressed as R(yl1) ((Al)Tyl1Bl) (Pd i1(Al 1,i)Tyl1 i Bl 1) ... (Pd i1(Al d,i)Tyl1 i Bl d) : (f1(yl1)) ... (f) , where f 1, . . . , f dare affine linear functions with one-dimensional range on the same input domain a1, b1d Rd, where a0, b0 a, banda, bis the range of (f1 1, . . . , f1 d)(a1, b1d). Thus, via Step 3, we construct LSNNs 1, . . . , dthat realize f 1, . . . , f dona1, b1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_454",
    "chunk_position": 454
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , f1 d)(a1, b1d). Thus, via Step 3, we construct LSNNs 1, . . . , dthat realize f 1, . . . , f dona1, b1. Note that by choosing appropriate parameters in the construction performed in Step 2 (as described below (40)), e.g., Al and Bl , we can employ the same input and output reference time for each 1, . . . , d. Consequently, we can parallelize 1, . . . , and obtain networks realizing Rona1, b1d. Finally, Lcan be directly realized via Step 2 by an LSNN . Although already performs the desired task of realizing Rwe can slightly simplify the network",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_455",
    "chunk_position": 455
  },
  {
    "id": "2308.08218v2",
    "text": ". Although already performs the desired task of realizing Rwe can slightly simplify the network. By construction in Step 3, each icontains two auxiliary neurons in the hidden layers. Since the input and output reference time is chosen consistently for 1, . . . , d, we observe that the auxiliary neurons in each iperform the same operations and have the same firing times. Therefore, without changing the realization of we can remove the auxiliary neurons in 2, . . . , dand introduce synapses from the auxiliary neurons in 1accordingly. This is exemplarily demonstrated in 4b for the case d 2",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_456",
    "chunk_position": 456
  },
  {
    "id": "2308.08218v2",
    "text": ". . . , dand introduce synapses from the auxiliary neurons in 1accordingly. This is exemplarily demonstrated in 4b for the case d 2. After this modification, we observe that 3 and d X i2 2N0( i) d(d1)(2 N0( 1)) 2(d1)(d1)(d 1) 4 d 3 for1 L, whereas 1 and 2 d 1. Step 5: Realizing compositions of layer-wise computations of . The last step is to compose the realizations R1, . . . ,RLto obtain the realization RL R 1RL R 1R. As in Step 3, it suffices again to verify that the concatenation of the networks R1, . . . ,RLis feasible. First, note that for 1, . .",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_457",
    "chunk_position": 457
  },
  {
    "id": "2308.08218v2",
    "text": ". As in Step 3, it suffices again to verify that the concatenation of the networks R1, . . . ,RLis feasible. First, note that for 1, . . . , L the input domain of Ris given by a1, b1dso that, we can fix the suitable output reference time T outt ou TRdbased on the parameters of the network, the domain a1, b1, and some input reference time T int i T Rd. By construction in Steps 2 - 4 T incan be chosen freely. Hence setting T1 in T outensures 38 that the reference times of the corresponding networks agree. It is left to align the input dimension of1and the output dimension of for 1, . . . , L 1",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_458",
    "chunk_position": 458
  },
  {
    "id": "2308.08218v2",
    "text": ". It is left to align the input dimension of1and the output dimension of for 1, . . . , L 1. Due to the auxiliary neuron in the input layer of 1, we also need to introduce an auxiliary neuron in the output layer of (see 4c) with the required firing time t1 in t out. Similarly, as in Step 3, it suffices to add a single synapse from the auxiliary neuron in the previous layer to obtain the desired firing time. Thus, we conclude that L 1realizes Rona, b, as desired",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_459",
    "chunk_position": 459
  },
  {
    "id": "2308.08218v2",
    "text": ". Thus, we conclude that L 1realizes Rona, b, as desired. The complexity of in the number of layers and neurons is given by LX 1 3 L2 3 2 and LX 2 N0() (L1) 4d 3 ( L2)(4d 3(d 1)) (2 d 1(d 1)) ( L1) 3(2d 1) (2d 2) Remark 29 Note that the delays play no significant role in the proof of the above theorem. Nevertheless, they can be employed to alter the timing of spikes, consequently impacting the firing time and the resulting output. However, the exact function of delays requires further investigation",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_460",
    "chunk_position": 460
  },
  {
    "id": "2308.08218v2",
    "text": ". However, the exact function of delays requires further investigation. The primary objective is to present a construction that proves the existence of an LSNN capable of accurately reproducing the output of any Re LU network. 39",
    "source": "arXiv",
    "chunk_id": "2308.08218v2_461",
    "chunk_position": 461
  },
  {
    "id": "2208.07502v2",
    "text": "Combinatorial optimization solving by coherent Ising machines based on spiking neural networks Bo Lu1, Yong-Pan Gao2, Kai Wen3, and Chuan Wang1 1School of Artificial Intelligence, Beijing Normal University, Beijing 100875, China 2School of Electronics Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China 3Beijing QBoson Quantum Technology Co., Ltd., Beijing 100015, China Spiking neural network is a kind of neuromorphic computing that is believed to improve the level of intelligence and provide advantages for quantum computing",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_462",
    "chunk_position": 462
  },
  {
    "id": "2208.07502v2",
    "text": ". In this work, we address this issue by designing an optical spiking neural network and find that it can be used to accelerate the speed of computation, especially on combinatorial optimization problems. Here the spiking neural network is constructed by the antisymmetrically coupled degenerate optical parametric oscillator pulses and dissipative pulses. A nonlinear transfer function is chosen to mitigateamplitudeinhomogeneitiesanddestabilize the resulting local minima according to the dynamical behavior of spiking neurons",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_463",
    "chunk_position": 463
  },
  {
    "id": "2208.07502v2",
    "text": ". It is numerically shown that the spiking neural network-coherent Ising machines have excellent performance on combinatorial optimization problems, which is expected to offer new applications for neural computing and optical computing. 1 Introduction Combinatorialoptimizationproblemsareofgreat importance in various fields of computer science. Generally, the combinatorial optimization problem belongs to the non-deterministic polynomial time (NP)-hard complexity class",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_464",
    "chunk_position": 464
  },
  {
    "id": "2208.07502v2",
    "text": ". Generally, the combinatorial optimization problem belongs to the non-deterministic polynomial time (NP)-hard complexity class. The effective selection of the best combination among candidates is required in solving the combinatorial optimization problems which aims to maximize the gains or minimize the losses. During the past decades, they have been ubiquitous in various fields, such as drug design 1, financial manageChuan Wang: wangchuanbnu.edu.cnment 2, and circuit design 3",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_465",
    "chunk_position": 465
  },
  {
    "id": "2208.07502v2",
    "text": ". The computation time of these problems on conventional computers tends to increase exponentially with the size of the problem due to the explosion in the number of combinations 4. The problem in the complexity class NP could be formulated as an Ising problem with only polynomial overhead, and the loss functions can be mappedtothe Isingmodeltocompletethesearch from the initial state to the ground state, which hasbeenimplementedbyseveralphysicalsystems such as quantum annealing 5, 6, optical oscillators 7, nanomagnetic-net arrays 8, 9",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_466",
    "chunk_position": 466
  },
  {
    "id": "2208.07502v2",
    "text": ". Among them, oscillator-based coherent Ising machines (CIM) are widely studied due to the advantages10of highspeed, programmability, and parallel search. Each Ising spin in the CIM can be encoded as the phase iof the light pulse in an optical mode. Employing the degenerate optical parametric oscillation, the phase values are enforced as the binary spin values as i 0,. During computation, the optical gain yields oscillationofthepluseseitherin-phaseorout-of-phase respective to the pump light. Meanwhile, the oscillators are coupled with a tunable coupling constant to form a network",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_467",
    "chunk_position": 467
  },
  {
    "id": "2208.07502v2",
    "text": ". Meanwhile, the oscillators are coupled with a tunable coupling constant to form a network. The oscillation network can be implemented by various ways, such as the optical oscillators 1115, opto-electronic oscillators 16, 17, spatial phase modulators 1820, and integrated photonic chips 21. As a hardware solver, CIM has excellent performance on small-scale combinatorial optimization problems 7, 10, 13, but on large-scale problems, an important technicality would arise in the model which is that the amplitude of the oscillators is not",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_468",
    "chunk_position": 468
  },
  {
    "id": "2208.07502v2",
    "text": ". This phenomenon is usuallycalledtheamplitudeheterogeneitywhichprevents CIM from being correctly mapped to the Ising model 22, as shown in .1(a). The Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0. 1ar Xiv:2208.07502v2 quant-ph 20 Oct 2023 spin configuration of the ground state of the Ising model differs from that of the CIM. During the calculation of CIM, the energy will be frozen in the excited state of the Ising model, as shown in .1(b)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_469",
    "chunk_position": 469
  },
  {
    "id": "2208.07502v2",
    "text": ". During the calculation of CIM, the energy will be frozen in the excited state of the Ising model, as shown in .1(b). To address the problem of amplitude heterogeneity, thereareseveralmethodstofixthe problem by adding a feedback mechanism to reduce the inequality of the amplitude, such as the additional uncorrelated driving signals 22, nonlinear functions 23, 24 and correcting the amplitude by the error signal 2527",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_470",
    "chunk_position": 470
  },
  {
    "id": "2208.07502v2",
    "text": ". Additionally, the exploration of dimensional lifting in Ising machines, particularly those based on multidimensional spins28, has also demonstrated significant enhancements in computational performance. In addition, we turn our attention to the spiking neural network (SNN) which has been simulated on the CIM based on the DOPOs neural network, and the collective behavior of neural clusters has been studied 29, 30. The spiking neurons are a special type of neuron that performs signal processing in the brain",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_471",
    "chunk_position": 471
  },
  {
    "id": "2208.07502v2",
    "text": ". The spiking neurons are a special type of neuron that performs signal processing in the brain. They are considered as the next generation of neural architectures and have been extensively studied in artificial intelligence recently 3133. For example, it can imitate the biological nervous system to produce more intelligent systems, such as brainlikelearning34,35. Furthermore,therearecomplex nonlinear dynamics and chaotic phenomena in the spiking neurons 36, 37, which hopefully achieve the instability of local minima where the computation of CIMs is frozen",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_472",
    "chunk_position": 472
  },
  {
    "id": "2208.07502v2",
    "text": ". In this work, we propose a new architecture of CIM, based on a spiking neuron network composed of DOPO pulses and dissipative pulses. We find that it is able to achieve amplitude heterogeneity and also freeze in local minima during computing for combinatorial optimization problems. By controlling the dissipative parameters ofspikingneuronscomposedofantisymmetrically coupled DOPO and dissipative pulses, the rate and direction of contraction of the phase space volumes can be controlled to bring the system out of the frozen state",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_473",
    "chunk_position": 473
  },
  {
    "id": "2208.07502v2",
    "text": ". More generally, we find the existence of neurons is similar to the dynamic behavior of class-II neurons. An SNN-CIM proposed to achieve the instability of the local minimum by adjusting the parameters, and the performance is compared with the only uses the nonlinear filter function. The results show that SNN-CIM has excellent perfor-mance and has more potential on complex graph structures. 2 SNN-CIM model The model of SNN-CIM we use can be summarized in .1(c)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_474",
    "chunk_position": 474
  },
  {
    "id": "2208.07502v2",
    "text": ". 2 SNN-CIM model The model of SNN-CIM we use can be summarized in .1(c). The system with correction of amplitude heterogeneity can be implemented using a CIM with a measurement-and-feedback (MFB)structurecomposedof DOPOpulses. The pulses for computing are represented by the optical SNN pulses which propagate cyclically in the fiber-ring cavity. For each SNN pulses, it can be divided into two types of pulses by controlling the pump, one is generated by the optical parametric oscillation with nonlinear gain process",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_475",
    "chunk_position": 475
  },
  {
    "id": "2208.07502v2",
    "text": ". It corresponds to the case where the pump is turned on, denoted as x, and the other pulse is the injected optical pulse with a dissipative process only, corresponding to the case where the pump is turned off, denoted as k. A spiking neuron consists of both a typex pulse and a typekpulse coupled to each other with coupling strengths of Jxkand Jk, respectively. The spiking neurons can be connected to each other, as shown in .1(d)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_476",
    "chunk_position": 476
  },
  {
    "id": "2208.07502v2",
    "text": ". The spiking neurons can be connected to each other, as shown in .1(d). Based on this principle, the dynamical behavior of theith neuron can be expressed as: dxi dtxix3 i Jxkki Iext, dki dtki Jkxxi, Iexttan.(1) wherexiis in-phase components of DOPO amplitudes, (ki)is dissipative pulse amplitude. denotes the nonlinear gain generated by the pump,andis the inherent dissipation of the system.Jijrepresents the connection between the ith and the jth neuron, and is the strenght of interaction. In the case of the power above the threshold, the DOPO pulses undergo spontaneous symmetry breaking",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_477",
    "chunk_position": 477
  },
  {
    "id": "2208.07502v2",
    "text": ". In the case of the power above the threshold, the DOPO pulses undergo spontaneous symmetry breaking. Then, the potential energies of the spiking neurons can be represented by the positive and negative amplitudes of the typex pulses. Here, the amplitude of each pulse circulating through the fiber loop is measured by the photon detector and recorded by the field programmable gate array (FPGA) module. The measurement results and the coupling matrix are Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_478",
    "chunk_position": 478
  },
  {
    "id": "2208.07502v2",
    "text": ". The measurement results and the coupling matrix are Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0. 2 spin configurationenergy Tenergypump00(a) (b)(c) GSGS (Ising ) (d) Spin Spin Spin Spin Spin -spin interaction SHG PSA 1212 FPGA IMPM 1: Schematic diagram of SNN-CIM. a. Variation of energies with spin configuration for homogeneous and inhomogeneous spin amplitudes. The red line is the pump intensity, which does not excite the correct ground state spin configuration. This is for illustration purposes only. b",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_479",
    "chunk_position": 479
  },
  {
    "id": "2208.07502v2",
    "text": ". The red line is the pump intensity, which does not excite the correct ground state spin configuration. This is for illustration purposes only. b. The time evolution of the energy, due to the amplitude heterogeneity, is frozen in the excited state. The dashed line is the ground state energy. c. Optical implementation of SNN-CIM. The pump is turned on to generate type-x DOPO, and when it is turned off, the modulation generates type-k pulses. d",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_480",
    "chunk_position": 480
  },
  {
    "id": "2208.07502v2",
    "text": ". The pump is turned on to generate type-x DOPO, and when it is turned off, the modulation generates type-k pulses. d. The type-x DOPO antisymmetrically coupled and dissipated type-k pulses constitute spiking neurons, and the connections between spiking neurons constitute a spiking neural network. IM: intensity modulator, PM: phase modulator, SHG: second harmonic generation, PSA: phaseensitive amplification, FPGA: field programmable gate array. multiplied in the FPGA, filtered by the nonlinear function, and output to the IM and PM to modulate the injected optical pulse",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_481",
    "chunk_position": 481
  },
  {
    "id": "2208.07502v2",
    "text": ". multiplied in the FPGA, filtered by the nonlinear function, and output to the IM and PM to modulate the injected optical pulse. The injected optical pulses are coupled with the optical pulse in the fiber-ring cavity. In Ref.23, the authors analyzed the nonlinear function filtering has a certain probability of correcting the CIM amplitude heterogeneity. Accordingtotheconstraintsofthe combinatorialoptimizationproblems,theconnection of the neurons in the neural network can be regarded as an Ising model that evolves a spin configuration into the ground state",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_482",
    "chunk_position": 482
  },
  {
    "id": "2208.07502v2",
    "text": ". 3 The dynamics of the spiking neurons According to Eq.(1), we find the dynamic properties of a spiking neuron depend on the linear stability of its fixed point. The fixed points under the steady-state solution of Eq.(1) should satisfy the relation as: 0 kixi, 0 xix3 iki Iext.(2) The solution satisfying Eq.(2) is denoted as x0 and the linear stability of a fixed point (x0,x0)canbeobtainedbytheeigenvaluesofits Jacobian matrix: Jibracketleftigg 3x2 0 1bracketrightigg",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_483",
    "chunk_position": 483
  },
  {
    "id": "2208.07502v2",
    "text": ". (3) Here the eigenvalues iof the Jacobian matrix are given as follows: i1 2(Tradicalbig T24), (4) where and Tare the determinant and trace of the Jacobian matrix, respectively. Then, we have Jxx Jkk Jxk Jkx (3x2 0 1)and TJxx Jkk3x2 0. Numerical simulations give the fixed-point linear ((x0,x0))stability as a function of Iextand the pump strength when 0.3, as shown in .2(a). The stabilityofthosefixedpointsdependson R, these fixed points become unstable when R0. However, the eigenvalues would become complex conjugated when T240. Hence, we mark R 0and T24 0in .2(a), respectively",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_484",
    "chunk_position": 484
  },
  {
    "id": "2208.07502v2",
    "text": ". However, the eigenvalues would become complex conjugated when T240. Hence, we mark R 0and T24 0in .2(a), respectively. Then, the time evolution and the oscillation phase space of those fixed points in regions 3 and 2 are shown in .2(b)(c) and .2(d)(e), respectively. Under a small disturbance of Iext, the system reciprocates on a limit cycle. And the system will shrink to a stable focus under a large Iext. Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_485",
    "chunk_position": 485
  },
  {
    "id": "2208.07502v2",
    "text": ". And the system will shrink to a stable focus under a large Iext. Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0. 3 (a) mn 43 2 1 (b) (d)(c) (e) (g)(f) t xkk x 0.05 0.075 0.1 tt Frequency (1roundtrips) 2: Dynamic behavior of spiking neurons. a. The effect of pump strength and Iexton the stability of the fixed point. The eigenvalues in region 1 and region 4 are real numbers, one has a positive number and the other is all negative",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_486",
    "chunk_position": 486
  },
  {
    "id": "2208.07502v2",
    "text": ". The eigenvalues in region 1 and region 4 are real numbers, one has a positive number and the other is all negative. The eigenvalues of region 2 and region 3 are complex conjugates, one is the positive real part and the other is the negative real part. 0.3. b. and c. The time evolution and phase space trajectories of the typex DOPO pulse and typekpulse at point n in (a), 1and Iext 0.25. d. and e. Corresponding to m points, 1 and Iext 0.05. f. Fourier transform of the time evolution of amplitude at different Iext. g",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_487",
    "chunk_position": 487
  },
  {
    "id": "2208.07502v2",
    "text": ". d. and e. Corresponding to m points, 1 and Iext 0.05. f. Fourier transform of the time evolution of amplitude at different Iext. g. The time evolution of the type-x DOPO pulse and typekpulse with the increase of Iext. The spiking behavior of single neurons can be achievedbycontrollingtheparameter Iext. When Iextincreases, the amplitude of neuron pulses gradually decreases. However, for the increment of Iextto a certain value, the neuron pulses will no longer appear, as shown in .2(g)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_488",
    "chunk_position": 488
  },
  {
    "id": "2208.07502v2",
    "text": ". However, for the increment of Iextto a certain value, the neuron pulses will no longer appear, as shown in .2(g). This process is consistent with the evolution direction of the arrow indicated by .2(a), that is, the system appears Andronov-Hopf (AH) bifurcation with the increase of Iext, which makes the fixed points change from an unstable limit cycle to a stable focused point. This shows that the optical pulse network has the properties of the class-II neurons by anti-symmetrically coupling a pair of nonlinear gain DOPO pulse and dissipation optical pulse",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_489",
    "chunk_position": 489
  },
  {
    "id": "2208.07502v2",
    "text": ". In addition, as shown in .2(f), we find a smaller Iextcould exhibit a larger firing rate by taking a Fourier transform of the temporal evolution of neurons under different Iext. For the Ising model with antiferromagnetic coupling, the evolution of the energy EIsing ijij2is accompanied by an amplification of the coupling term. In this model, Iextcorresponds to the coupling term on the ith spin, that is, the firing rate decreases (Iextincreases) during the evolution of the Ising model. Moreover, the cluster neurons can also be synchronized to a low firing rate",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_490",
    "chunk_position": 490
  },
  {
    "id": "2208.07502v2",
    "text": ". Moreover, the cluster neurons can also be synchronized to a low firing rate. To foster a more profound comprehension of P Bifurcation : Relation between amplitude heterogeneity correction and coupling parameters. The top shows the variation of the success rate Pof solving the Max-Cut problem of the N25 graph with the coupling strength (normalized by the pump strength ). The bottom is the correction of amplitude inhomogeneityxunder different coupling strength . Other parameters 1.0. Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_491",
    "chunk_position": 491
  },
  {
    "id": "2208.07502v2",
    "text": ". Other parameters 1.0. Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0. 4 theamplitudeheterogeneitycorrectionfacilitated bythe CIMbasedonspikingneuralnetworks, our study focuses on a simplified graph with dimensions N 25nodes and E 75edges, selected as the testbed. We meticulously examined the success rate of solutions denoted as Pand the mean absolute error of the amplitude of different nodes represented as xacross varying coupling strengths, as shown in .3",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_492",
    "chunk_position": 492
  },
  {
    "id": "2208.07502v2",
    "text": ". We can find that as the coupling strength increases, the system energy starts to decrease with the increase of Iext, so the success rate gradually increases. When thetahfunction response is saturated, the probability gradually stabilizes. Moreover, as the coupling strength increases, the difference in amplitudeincreasesintheconventional CIM23,but in our scheme, the nonlinear function causes the difference to disappear gradually. Furthermore, there is a correlation between the ability to find the ground state and the amount of amplitude inhomogeneity",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_493",
    "chunk_position": 493
  },
  {
    "id": "2208.07502v2",
    "text": ". Furthermore, there is a correlation between the ability to find the ground state and the amount of amplitude inhomogeneity. As the inhomogeneity of the magnitude decreases, the mapped Ising model gets closer to the target Hamiltonian, increasing the ability to find the optimal solution. 4 Instability of the local Minimum and SNN-CIM Due to the heterogeneity of amplitudes in the computation of CIM, there are differences of the ground state energy configuration between 0 and 0, as shown in . 1(a)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_494",
    "chunk_position": 494
  },
  {
    "id": "2208.07502v2",
    "text": ". 1(a). Meanwhile, the Ising machines will be frozen at the local minimum and cannot evolve towards the global optimum, as shown in . 1(b), which is observed by Ref. 38onthe CIMimplementedbythe DOPOs pulses network. Similar to the analysis of the linear stability of the pump and Iexton the fixed points, .4(a) shows the effect of dissipation and Iexton the linear stability of those fixed points. Those fixed points that freeze at local minima tend to be located in region 3",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_495",
    "chunk_position": 495
  },
  {
    "id": "2208.07502v2",
    "text": ". Those fixed points that freeze at local minima tend to be located in region 3. The stability of the local minima can be influenced to escape from the frozen local minima by adjusting the dissipation and coupling strength. To overcome the local minima and make the system to escape from the frozen state, we choose R0by reducing the dissipation and coupling strength which forces these fixed points into region 2 to destabilized. Then, the system continues to search forthe ground state with lower energy by gradually increasing the coupling strength",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_496",
    "chunk_position": 496
  },
  {
    "id": "2208.07502v2",
    "text": ". Then, the system continues to search forthe ground state with lower energy by gradually increasing the coupling strength. Wedesignan SNN-CIMalgorithmaccordingto Eq.(1), which evolves under the pumping intensity 1. We reduce the energy of the system by increasing the coupling strength and dissipation coefficient , so that the system transitions from oscillation to stable bifurcation. After each iteration, we record the energy of the system at this moment Etand compare it with the lowest energybeforetheiteration",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_497",
    "chunk_position": 497
  },
  {
    "id": "2208.07502v2",
    "text": ". After each iteration, we record the energy of the system at this moment Etand compare it with the lowest energybeforetheiteration. Iftheenergyishigher than the minimum energy at this time, it means that the system evolution has entered a locally optimal solution. We need to reset the lower coupling strength and dissipation coefficient to make the system enter the unstable region (1 or 2) to re-oscillate to search for the optimal solution. After repeating the above operations, after the total calculation time Nt, SNN-CIM can give an optimal solution or an acceptable suboptimal solution",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_498",
    "chunk_position": 498
  },
  {
    "id": "2208.07502v2",
    "text": ". After repeating the above operations, after the total calculation time Nt, SNN-CIM can give an optimal solution or an acceptable suboptimal solution. The selection of parameters andcan be adjusted according to the fixed point stability of .4(a). Here, we establish a step size of h 0.05, with an initial value of 0 0for the coupling strength and0 0.3for the dissipation strength . During each iteration, we incrementby0.01handby0.1h. In the event that the energy Etdoes not exhibit a reduction during the iteration process, the parameters andare reset to their respective initial values",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_499",
    "chunk_position": 499
  },
  {
    "id": "2208.07502v2",
    "text": ". To give an example of the algorithm, we apply the SNN-CIM the MAX-CUT problem of G1. We start with a graph, in which some of the vertices are connected via edges. The aim of the MAX-CUT problem is to group the vertices into two types with the number of edges as large as possible, whose mathematical expression is generally considered to be to the Ising model as edges 1 4(summationdisplay ij Jijsummationdisplay ij Jijij). (5) In .4(b), the time evolution of the typex DOPO and typekpulses, the energy, and the number of cuts of the system are given",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_500",
    "chunk_position": 500
  },
  {
    "id": "2208.07502v2",
    "text": ". (5) In .4(b), the time evolution of the typex DOPO and typekpulses, the energy, and the number of cuts of the system are given. Specifically, if the energy is frozen in a local minima state, we tend to reduce which makes the system unstable. Then, there is fluctuation of the energy and the system continues to search for the lower energy state. Finally, if the system energy is higher than the lowest energy before this moment, as in .4, the optimal solutions could be Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_501",
    "chunk_position": 501
  },
  {
    "id": "2208.07502v2",
    "text": ". Published under CC-BY 4.0. 5 0 500 1000 1500 200011.051.11.151.2104 -4-3-2-10 0 500 1000 1500 200000.51.52 051015104 0.20 .4 0.60 .8 10.20.40.60.81Iext T2-40 R04 123 (e) Nt ECut 1 N(d) (b) (a) 4: SNN-CIM solves the MAX-CUT problem of G1. a. The effect of and Iexton the stability of the fixed point. The eigenvalues in region 1 and region 4 are real numbers, one has a positive number and the other is all negative. The eigenvalues of region 2 and region 3 are complex conjugates, one is the positive real part and the other is the negative real part. 1.0. b",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_502",
    "chunk_position": 502
  },
  {
    "id": "2208.07502v2",
    "text": ". The eigenvalues of region 2 and region 3 are complex conjugates, one is the positive real part and the other is the negative real part. 1.0. b. Evolution of typex DOPO pulse amplitude with calculation time. c. Evolution of type-kpulse amplitude with calculation time. d. Energy and number of cuts as a function of computation time. e. The coupling strengths and the dissipation as a function of computation time. obtained as edges 11624 near Nt 1350, after that the system will be unstable until another lower energy is searched",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_503",
    "chunk_position": 503
  },
  {
    "id": "2208.07502v2",
    "text": ". obtained as edges 11624 near Nt 1350, after that the system will be unstable until another lower energy is searched. 5 The performance of SNN-CIM MAX-CUT Problem Here in this part, we describe the performance of the SNN-CIM the Max-Cut problem and then compare it with other types of computing devices such as those based on simulated annealing and conventional CIM. We discuss the performance of these investigated devices on different sizes of the Ising problems",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_504",
    "chunk_position": 504
  },
  {
    "id": "2208.07502v2",
    "text": ". We discuss the performance of these investigated devices on different sizes of the Ising problems. We chose the simulated annealing (SA) the SNN-CIM the G1G21 graph of 800 nodes, and the G22-G42 graph of 2000 nodes, and also compared the performance with the conventional CIM and the CIM with sigmoid function optimization in Ref.23. The performance of the computational results are shown in .5. Here we choose the relative error C 100(1CC opt)of the optimal value during50runsastheresult,and Copt)denotesthe best-known value reported in the literature",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_505",
    "chunk_position": 505
  },
  {
    "id": "2208.07502v2",
    "text": ". In the 800 nodes problem, CIM with sigmoid function and SNN-CIM often find the solutions very close to the optimal value, while the performance of the SA and CIM are relatively poor. And, the mean values of the relative errors of SNN-CIM and CIM with sigmoid function in the optimal value of G1-G21 are 0.3and0.4, respectively.The performance of SNN-CIM is slightly better than that of CIM with a sigmoid function. Under the more complex 2000-node problem, SNN-CIM is significantly closer to the optimal value than the other three",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_506",
    "chunk_position": 506
  },
  {
    "id": "2208.07502v2",
    "text": ". Under the more complex 2000-node problem, SNN-CIM is significantly closer to the optimal value than the other three . In addition, for more complex problems with random connectivity and non-uniform node degrees (G39,G40,G41,G42), SNN-CIM with unstable local optima can also achieve better computational performance. This shows that SNN-CIM will have more potential in more complex combinatorial optimization problems. The computational performance of the SNNCIM GSET is also shown in , which is compared with SA and CIM",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_507",
    "chunk_position": 507
  },
  {
    "id": "2208.07502v2",
    "text": ". The computational performance of the SNNCIM GSET is also shown in , which is compared with SA and CIM. We note that the SNN-CIM has an unparalleled advantage in computing time to find suboptimal but adequate solutions for the application, both in the time and accuracy of the solutions. Based on the above analysis, our proposed SNNCIM has excellent performance on combinatorial optimization problems. 6 Conclusion In summary, we propose a spiking neural network consistingof DOPO pulsesanddissipative pulses",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_508",
    "chunk_position": 508
  },
  {
    "id": "2208.07502v2",
    "text": ". 6 Conclusion In summary, we propose a spiking neural network consistingof DOPO pulsesanddissipative pulses. We analyze the fixed points stability of a single spiking neuron and investigate the dynamical behavior under different Iextthrough numerical simulations. The neuron is in an unstable firing state when the smaller coupling term acts as Iext, Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_509",
    "chunk_position": 509
  },
  {
    "id": "2208.07502v2",
    "text": ". 6 0.1 0.2 0.2 0.2 0.1 0.7 0.8 0.3 1.0 0.9 3.3 2.7 2.0 0.4 0.6 0.6 0.6 1.3 1.3 1.5 1.7 0.3 0.2 0.2 0.1 0.2 1.5 0.9 1.2 1.0 1.1 1.6 0.9 1.4 2.0 1.5 2.0 1.9 5.3 5.3 5.8 5.8 0.2 0.2 0.2 0.1 0.2 1.1 1.0 1.0 0.9 0.7 1.6 0.7 1.4 0.8 0.6 0.8 0.7 2.4 2.4 2.6 1.8 0.1 0.1 0.1 0.1 0.1 0.1 0.4 0.1 0.1 0.3 0.9 0.7 0.9 0.7 0.7 0.7 0.7 1.5 1.3 1.5 1.4 0.0 5.0 10.0 G22 G23 G24 G25 G26 G27 G28 G29 G30 G31 G32 G33 G34 G35 G36 G37 G38 G39 G40 G41 G42G22-G42 (2000 node) SA CIM CIM with sigmoid functions SNN-CIM0.2 0.5 0.4 0.3 0.6 1.8 2.8 2.8 2.0 2.6 3.2 3.6 2.4 0.7 0.6 0.9 0.9 0.9 1.8 0.9 1.9 0.1 0.1 0.1 0.1 0.2",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_510",
    "chunk_position": 510
  },
  {
    "id": "2208.07502v2",
    "text": "SA CIM CIM with sigmoid functions SNN-CIM0.2 0.5 0.4 0.3 0.6 1.8 2.8 2.8 2.0 2.6 3.2 3.6 2.4 0.7 0.6 0.9 0.9 0.9 1.8 0.9 1.9 0.1 0.1 0.1 0.1 0.2 0.6 0.4 0.6 0.4 0.5 1.1 0.7 0.7 1.3 1.2 1.4 1.3 3.0 4.0 2.6 3.4 0.0 0.0 0.0 0.0 0.0 0.2 0.1 0.0 0.0 0.0 1.1 0.4 0.7 0.6 0.5 0.6 0.6 1.1 1.4 0.3 1.0 0.0 0.0 0.0 0.0 0.0 0.3 0.0 0.0 0.3 0.2 0.4 0.0 0.3 0.5 0.6 0.6 0.4 0.6 0.7 0.5 1.1 0.0 5.0 10.0 G1 G2 G3 G4 G5 G6 G7 G8 G9 G10 G11 G12 G13 G14 G15 G16 G17 G18 G19 G20 G21G1-G21 (800 node)Difference with Best Known Solution (percent) 5: Computational performance of the SNN-CIM the MAX-CUT problem for a",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_511",
    "chunk_position": 511
  },
  {
    "id": "2208.07502v2",
    "text": "G18 G19 G20 G21G1-G21 (800 node)Difference with Best Known Solution (percent) 5: Computational performance of the SNN-CIM the MAX-CUT problem for a given problem",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_512",
    "chunk_position": 512
  },
  {
    "id": "2208.07502v2",
    "text": ". a. Relative error of the optimal value of simulated annealing algorithm, CIM, CIM with sigmoid function, and SNN-CIM for 50 runs on the MAX-CUT problem of a graph with 800 nodes. b. More complex 2000 node situation. andtheneuronpulsedisappearswiththeincrease of the coupling term, corresponding to the AH bifurcation process, simulating the class-II neurons. Based on the properties of spiking neurons, an SNN-CIM solving combinatorial optimizationproblemsisproposed",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_513",
    "chunk_position": 513
  },
  {
    "id": "2208.07502v2",
    "text": ". Based on the properties of spiking neurons, an SNN-CIM solving combinatorial optimizationproblemsisproposed. Byincreasing the coupling strength, the neuron cluster in the firing state is bifurcated to search for the ground state of the Ising model, and a nonlinear filter function is used to eliminate the amplitude heterogeneity. For the local minimum point frozen in the excited state during the filtering process, it can be destabilized by adjusting the dissipation parameter, so as to continuously search for the ground state",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_514",
    "chunk_position": 514
  },
  {
    "id": "2208.07502v2",
    "text": ". We also compare the performance ofthe SNN-CIMalgorithmoncombinatorialoptimization problems with SA, CIM, and CIM with sigmoid function. The results show that SNNCIM has excellent performance on combinatorial optimization problems, and has more potential on more complex problems with random connectivity and non-uniform node degrees",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_515",
    "chunk_position": 515
  },
  {
    "id": "2208.07502v2",
    "text": ". In addition, the model can be implemented by adding dissipative pulses to the existing CIM based on the fiber ring cavity, which is expected to improve its computational performance and solve the problem of freezing in the excited state during the calculation process",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_516",
    "chunk_position": 516
  },
  {
    "id": "2208.07502v2",
    "text": ". By using integrated photonics direct coupling and high repetition frequency lasers, it is expected to increase the pulse firing rate and the number of neurons, improve computing power, and provide a better simulation platform for spiking neural networks.Finally, the use of the sigmoid function for the coupled term response in this model, which is widely used in neural networks, creates an interesting connection between CIM and neuromorphic computing, with the potential to excite the research in the perceptron and pattern recognition of fiber-rings cavity based CIM",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_517",
    "chunk_position": 517
  },
  {
    "id": "2208.07502v2",
    "text": ". 7 Acknowledgement This research was funded by the National Natural Science Foundation of China (Grants Nos. 62131002, 62371050), and the Fundamental Research Funds for the Central Universities (BNU). 8 Data availability The GSET instances used for the tests are available at yyyeyyye Gset. The SA adjustment is based on a logarithmic function, TT0log2(1 t0t). All programs are written by Matlab and run on intel i7-10700F 2.9GHz. References 1 Douglas B Kitchen, Hlne Decornez, John RFurr, and Jrgen Bajorath. Docking and scoring in virtual screening for drug discovery: methods and applications",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_518",
    "chunk_position": 518
  },
  {
    "id": "2208.07502v2",
    "text": ". Docking and scoring in virtual screening for drug discovery: methods and applications. Nature reviews Drug discovery 3, 935949 (2004). Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0. 7 : Computational performance of SA, CIM, and SNN-CIM on GSET. TTS is a time-to-solution of reaching 98 percent of the best solution. The optimal solutions for SA, CIM, and SNN-CIM are obtained under 10, 50, and 50 runs, respectively. Values in parentheses indicate that the results of TTS 98cannot be obtained, and are replaced by TTS 95",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_519",
    "chunk_position": 519
  },
  {
    "id": "2208.07502v2",
    "text": ". The best maximum cuts are known from Ref.39 ID N Jij Edge Copt CSACCIMCSCTTS STTS CITTS S 1 8000,119176 11624 11604 11597 11624 16.1514 3.8065 0.0373 6 8000,119176 2178 2138 2149 2172 20.4624 4.0332 1.3423 11 8000,11600 564 546 550 562 (8.46337) (0.4107) 0.4775 14 8000,14694 3064 3042 2993 3049 11.9894 (1.8703) 0.0786 18 8000,14694 992 983 942 986 20.4379 (8.418) 1.52 22 20000,119990 13359 13340 13279 13352 2169.69 14.048 0.9654 27 20000,119990 3341 3316 3271 3339 2760.23 (23.568) 27.768 32 20000,14000 1410 1364 1364 1398 (2909.38) (7.1411) 32.101 35 20000,111778 7684 7656 7448 7634 1644.46",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_520",
    "chunk_position": 520
  },
  {
    "id": "2208.07502v2",
    "text": "3341 3316 3271 3339 2760.23 (23.568) 27.768 32 20000,14000 1410 1364 1364 1398 (2909.38) (7.1411) 32.101 35 20000,111778 7684 7656 7448 7634 1644.46 (14.806) 1.2013 2 Amparo Soler-Dominguez, Angel A Juan, and Renatas Kizys",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_521",
    "chunk_position": 521
  },
  {
    "id": "2208.07502v2",
    "text": ". A survey on financial applications of metaheuristics. ACM Computing Surveys (CSUR) 50, 123 (2017). url: 3 Francisco Barahona, Martin Grtschel, Michael Jnger, and Gerhard Reinelt. An application of combinatorial optimization to statistical physics and circuit layout design. Operations Research 36, 493513 (1988). ar Xiv: 4 Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach (2009). 5 Tadashi Kadowaki and Hidetoshi Nishimori. Quantum annealing in the transverse ising model. Physical Review E 58, 5355 (1998). url: Rev E.58.5355. 6 Arnab Das and Bikas K Chakrabarti",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_522",
    "chunk_position": 522
  },
  {
    "id": "2208.07502v2",
    "text": ". Quantum annealing in the transverse ising model. Physical Review E 58, 5355 (1998). url: Rev E.58.5355. 6 Arnab Das and Bikas K Chakrabarti. Colloquium: Quantum annealing and analog quantum computation. Reviews of Modern Physics 80, 1061 (2008). url: Mod Phys.80.1061. 7 Zhe Wang, Alireza Marandi, Kai Wen, Robert L Byer, and Yoshihisa Yamamoto. Coherent ising machine based on degenerate optical parametric oscillators. Physical Review A 88, 063853 (2013). url: Rev A.88.063853. 8 Brian Sutton, Kerem Yunus Camsari, Behtash Behin-Aein, and Supriyo Datta",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_523",
    "chunk_position": 523
  },
  {
    "id": "2208.07502v2",
    "text": ". Physical Review A 88, 063853 (2013). url: Rev A.88.063853. 8 Brian Sutton, Kerem Yunus Camsari, Behtash Behin-Aein, and Supriyo Datta. In-trinsic optimization using stochastic nanomagnets. Scientific reports 7, 19 (2017). url: 9 Michael Saccone, Francesco Caravelli, Kevin Hofhuis, Sergii Parchenko, Yorick A Birkhlzer, Scott Dhuey, Armin Kleibert, Sebastiaan Van Dijken, Cristiano Nisoli, and Alan Farhan. Direct observation of a dynamical glass transition in a nanomagnetic artificial hopfield network. Nature Physics18, 517521 (2022)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_524",
    "chunk_position": 524
  },
  {
    "id": "2208.07502v2",
    "text": ". Direct observation of a dynamical glass transition in a nanomagnetic artificial hopfield network. Nature Physics18, 517521 (2022). 10 Ryan Hamerly, Takahiro Inagaki, Peter L Mc Mahon, Davide Venturelli, Alireza Marandi, Tatsuhiro Onodera, Edwin Ng, Carsten Langrock, Kensuke Inaba, Toshimori Honjo, et al. Experimental investigation of performance differences between coherent ising machines and a quantum annealer. Science advances 5, eaau0823 (2019). 11 Alireza Marandi, Zhe Wang, Kenta Takata, Robert L Byer, and Yoshihisa Yamamoto",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_525",
    "chunk_position": 525
  },
  {
    "id": "2208.07502v2",
    "text": ". Science advances 5, eaau0823 (2019). 11 Alireza Marandi, Zhe Wang, Kenta Takata, Robert L Byer, and Yoshihisa Yamamoto. Network of time-multiplexed optical parametric oscillators as a coherent ising machine. Nature Photonics 8, 937942 (2014). url: 12 Takahiro Inagaki, Kensuke Inaba, Ryan Hamerly, Kyo Inoue, Yoshihisa Yamamoto, and Hiroki Takesue. Large-scale ising spin network based on degenerate optical paraAccepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0. 8 metric oscillators. Nature Photonics 10, 415419 (2016)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_526",
    "chunk_position": 526
  },
  {
    "id": "2208.07502v2",
    "text": ". Published under CC-BY 4.0. 8 metric oscillators. Nature Photonics 10, 415419 (2016). 13 Peter L Mc Mahon, Alireza Marandi, Yoshitaka Haribara, Ryan Hamerly, Carsten Langrock, Shuhei Tamate, Takahiro Inagaki, Hiroki Takesue, Shoko Utsunomiya, Kazuyuki Aihara, et al. A fully programmable 100spin coherent ising machine with all-to-all connections. Science 354, 614617 (2016). 14 Takahiro Inagaki, Yoshitaka Haribara, Koji Igarashi, Tomohiro Sonobe, Shuhei Tamate, Toshimori Honjo, Alireza Marandi, Peter L Mc Mahon, Takeshi Umeki, Koji Enbutsu, et al",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_527",
    "chunk_position": 527
  },
  {
    "id": "2208.07502v2",
    "text": ". A coherent ising machine for 2000-node optimization problems. Science 354, 603606 (2016). url: 15 Toshimori Honjo, Tomohiro Sonobe, Kensuke Inaba, Takahiro Inagaki, Takuya Ikuta, Yasuhiro Yamada, Takushi Kazama, Koji Enbutsu, Takeshi Umeki, Ryoichi Kasahara, et al. 100,000-spin coherent ising machine. Science advances 7, eabh0952 (2021). 16 Fabian, Bhm, Guy, Verschaffelt, Van, der, and Sande. A poor mans coherent ising machine based on opto-electronic feedback systems for solving optimization problems.. Nature communications 10, 3538 3538 (2019)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_528",
    "chunk_position": 528
  },
  {
    "id": "2208.07502v2",
    "text": ". Nature communications 10, 3538 3538 (2019). url: 17 Qizhuang Cen, Tengfei Hao, Hao Ding, Shanhong Guan, Zhiqiang Qin, Kun Xu, Yitang Dai, and Ming Li. Microwave photonic ising machine (2020). ar Xiv:2011.00064. 18 D Pierangeli, G Marcucci, and C Conti. Large-scale photonic ising machine by spatial light modulation. Physical Review Letters 122, 213902 (2019). url: Rev Lett.122.213902. 19 Yisheng Fang, Junyi Huang, and Zhichao Ruan. Experimental observation of phase transitions in spatial photonic ising machine. Physical Review Letters 127, 043902 (2021)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_529",
    "chunk_position": 529
  },
  {
    "id": "2208.07502v2",
    "text": ". Experimental observation of phase transitions in spatial photonic ising machine. Physical Review Letters 127, 043902 (2021). 20 Wenchen Sun, Wenjia Zhang, Yuanyuan Liu, Qingwen Liu, and Zuyuan He. Quadrature photonic spatial ising machine. Optics Letters47, 14981501 (2022). 21 Yoshitomo Okawachi, Mengjie Yu, Jae K Jang, Xingchen Ji, Yun Zhao, Bok Young Kim, Michal Lipson, and Alexander LGaeta. Demonstration of chip-based coupled degenerate optical parametric oscillators for realizing a nanophotonic spin-glass. Nature communications 11, 17 (2020)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_530",
    "chunk_position": 530
  },
  {
    "id": "2208.07502v2",
    "text": ". Nature communications 11, 17 (2020). 22 Timothe Leleu, Yoshihisa Yamamoto, Shoko Utsunomiya, and Kazuyuki Aihara. Combinatorial optimization using dynamical phase transitions in driven-dissipative systems. Phys. Rev. E 95, 022118 (2017). 23 Fabian Bhm, Thomas Van Vaerenbergh, Guy Verschaffelt, and Guy Van der Sande. Order-of-magnitude differences in computational performance of analog ising machines induced by the choice of nonlinearity. Communications Physics 4, 111 (2021). 24 Sam Reifenstein, Satoshi Kako, Farad Khoyratee, Timothee Leleu, and Yoshihisa Yamamoto",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_531",
    "chunk_position": 531
  },
  {
    "id": "2208.07502v2",
    "text": ". Communications Physics 4, 111 (2021). 24 Sam Reifenstein, Satoshi Kako, Farad Khoyratee, Timothee Leleu, and Yoshihisa Yamamoto. Coherent ising machines with optical error correction circuits. Advanced Quantum Technologies 4, 2100077 (2021). 25 Timothe Leleu, Yoshihisa Yamamoto, Peter L. Mc Mahon, and Kazuyuki Aihara. Destabilization of local minima in analog spin systems by correction of amplitude heterogeneity. Phys. Rev. Lett. 122, 040607 (2019). 26 Satoshi Kako, Timothe Leleu, Yoshitaka Inui, Farad Khoyratee, Sam Reifenstein, and Yoshihisa Yamamoto",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_532",
    "chunk_position": 532
  },
  {
    "id": "2208.07502v2",
    "text": ". Phys. Rev. Lett. 122, 040607 (2019). 26 Satoshi Kako, Timothe Leleu, Yoshitaka Inui, Farad Khoyratee, Sam Reifenstein, and Yoshihisa Yamamoto. Coherent ising machines with error correction feedback. Advanced Quantum Technologies 3, 2000045 (2020). 27 Timothe Leleu, Farad Khoyratee, Timothe Levi, Ryan Hamerly, Takashi Kohno, and Kazuyuki Aihara. Scaling advantage of chaotic amplitude control for highperformance combinatorial optimization. Communications Physics 4, 110 (2021). 28 Marcello Calvanese Strinati and Claudio Conti. Multidimensional hyperspin machine",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_533",
    "chunk_position": 533
  },
  {
    "id": "2208.07502v2",
    "text": ". Communications Physics 4, 110 (2021). 28 Marcello Calvanese Strinati and Claudio Conti. Multidimensional hyperspin machine. Nature Communications 13, 7248 (2022). 29 Takahiro Inagaki, Kensuke Inaba, Timothe Leleu, Toshimori Honjo, Takuya Ikuta, Koji Enbutsu, Takeshi Umeki, Ryoichi Kasahara, Kazuyuki Aihara, and Hiroki Takesue. Collective and synchronous dynamics of photonic spiking neurons. Nature communications12, 18 (2021). 30 Bo Lu, Chen-Rui Fan, Lu Liu, Kai Wen, and Chuan Wang. Speed-up coherent ising maAccepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_534",
    "chunk_position": 534
  },
  {
    "id": "2208.07502v2",
    "text": ". Speed-up coherent ising maAccepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0. 9 chine with a spiking neural network. Optics Express31, 36763684 (2023). 31 Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models. Neural networks 10, 1659 1671 (1997). 32 Romain Brette, Michelle Rudolph, Ted Carnevale, Michael Hines, David Beeman, James M Bower, Markus Diesmann, Abigail Morrison, Philip H Goodman, Frederick C Harris, etal. Simulationofnetworksofspiking neurons: a review of tools and strategies",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_535",
    "chunk_position": 535
  },
  {
    "id": "2208.07502v2",
    "text": ". Simulationofnetworksofspiking neurons: a review of tools and strategies. Journal of computational neuroscience 23, 349398 (2007). 33 Eugene M Izhikevich. Simple model of spiking neurons. IEEE Transactions on neural networks14, 15691572 (2003). 34 Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature 518, 529533 (2015). 35 Qiang Yu, Kay Chen Tan, and Huajin Tang",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_536",
    "chunk_position": 536
  },
  {
    "id": "2208.07502v2",
    "text": ". Human-level control through deep reinforcement learning. Nature 518, 529533 (2015). 35 Qiang Yu, Kay Chen Tan, and Huajin Tang. Pattern recognition computation in a spiking neural network with temporal encoding and learning. In The 2012 International Joint Conference on Neural Networks (IJCNN). 17. IEEE (2012). 36 Roger Rodriguez and Henry C Tuckwell. Statisticalpropertiesofstochasticnonlinear dynamical models of single spiking neurons andneuralnetworks. Physical Review E 54, 5585 (1996). 37 Omri Harish and David Hansel. Asynchronous rate chaos in spiking neuronal circuits",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_537",
    "chunk_position": 537
  },
  {
    "id": "2208.07502v2",
    "text": ". Physical Review E 54, 5585 (1996). 37 Omri Harish and David Hansel. Asynchronous rate chaos in spiking neuronal circuits. PLo S computational biology 11, e1004266 (2015). 38 Fabian Bhm, Takahiro Inagaki, Kensuke Inaba, Toshimori Honjo, Koji Enbutsu, Takeshi Umeki, Ryoichi Kasahara, and Hiroki Takesue. Understanding dynamics of coherent ising machines through simulation of large-scale 2d ising models. Nature communications 9, 19 (2018). 39 Fuda Ma and Jin-Kao Hao. A multiple search operator heuristic for the max-k-cut problem. Annals of Operations Research 248, 365403 (2017)",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_538",
    "chunk_position": 538
  },
  {
    "id": "2208.07502v2",
    "text": ". 39 Fuda Ma and Jin-Kao Hao. A multiple search operator heuristic for the max-k-cut problem. Annals of Operations Research 248, 365403 (2017). Accepted in Quantum2023-10-19, click title to verify. Published under CC-BY 4.0. 10",
    "source": "arXiv",
    "chunk_id": "2208.07502v2_539",
    "chunk_position": 539
  },
  {
    "id": "2404.14964v3",
    "text": "ELUCIDATING THE THEORETICAL UNDERPINNINGS OF SURROGATE GRADIENT LEARNING IN SPIKING NEURAL NETWORKS Julia Gygax1,2 Friedemann Zenke1,2 1Friedrich Miescher Institute for Biomedical Research 2Faculty of Science, University of Basel Basel, Switzerland firstname.lastnamefmi.ch ABSTRACT Training spiking neural networks to approximate universal functions is essential for studying information processing in the brain and for neuromorphic computing. Yet the binary nature of spikes poses a challenge for direct gradient-based training",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_540",
    "chunk_position": 540
  },
  {
    "id": "2404.14964v3",
    "text": ". Yet the binary nature of spikes poses a challenge for direct gradient-based training. Surrogate gradients have been empirically successful in circumventing this problem, but their theoretical foundation remains elusive. Here, we investigate the relation of surrogate gradients to two theoretically well-founded approaches. On the one hand, we consider smoothed probabilistic models, which, due to the lack of support for automatic differentiation, are impractical for training multi-layer spiking neural networks but provide derivatives to surrogate gradients for single neurons",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_541",
    "chunk_position": 541
  },
  {
    "id": "2404.14964v3",
    "text": ". On the other hand, we investigate stochastic automatic differentiation, which is compatible with discrete randomness but has not yet been used to train spiking neural networks. We find that the latter gives surrogate gradients a theoretical basis in stochastic spiking neural networks, where the surrogate derivative matches the derivative of the neuronal escape noise function. This finding supports the effectiveness of surrogate gradients in practice and suggests their suitability for stochastic spiking neural networks",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_542",
    "chunk_position": 542
  },
  {
    "id": "2404.14964v3",
    "text": ". This finding supports the effectiveness of surrogate gradients in practice and suggests their suitability for stochastic spiking neural networks. However, surrogate gradients are generally not gradients of a surrogate loss despite their relation to stochastic automatic differentiation. Nevertheless, we empirically confirm the effectiveness of surrogate gradients in stochastic multi-layer spiking neural networks and discuss their relation to deterministic networks as a special case",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_543",
    "chunk_position": 543
  },
  {
    "id": "2404.14964v3",
    "text": ". Our work gives theoretical support to surrogate gradients and the choice of a suitable surrogate derivative in stochastic spiking neural networks. Keywords Spiking neural networks surrogate gradients stochastic automatic differentiation stochastic spiking neural networks 1 Introduction Our brains efficiently process information in spiking neural networks ( SNN s) that communicate through short stereotypical electrical pulses called spikes. SNN s are an indispensable tool for understanding information processing in the brain and instantiating similar capabilities in silico",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_544",
    "chunk_position": 544
  },
  {
    "id": "2404.14964v3",
    "text": ". SNN s are an indispensable tool for understanding information processing in the brain and instantiating similar capabilities in silico. Like conventional artificial neural networks ( ANN s),SNN s require training to implement specific functions. However, SNN models, which are commonly simulated in discrete time 1, are not differentiable due to the binary nature of the spike, which precludes the use of standard gradient-based training techniques based on back-propagation ( BP) 2. There are several ways to overcome this problem",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_545",
    "chunk_position": 545
  },
  {
    "id": "2404.14964v3",
    "text": ". There are several ways to overcome this problem. One can dispense with hidden layers altogether 3, 4, but this limits the networks expressivity and precludes solving more complex tasks. Alternatively, one makes the neuron model differentiable 5 or considers the timing of existing spikes for which gradients exist 69. This approach, however, requires additional mechanisms to create or remove spikes, as well as dealing with silent neurons, where the spike time is not defined. Finally, one can replace the gradient with a suitable surrogate 1, 1015. In this article, we focus on the latter",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_546",
    "chunk_position": 546
  },
  {
    "id": "2404.14964v3",
    "text": ". Finally, one can replace the gradient with a suitable surrogate 1, 1015. In this article, we focus on the latter. Surrogate gradient ( SG) approaches are empirically successful and not limited to changing the timing of existing spikes while working with non-differentiable neuron models.ar Xiv:2404.14964v3 cs.NE 17 Nov 2024 However, at present SGs are a heuristic that lacks a theoretical foundation. Consequently, we still do not understand why SGdescent works well in practice 16",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_547",
    "chunk_position": 547
  },
  {
    "id": "2404.14964v3",
    "text": ". Consequently, we still do not understand why SGdescent works well in practice 16. We do not know if it is optimal in any sense, nor do we know if there are better strategies for updating the weights. To address this gap in our understanding we analyze the commonalities and differences of SGs with theoretically well-founded approaches based on stochastic networks. Specifically, we focus on smoothed probabilistic models ( SPM s) 15, 17 and the more recently proposed stochastic automatic differentiation (stoch AD ) framework 18",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_548",
    "chunk_position": 548
  },
  {
    "id": "2404.14964v3",
    "text": ". SPM s typically rely on stochasticity to smooth out the optimization landscape in expectation to enable gradient computation on this smoothed loss. In this article, we particularly focus on neuron models with escape noise 19 which are commonly used to smooth the non-differentiable spikes in expectation and for which exact gradients are computable 20, 21. However, extending SPM s to multi-layer neural networks has been difficult because they preclude using BPand hence efficient gradient estimation requires additional approximations 22",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_549",
    "chunk_position": 549
  },
  {
    "id": "2404.14964v3",
    "text": ". In contrast, stoch AD is a recently developed framework for automatic differentiation ( AD) in programs with discrete randomness, i.e., with discrete random variables. While this opens the door for BPor other recursive gradient computation schemes, the framework has not yet been applied to SNNs. Here, we jointly analyze the above methods, elucidate their relations, provide a rigorous theoretical foundation for SGdescent in stochastic SNN s, and discuss their implications for deterministic SNN s",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_550",
    "chunk_position": 550
  },
  {
    "id": "2404.14964v3",
    "text": ". We first provide some background information on each of the above methods before starting our analysis with the case of a single Perceptron. From there, we move to the more complex case of multi-layer Perceptrons (MLPs), where we elaborate the theoretical connection between SGs and stoch AD , and end with multi-layer networks of leaky integrate-and-fire ( LIF) neurons. Finally, we substantiate the theoretical results with empirical simulations",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_551",
    "chunk_position": 551
  },
  {
    "id": "2404.14964v3",
    "text": ". Finally, we substantiate the theoretical results with empirical simulations. 2 Background on surrogate gradients, smoothed probabilistic models, and stochastic automatic differentiation Before analyzing the relation between SGs, gradients of the log-likelihood in SPM s, and stoch AD in , we briefly review these methods here. While their common goal is to compute reasonable weight updates, the first two approaches are specifically designed for training SNN s. The third approach aims to compute unbiased gradients of arbitrary functions with discrete randomness based on AD",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_552",
    "chunk_position": 552
  },
  {
    "id": "2404.14964v3",
    "text": ". The third approach aims to compute unbiased gradients of arbitrary functions with discrete randomness based on AD. Smoothed probabilistic models. SPM s are based on stochastic networks in which the gradients are well-defined in expectation 15, 17. Typically such networks consist of a noisy neuron model, such as the LIFneuron with escape noise 19 and a probabilistic loss function. On the one hand, this includes models that require optimizing the log-likelihood of the target spike train being generated by the current model 20. However, this method only works effectively without hidden units",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_553",
    "chunk_position": 553
  },
  {
    "id": "2404.14964v3",
    "text": ". However, this method only works effectively without hidden units. On the other hand, SPM s also comprises models that follow a variational strategy 21, which makes them applicable to networks with a single hidden layer. In general, the gradients that are computed within the SPM framework are given by w E, where Lis the loss and wis an arbitrary model parameter. The computational cost associated with evaluating the expected value of the loss ELprecludes training multi-layer networks in practice. This is because SPM s lack support for AD, as we will explain in detail in .2.1",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_554",
    "chunk_position": 554
  },
  {
    "id": "2404.14964v3",
    "text": ". This is because SPM s lack support for AD, as we will explain in detail in .2.1. Surrogate gradients. SGs are a heuristic that relies on a continuous relaxation of the non-differentiable spiking activation function that occurs when computing gradients in SNN s 15, and is commonly applied in deterministic networks. To that end, one systematically replaces the derivative of the hard threshold by a surrogate derivative ( SD), also called pseudo-derivative 14, when applying the chain rule. The result then serves as a surrogate for the gradient",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_555",
    "chunk_position": 555
  },
  {
    "id": "2404.14964v3",
    "text": ". The result then serves as a surrogate for the gradient. For example, when computing the derivative of the spike train Swith respect to a weight w, the problematic derivative of a Heaviside uis replaced by a SD e S wS uz SDu w, (1) where the tilde denotes the introduction of the surrogate. denotes the Heaviside function, uis the membrane potential, is the firing threshold, and SGis a differentiable function parametrized by SGused to define the SD. Different functions are used in practice",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_556",
    "chunk_position": 556
  },
  {
    "id": "2404.14964v3",
    "text": ". Different functions are used in practice. For instance, the derivative of rectified linear functions or the arctangent function have been used successfully as SDs 6, 12, 14, 23, whereas Super Spike 13 used a scaled derivative of a fast sigmoid 1 (SGx1)2withSGcontrolling its steepness. However, SNN training is robust to the choice of the SD 16, 24. 2 Stochastic surrogate gradients. In practice, SGs are most commonly used for training deterministic SNN s and have rarely been used in stochastic SNN s, with some notable exceptions 22",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_557",
    "chunk_position": 557
  },
  {
    "id": "2404.14964v3",
    "text": ". Here we define stochastic SGdescent analogously to the deterministic case as a continuous relaxation of the non-differentiable spiking activation function applied to networks of stochastic neurons with escape noise 19, 25. In these networks, spike generation is stochastic in the forward path, whereby an escape noise function yields the neuronal firing probability. , one can formulate stochastic neurons as neurons with a stochastic threshold 25. For the backward path, one considers the derivative of a differentiable function of the membrane potential",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_558",
    "chunk_position": 558
  },
  {
    "id": "2404.14964v3",
    "text": ". For the backward path, one considers the derivative of a differentiable function of the membrane potential. Analogous to the deterministic case, the derivative of a spike train is hence given as e S wS uz SDu w, (2) where the mean threshold is to in the deterministic case (for details see Methods .2). In stochastic binary neural networks, the above notion of SGs is known as the straight-through estimator ( STE) 11, 2628. Stochastic automatic differentiation. To compute derivatives efficiently, popular ADalgorithms rely on the chain rule",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_559",
    "chunk_position": 559
  },
  {
    "id": "2404.14964v3",
    "text": ". Stochastic automatic differentiation. To compute derivatives efficiently, popular ADalgorithms rely on the chain rule. The required terms can be calculated in forward or backward mode, with the latter being called BP29. To extend this to exact ADin programs with discrete randomness, which otherwise preclude analytical gradient computation, thestoch AD framework 18 introduces stochastic derivatives",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_560",
    "chunk_position": 560
  },
  {
    "id": "2404.14964v3",
    "text": ". This framework provides an unbiased and composable solution to compute derivatives in discrete stochastic systems that does not suffer from the high variance found in methods that require sampling and averaging such as e.g. finite differences methods (cf. . 3 and .2.2 for a comparison). To deal with discrete randomness, stochastic derivatives consider not only infinitesimally small continuous changes but also finite changes with infinitesimally small probability. As defined by Arya et al. 18, a stochastic derivative of a random variable consists of a triple (, w, Y )",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_561",
    "chunk_position": 561
  },
  {
    "id": "2404.14964v3",
    "text": ". As defined by Arya et al. 18, a stochastic derivative of a random variable consists of a triple (, w, Y ). Here is the almost sure derivative, meaning the derivative of the continuous part, wis the derivative of the probability of a finite change and Yis the alternate value, i.e. the value of in case of a finite jump. Although stochastic derivatives are suitable for forward mode AD, they cannot be used directly for BPas of now. This is because the application of the chain rule would require the derivatives to be scalars, whereas they consist of a triple",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_562",
    "chunk_position": 562
  },
  {
    "id": "2404.14964v3",
    "text": ". This is because the application of the chain rule would require the derivatives to be scalars, whereas they consist of a triple. The stoch AD framework outlines a way to convert them to a single scalar value, which the authors call the smoothed stochastic derivative. This transformation renders the framework compatible with BP albeit at the cost of introducing bias. Given a stochastic derivative, its smoothed stochastic derivative is E) (3) for one realization of the random variable . Arya et al",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_563",
    "chunk_position": 563
  },
  {
    "id": "2404.14964v3",
    "text": ". Given a stochastic derivative, its smoothed stochastic derivative is E) (3) for one realization of the random variable . Arya et al. 18 also showed that the smoothed stochastic derivatives recover the STE 26, 27 for a Bernoulli random variable in their example A.8. 3 Analysis of the relation between SGs, SPMs, and stoch AD for direct training of SNNs To fathom the theoretical foundation of SGlearning, we focused on the theoretically well-grounded SPM s and the recently proposed stoch AD framework",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_564",
    "chunk_position": 564
  },
  {
    "id": "2404.14964v3",
    "text": ". Because both approaches assume stochasticity whereas SGs are typically applied in deterministic settings, we focus our analysis on stochastic networks (cf. ). We will later discuss deterministic networks as a special case. To keep our analysis general and independent of the choice of the loss function L, we consider the Jacobian wy defined at the networks output ywhere ware the trainable parameters or weights. For simplicity and without loss of generality, we consider networks with only one output such that the above is to studying y w1y, . . . , wny",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_565",
    "chunk_position": 565
  },
  {
    "id": "2404.14964v3",
    "text": ". For simplicity and without loss of generality, we consider networks with only one output such that the above is to studying y w1y, . . . , wny . To further ease the analysis, we start with binary Perceptrons, thereby neglecting all temporal dynamics and the reset of conventional spiking neuron models, while retaining the essential binary spike generation process (see . 1A, Methods .2). We begin our comparison by examining a single neuron before moving on to MLP s. We defer the discussion of LIF neurons to",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_566",
    "chunk_position": 566
  },
  {
    "id": "2404.14964v3",
    "text": ". 1A, Methods .2). We begin our comparison by examining a single neuron before moving on to MLP s. We defer the discussion of LIF neurons to . 3.1 Analysis of the binary Perceptron Before we compare the different methods of computing gradients or SGs, we define our deterministic and stochastic Perceptron here. 3 Ber Deterministic Stochastic A B C D 1. SDs are to derivatives of expected outputs in SPM s and smoothed stochastic derivatives in binary Perceptrons. (A) Membrane potential dynamics of an LIFneuron (maroon) in comparison to the Perceptron",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_567",
    "chunk_position": 567
  },
  {
    "id": "2404.14964v3",
    "text": ". (A) Membrane potential dynamics of an LIFneuron (maroon) in comparison to the Perceptron. When input spikes (from input neurons nin1andnin2) are received, they excite the LIFneuron which causes the membrane potential to increase. Once it reaches the threshold, output spikes noutare emitted. In the limit of a large simulation time step ( dtmem) and appropriate scaling of the input currents, the LIFneuron approximates a Perceptron receiving time-locked input (gray line). (B)Left: The simplified computational graph of a deterministic (blue) and a stochastic (yellow) Perceptron",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_568",
    "chunk_position": 568
  },
  {
    "id": "2404.14964v3",
    "text": ". (B)Left: The simplified computational graph of a deterministic (blue) and a stochastic (yellow) Perceptron. Right: Forward pass in a deterministic (top) or stochastic (bottom) Perceptron. The colored arrows indicate that both use the derivative of Son the backward pass, which is the derivative of the expected output of the stochastic neuron in case SGN. In the case of the deterministic neuron, this constitutes the SGused instead of the non-existing derivative of the step function. (C)Left: Network output over multiple trials in the deterministic Perceptron",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_569",
    "chunk_position": 569
  },
  {
    "id": "2404.14964v3",
    "text": ". (C)Left: Network output over multiple trials in the deterministic Perceptron. Right: The S is the derivative of a sigmoid (gray), which is used to approximate the non-existing derivative of the step function (black). (D)Same as (C) but for the stochastic Perceptron. Left: Escape noise leads to variability in the spike trains over trials. Right: The expected output follows a sigmoid, and we can compute the derivative (yellow) of the expected output. Deterministic Perceptron",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_570",
    "chunk_position": 570
  },
  {
    "id": "2404.14964v3",
    "text": ". Right: The expected output follows a sigmoid, and we can compute the derivative (yellow) of the expected output. Deterministic Perceptron. The deterministic Perceptron is defined as u WTxb (4) y, (5) where is again the Heaviside step function and uis the membrane potential, which depends on the weights W, the biasb, the input x, and the firing threshold . The Jacobian is related to the gradient via L wi L yy wi, where the problematic derivative of the non-differentiable Heaviside function appears iny wiwithwibeing the ith weight",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_571",
    "chunk_position": 571
  },
  {
    "id": "2404.14964v3",
    "text": ". When computing SGs the derivative of the Heaviside function is replaced with the corresponding SD e wi wiS. (6) Here we chose the SD as the derivative of a sigmoid with steepness SG, hence S 1 1ex). Stochastic Perceptron. To see how the above expression compares to SDs and derivatives of the expected output in SPMs in the corresponding stochastic setting, we consider the stochastic Perceptron u WTxb p (7) y Be. Note that the membrane potential uis to the deterministic case (cf. Eq. (4)), as the models share the same input. To model stochasticity, we add escape noise",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_572",
    "chunk_position": 572
  },
  {
    "id": "2404.14964v3",
    "text": ". Eq. (4)), as the models share the same input. To model stochasticity, we add escape noise. This means the Perceptron fires with a probability p, which is a function f 4 of the membrane potential. To that end, we choose as the sigmoid function 1 1ex), where Ncontrols the steepness of the escape noise function 21. Importantly, there is some degree of freedom as to which escape noise function we choose. Other common choices in the realm of spiking neurons are the exponential or error function 19",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_573",
    "chunk_position": 573
  },
  {
    "id": "2404.14964v3",
    "text": ". Other common choices in the realm of spiking neurons are the exponential or error function 19. For our current choice, the sigmoid, we find that it approaches the step function in the limit N , and the stochastic Perceptron becomes deterministic. Let us now compare SGdescent in a deterministic and a stochastic unit. To do so, we first incorporate the spike generation of the stochastic binary Perceptron into the threshold and reformulate the stochastic firing using . The stochastic threshold is given by , where is randomly drawn noise from a zero-mean distribution",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_574",
    "chunk_position": 574
  },
  {
    "id": "2404.14964v3",
    "text": ". The stochastic threshold is given by , where is randomly drawn noise from a zero-mean distribution. This formulation is to stochastic firing as in Eq. (7)for a suitable choice of the distribution underlying (cf. Eq. (25)). The stochastic SDis then as in Eq. (2). Since the input and the weights are the same in both cases, both units have the same membrane potential. Because the SDdepends only on the membrane potential the resulting weight updates are provided that both use the same San)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_575",
    "chunk_position": 575
  },
  {
    "id": "2404.14964v3",
    "text": ". Because the SDdepends only on the membrane potential the resulting weight updates are provided that both use the same San). Thus, the resulting SDs in the stochastic and deterministic Perceptron are although the outputs of the two units are generally different (. 1C, D), and hence the overall weight update is different. We now turn to comparing SGs to SPM s. In SPM s, the idea is to compute the derivative of the expected loss at the output to smooth out the discrete spiking non-linearity 20",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_576",
    "chunk_position": 576
  },
  {
    "id": "2404.14964v3",
    "text": ". In SPM s, the idea is to compute the derivative of the expected loss at the output to smooth out the discrete spiking non-linearity 20. In our setup, this amounts to computing the expected output of the stochastic Perceptron, Ey , and taking its derivative wi Ey wi. (8) We note that the right-hand sides of Expressions (8)anare the same when setting NSG. Thus, the derivative of the expected output of the stochastic Perceptron is to the SDof the output of the Perceptron, if the SDis chosen such that it matches the derivative of the neuronal escape noise (see . 1B)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_577",
    "chunk_position": 577
  },
  {
    "id": "2404.14964v3",
    "text": ". 1B). Finally, we compare the above findings to the derivative obtained from the stoch AD framework. To that end, we first apply the chain rule given one realization of y y wi p Be wip (9) and use the smoothed stochastic derivative of a Bernoulli random variable following the steps of Arya et al. 18. According to their example A.8 18, the right stochastic derivative for a Bernoulli random variable is given by (R, w R, YR) (0 ,1 1p,1)if the outcome of the random variable was zero ( Be 0 ) and zero otherwise",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_578",
    "chunk_position": 578
  },
  {
    "id": "2404.14964v3",
    "text": ". The left stochastic derivative is given by (L, w L, YL) (0 ,1 p,0)if Be 1 and also zero otherwise. The corresponding smoothed versions are eR1 1p10andeL1 p11(cf. Arya et al. 18 or Eq. (3)) . Since every affine combination of the left and right derivatives is a valid derivative, we can use (1p)eRpeL 1as the smoothed stochastic derivative of the Bernoulli random variable. Hence, as previously seen in Arya et al. 18, the smoothed stochastic derivative of a Bernoulli can be to 1. This results in y wi1 wi when inserted into Eq. (9). Yet again, we obtain the same right-hand side as above",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_579",
    "chunk_position": 579
  },
  {
    "id": "2404.14964v3",
    "text": ". This results in y wi1 wi when inserted into Eq. (9). Yet again, we obtain the same right-hand side as above. It is worth noting that in the comparison with SGdescent, we consider not only the smoothed stochastic derivative of the Bernoulli random variable but also its composition with the derivative of the firing probability, or , the neurons escape noise function. This composition is necessary because we require a surrogate for the ill-defined derivative of the spike train with respect to the membrane potential S uthat is also valid in the deterministic case",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_580",
    "chunk_position": 580
  },
  {
    "id": "2404.14964v3",
    "text": ". In summary, when analyzing different ways of computing or approximating derivatives, the resulting expressions are identical in single units, if the SDmatches the derivative of the escape noise function of the stochastic neuron model. In the single-neuron case, this match is to probabilistic smoothing of the loss, a well-known fact in the literature 20, which allows computing exact gradients of the expected value",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_581",
    "chunk_position": 581
  },
  {
    "id": "2404.14964v3",
    "text": ". Nevertheless, the resulting weight updates are generally different in the deterministic and stochastic cases because these expressions also depend on the neuronal output, which is different in the deterministic and stochastic neurons. We will see in the next section, which are broken in multi-layer networks due to these and other notable differences. 5 C D B AFigure 2. Derivative computation in MLP s.Schematic of an example network for which (surrogate) derivatives are computed according to different methods. The colored arrows indicate where partial derivatives are calculated",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_582",
    "chunk_position": 582
  },
  {
    "id": "2404.14964v3",
    "text": ". The colored arrows indicate where partial derivatives are calculated. (A): SGdescent relies on the chain rule for efficient gradient computation in a deterministic MLP . Thus, the derivative of the output with respect to a given weight is factorized into its primitives, which are indicated by the colored arrows. (B)SPM s approach the problem of non-differentiable spike trains by adding noise and then smoothing the output based on its expected value. Since this method does not allow the use of the chain rule, the derivative for each weight must be computed directly",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_583",
    "chunk_position": 583
  },
  {
    "id": "2404.14964v3",
    "text": ". Since this method does not allow the use of the chain rule, the derivative for each weight must be computed directly. (C)The derivative and the expected value are not interchangeable, which makes this option mathematically invalid. Furthermore, it is not possible to achieve the necessary smoothing using the expected value after such an interchange. (D)Smoothed stochastic derivatives in stoch AD use the expected value of each node to compute the derivative. However, the method relies on expectation values conditioned on the activity of a specific forward pass",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_584",
    "chunk_position": 584
  },
  {
    "id": "2404.14964v3",
    "text": ". However, the method relies on expectation values conditioned on the activity of a specific forward pass. 3.2 Analysis of the multi-layer Perceptron To analyze the relation of the different methods in the multi-layer setting, we begin by examining SPM s, which lack support for ADand thus an efficient compute gradients. We then further discuss how stoch AD provides smooth stochastic derivatives to SGs in multi-layer networks",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_585",
    "chunk_position": 585
  },
  {
    "id": "2404.14964v3",
    "text": ". We then further discuss how stoch AD provides smooth stochastic derivatives to SGs in multi-layer networks. 3.2.1 Output smoothing in multi-layer SPMs precludes efficient gradient computation SPM s lack support for AD, because they smooth the expected loss landscape through stochasticity and therefore require the calculation of expected values at the network output. While output smoothing allows the implementation of the finite difference algorithm, this not scale to large models and is therefore of little practical use for training ANN s 30, 31",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_586",
    "chunk_position": 586
  },
  {
    "id": "2404.14964v3",
    "text": ". The application of AD, however, requires differentiable models, like standard ANN s, so that the chain rule can be used to decompose the gradient computation into simple primitives. This composability is the basis for efficient recursive like BP and real-time recurrent learning (RTRL) 29. To see why SPM s do not support AD, let us consider a simple example network: Let ybe the output of a binary neural network with input xand two hidden layers with activities h1,h2(. 2). The output yhas the firing probability pyas in Eq",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_587",
    "chunk_position": 587
  },
  {
    "id": "2404.14964v3",
    "text": ". 2). The output yhas the firing probability pyas in Eq. (7), and the hidden layers have the firing probabilities p1and p2respectively. We are looking for a closed-form expression of the derivative of the expected output for each parameter, e.g., w1Eyfor weight w1(. 2B). In a deterministic and differentiable network, one can use the chain rule to split the expression into a product of partial derivatives as w1yy pypy h2h2 p2. . .p1 w1(see . 2A)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_588",
    "chunk_position": 588
  },
  {
    "id": "2404.14964v3",
    "text": ". . .p1 w1(see . 2A). However, this is not possible for SPM s because w1Ey Eh w1yi , where the right-hand side would involve the derivative of the non-differentiable binary output. Even if we could exchange the expectation value and the derivative, we would still be faced with the fact that the expectation of a product is usually not to the product of expectation values, unless the factors are independent (. 2C), hence E pyy h2py p2h2. . . w1p1 E pyy E h2py E p2h2 . . .E w1p1 py Ey h2Epy p2Eh2. . . w1Ep1",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_589",
    "chunk_position": 589
  },
  {
    "id": "2404.14964v3",
    "text": ". 2C), hence E pyy h2py p2h2. . . w1p1 E pyy E h2py E p2h2 . . .E w1p1 py Ey h2Epy p2Eh2. . . w1Ep1. Clearly, in neural networks, the factors are not independent, because the activity of downstream neurons depends on the activity of their upstream partners. Thus, it is not obvious how to compute gradients in multi-layer SPM networks",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_590",
    "chunk_position": 590
  },
  {
    "id": "2404.14964v3",
    "text": ". Thus, it is not obvious how to compute gradients in multi-layer SPM networks. We will see that stoch AD suggests sensible solutions to the aforementioned problems which ultimately justify why we 6 Finite dierences approach How to compute gradients? Stochastic SG descent Each stochastic trial creates a path Smooth onto a selected path Deterministic SG descent select path with deterministic firing pattern",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_591",
    "chunk_position": 591
  },
  {
    "id": "2404.14964v3",
    "text": ". SGs correspond to smoothed stochastic derivatives in stochastic SNN s.The tree illustrates the discrete decisions associated with the binary spike generation process at different points over different layers of a stochastic MLP . A single forward pass in the network corresponds to a specific path through the tree which yields a specific set of spike trains. Another forward pass will result in a different path and spike patterns",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_592",
    "chunk_position": 592
  },
  {
    "id": "2404.14964v3",
    "text": ". Another forward pass will result in a different path and spike patterns. Computing gradients using finite differences requires randomly sampling paths from the network and evaluating their averaged loss before and after a given weight perturbation. Although this approach is unbiased for small perturbations, the random path selection results in high variance. Furthermore, that approach is not scalable to large networks. Stochastic SGdescent is to smoothed stochastic derivatives in the stoch AD framework",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_593",
    "chunk_position": 593
  },
  {
    "id": "2404.14964v3",
    "text": ". Furthermore, that approach is not scalable to large networks. Stochastic SGdescent is to smoothed stochastic derivatives in the stoch AD framework. To compute the gradient, we roll out the network once and sample a random path in the tree which we now keep fixed (yellow). At each node, we then compute the expected output given the fixed activation of the previous layer Ehihi1, which yields a low-variance estimate (see inset: spike raster, selected trial shown in yellow, spike trains of other trials in gray, expectation shown as shaded overlay)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_594",
    "chunk_position": 594
  },
  {
    "id": "2404.14964v3",
    "text": ". By choosing a surrogate function that matches the escape noise process, both methods give the same derivative for a spike with respect to the membrane potential. Deterministic SGdescent can be seen as a special case in which the random sampling of the path is replaced by a point estimate given by the deterministic roll-out (blue). canin fact do some of the above operations. Consequently, in the following, we will only consider SGs and stoch AD , which support BP",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_595",
    "chunk_position": 595
  },
  {
    "id": "2404.14964v3",
    "text": ". canin fact do some of the above operations. Consequently, in the following, we will only consider SGs and stoch AD , which support BP. 3.2.2 Stochastic automatic differentiation bestows theoretical support to surrogate gradients Here we show how smoothed stochastic derivatives for stochastic binary MLP s relate to SGs. Arya et al. 18 defined the smoothed stochastic derivative (repeated in Eq. (3)for convenience) for one realization of a random variable , where the discrete random variables are usually Bernoulli random variables in stochastic binary MLPs",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_596",
    "chunk_position": 596
  },
  {
    "id": "2404.14964v3",
    "text": ". To gain an intuitive understanding of the essence of smoothed stochastic derivatives, we consider a single realization of a stochastic program, which, in our case, is a stochastic MLP . In other words, we run a single stochastic forward pass in the MLP and condition on its activity. At each point in time, each neuron either emits a one or a zero, i.e., a spike or none. Thus, we can think of all these binary decisions as edges in a tree, with each node representing the spike pattern of all units in a given layer",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_597",
    "chunk_position": 597
  },
  {
    "id": "2404.14964v3",
    "text": ". Thus, we can think of all these binary decisions as edges in a tree, with each node representing the spike pattern of all units in a given layer. Any roll-out of a forward pass then corresponds to a particular randomly chosen path through this tree (. 3). These paths originate from the same root for a given input x, yet the stochasticity in all of them is independent. In this tree of possible spike patterns, it is easy to understand how different methods for gradient estimation work. Let us first consider the finite differences method",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_598",
    "chunk_position": 598
  },
  {
    "id": "2404.14964v3",
    "text": ". Let us first consider the finite differences method. In this approach, the loss is evaluated once using the parameters wand once using w w, either considering a single trial or averaging over several independent trials. Hence, one randomly samples paths in the tree of spike patterns and compares their averaged output before and after weight perturbation. Since the randomness of the sampled paths is uncoupled, the finite difference approach results in high variance (cf. . 3, gray), which scales with the inverse of the squared perturbation w",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_599",
    "chunk_position": 599
  },
  {
    "id": "2404.14964v3",
    "text": ". . 3, gray), which scales with the inverse of the squared perturbation w. However, smaller perturbations allow a more accurate gradient estimation 32, 33. Reducing this variance requires instead the introduction of a coupling between the sampled paths, which is at the heart of stoch AD 18. The key idea is to condition on the previous layers output hi1and then consider all possible spike patterns for the current layer Ehihi1. Thus, we can consider the 7 expected layer activity, given a randomly chosen path, e.g., the yellow path in . 3",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_600",
    "chunk_position": 600
  },
  {
    "id": "2404.14964v3",
    "text": ". Thus, we can consider the 7 expected layer activity, given a randomly chosen path, e.g., the yellow path in . 3. For this path, the derivatives now need to be smoothed at each node. To do so, we compute the expectation in each layer conditioned on the activity of the previous layer along the selected path. After smoothing, it is possible to compute the path-wise derivative along a selected path with activity h 1, h 2, y: w1Ey 1 npaths X paths py Eyh 2 h2Epyh 2 p2Eh2h 1 w1Ep1x. Here we averaged over all possible paths, i.e., all possible combinations of the activities h 1, h 2",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_601",
    "chunk_position": 601
  },
  {
    "id": "2404.14964v3",
    "text": ". Here we averaged over all possible paths, i.e., all possible combinations of the activities h 1, h 2. In practice, it is rarely possible to average over all possible combinations and one instead uses a Monte Carlo estimate, which still yields significantly lower variance than other schemes and can be computed efficiently using BPfor a single path per update. Given the above method for computing smoothed stochastic derivatives, we are now in the position to understand their relationship to SGs",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_602",
    "chunk_position": 602
  },
  {
    "id": "2404.14964v3",
    "text": ". Given the above method for computing smoothed stochastic derivatives, we are now in the position to understand their relationship to SGs. Since we condition on a specific path in the tree of all possible spike patterns, we only compute derivatives of the expected output hiconditional on the output of the previous layer hi1according to the chosen path at each node. Such an approach exactly corresponds to treating each node as a single unit like in the previous .1",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_603",
    "chunk_position": 603
  },
  {
    "id": "2404.14964v3",
    "text": ". Such an approach exactly corresponds to treating each node as a single unit like in the previous .1. As we saw above, the derivative of the expected output of a single unit with escape noise is to the corresponding smoothed stochastic derivative and the SDin that unit both with or without escape noise. Furthermore, when using SGs, there is no difference in how the method is applied in single versus multi-layer cases and there is always a well-defined path to condition on. So, SGs can also be understood as treating all units at each layer as single units",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_604",
    "chunk_position": 604
  },
  {
    "id": "2404.14964v3",
    "text": ". So, SGs can also be understood as treating all units at each layer as single units. The same is true for smoothed stochastic derivatives in the stoch AD framework. Thus, the stoch AD framework provides theoretical support to SGs in stochastic networks and also stipulates that the SDshould be matched to mirror the derivative of the neuronal escape noise, thereby extending this matching principle from the single neuron case 20, 21 to the multi-layer network setting and removing some of the ambiguity associated with choosing this function in practice 16",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_605",
    "chunk_position": 605
  },
  {
    "id": "2404.14964v3",
    "text": ". 3.2.3 Surrogate gradients in deterministic networks SGs are commonly used to train deterministic SNN s. Deterministic networks show two essential differences to their stochastic counterparts which result in different weight updates from what we have learned above. First, while a stochastic network typically selects a different path in the tree of spike patterns in each trial, there is only one path per input and parameter set in the deterministic case. This means that the expectation value over sampling random paths is replaced by a point estimate provided by the deterministic roll-out",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_606",
    "chunk_position": 606
  },
  {
    "id": "2404.14964v3",
    "text": ". This means that the expectation value over sampling random paths is replaced by a point estimate provided by the deterministic roll-out. This difference introduces a selection bias in the estimation of the stochastic implementation. However, in practice, this bias is often small (Supplementary . S1). Second, when training deterministic networks with SGs the slope of the SDat threshold ( SG) is typically chosen ad-hoc. Typical values are on the order one compared to the neuronal rheobase",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_607",
    "chunk_position": 607
  },
  {
    "id": "2404.14964v3",
    "text": ". Typical values are on the order one compared to the neuronal rheobase. This is stark in contrast to the correct slope of the corresponding asymptotic escape noise function from taking the limit N . Approaching this limit empirically with SGleads to unstable training and poor performance 16. The choice SGNinevitably results in different weight updates. While this is a desired effect that allows training in the first place by avoiding the problem of zero and infinite weight updates, it is less clear whether this approximation has any undesirable off-target consequences",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_608",
    "chunk_position": 608
  },
  {
    "id": "2404.14964v3",
    "text": ". To address this question we will examine the properties of SGs in deterministic networks in the next section. 4 Analysis of surrogate gradient properties in deterministic networks By design SGs deviate from the actual gradient, as they provide a non-zero surrogate in cases where the actual gradient is zero. It is not clear a-priori what consequences such a deviation has and whether SGs are gradients at all, which can be obtained by differentiating a surrogate loss",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_609",
    "chunk_position": 609
  },
  {
    "id": "2404.14964v3",
    "text": ". However, it is difficult to get a quantitative understanding when comparing either to the zero vector or to the ill-defined derivative at the point of the spike. To take a closer look at how SGs deviate from actual gradients and the resulting consequences we now move to differentiable network models that have a well-defined gradient. While SGtraining is not required in such networks, it allows us to develop a quantitative understanding of the commonalities and differences we should expect. This comparison also allows us to check whether SGs satisfy the formal criteria of gradients",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_610",
    "chunk_position": 610
  },
  {
    "id": "2404.14964v3",
    "text": ". This comparison also allows us to check whether SGs satisfy the formal criteria of gradients. 4.1 Deviations of SGs from actual gradients in differentiable MLPs and the consequences We first sought to understand whether SGs point in a similar direction as the actual gradient. Specifically, we asked whether SGs ensure sign concordance, i.e., whether they preserve the sign of the gradient components. To investigate 8 out A f BC actual . SGs deviate from actual gradients in differentiable MLP s",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_611",
    "chunk_position": 611
  },
  {
    "id": "2404.14964v3",
    "text": ". To investigate 8 out A f BC actual . SGs deviate from actual gradients in differentiable MLP s. (A) Schematic of a differentiable network (left) with sigmoid activations (right) for which we compute an SDusing the derivative of a flatter sigmoid (yellow) in contrast to the actual activation (black). (B)Top row: Network output (solid gray), smoothed network output (dashed), and integrated S as a function of w. The triangles on the x-axis indicate the minimum of the corresponding curves. Bottom row: Derivatives of the top row",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_612",
    "chunk_position": 612
  },
  {
    "id": "2404.14964v3",
    "text": ". The triangles on the x-axis indicate the minimum of the corresponding curves. Bottom row: Derivatives of the top row. Left and right correspond to a flatter ( SG 15 ) and a steeper (SG 25 )SD, see for network parameters. Note that the actual derivative and the surrogate can have opposite signs. (C)Heatmap of the optimization landscape along v1andv2for different SGvalues (top to bottom). While the actual gradient can be asymptotically zero (see yellow dot, bottom), the SDprovides a descent direction (yellow arrow), thereby enabling learning (top and middle).",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_613",
    "chunk_position": 613
  },
  {
    "id": "2404.14964v3",
    "text": ". . Parameter values for sign flip example. Parameter values for the network in . 4A, with input x 1, which serve as an example, that SGs can have the opposite sign of the actual gradient and thus point towards the opposite direction. Therefore, we cannot guarantee the SG to align with the actual gradient. Parameter w v 1 v2u1u2fSG Input x Value 0 0.05 0.1 1 -1 100 25 1 this question, we consider a small network with input xand output defined by g h1 h2 y. (10) The network parameters are denoted by w,v1,v2,u1, andu2, while g,h1, andh2are the hidden layer activations",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_614",
    "chunk_position": 614
  },
  {
    "id": "2404.14964v3",
    "text": ". (10) The network parameters are denoted by w,v1,v2,u1, andu2, while g,h1, andh2are the hidden layer activations. This network has a well-defined gradient and provides a minimal working example. To study the effect of computing a SD, we replace the sigmoid parameterized with fused in the forward pass by a surrogate function with SGwhich is used to compute the SD in the backward pass. Hence the SD for a steeper sigmoid is given by e w(fwx)xSG(SGwx)(1(SGwx)) (11) withf SG. As before, the deterministic binary Perceptron corresponds to the limiting case f",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_615",
    "chunk_position": 615
  },
  {
    "id": "2404.14964v3",
    "text": ". As before, the deterministic binary Perceptron corresponds to the limiting case f . To investigate the differences between the SGand the actual gradient, we are particularly interested in the derivative of the output with respect to the hidden layer weight . The partial derivative of the output with respect to w 9 A B CFigure 5. SGs are not gradients. (A) Heat map of the network output while moving along two random directions in the parameter space of the example network with step activation (top) and sigmoid activation (bottom) (see . 4 A)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_616",
    "chunk_position": 616
  },
  {
    "id": "2404.14964v3",
    "text": ". 4 A). The circles indicate a closed integration path through parameter space, starting at the arrowhead. (B)Integral values of SDs as a function of the angle along the closed circular path shown in (A). Different shades of yellow correspond to different values of SG. The blue lines correspond to the actual output of the network with step activation function (solid) or sigmoid activation (dashed). The integrated actual derivative of the network with sigmoid activation matches the output (dashed line) and is thus not visible in the plot",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_617",
    "chunk_position": 617
  },
  {
    "id": "2404.14964v3",
    "text": ". The integrated actual derivative of the network with sigmoid activation matches the output (dashed line) and is thus not visible in the plot. (C)Absolute difference between actual loss value and integrated SDas function of the number of integration steps. The numerical integrals converge to finite values. Thus the observed difference is not an artifact of the numerical integration. is given as wyy h1h1 gg wz bluepathy h2h2 gg wz maroon path. Now inserting the SDs using Eq",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_618",
    "chunk_position": 618
  },
  {
    "id": "2404.14964v3",
    "text": ". is given as wyy h1h1 gg wz bluepathy h2h2 gg wz maroon path. Now inserting the SDs using Eq. (11) leads to e wy ey h1eh1 gey h2eh2 g!eg w u1v1 S u2v2 S z responsible for sign flip3 SG S S z positive factorx , (12) where only the first factor in Eq. (12), which consists of two terms, determines the relative sign of the SDwith respect to the actual derivative. This is because the second factor, as well as SG, are always positive. Furthermore, the derivative of the sigmoid is always positive independently of the choice of forSG",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_619",
    "chunk_position": 619
  },
  {
    "id": "2404.14964v3",
    "text": ". Furthermore, the derivative of the sigmoid is always positive independently of the choice of forSG. Finally, x, the input data, does not change its sign dependent on SG. However, the first factor can change its sign, since it is a summation of two nonlinear functions with changed hyperparameters forSGand different weights, which may be negative. For instance, when we use specific parameter values (given in ) in Eq. (12), the SDhas the opposite sign of the actual derivative (. 4B)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_620",
    "chunk_position": 620
  },
  {
    "id": "2404.14964v3",
    "text": ". For instance, when we use specific parameter values (given in ) in Eq. (12), the SDhas the opposite sign of the actual derivative (. 4B). Thus, already in this simple example, there is no guarantee that the sign of the SGis preserved with respect to the actual gradient. As a consequence following the SGwill not necessarily find parameter combinations that correspond to a minimum of the loss. 4.2 Surrogate gradients are not gradients of a surrogate loss Given the above insight, we wondered whether a SGcan be understood as the gradient of a surrogate loss that is not explicitly defined",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_621",
    "chunk_position": 621
  },
  {
    "id": "2404.14964v3",
    "text": ". To answer this question, we note that if, and only if, the SGis the gradient of a scalar function, i.e., corresponds to a conservative field, then integrating over any closed path must yield a zero integral. To check this, we considered the approximate Jacobian obtained using the SDs in the above example and numerically computed the integral over a closed circular path parameterized by the angle in a two-dimensional plane in parameter space for different values of SG ISGZ360 0ed dd dd , where the network parameters at any position are given by (. 5A, B; Methods .3)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_622",
    "chunk_position": 622
  },
  {
    "id": "2404.14964v3",
    "text": ". 5A, B; Methods .3). We found that integrating the SDdid not yield a zero integral, whereas using the actual derivatives resulted in a zero integral as expected. Importantly, this difference was not explained by numerical integration errors due to the finite step size (. 5C). Thus SGs cannot be understood as gradients of a surrogate loss. 10 5 From Perceptrons to LIF neurons In our above treatment, we focused on binary Perceptrons for ease of analysis. In the following, we show that our findings readily generalize to networks of LIFneurons",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_623",
    "chunk_position": 623
  },
  {
    "id": "2404.14964v3",
    "text": ". In the following, we show that our findings readily generalize to networks of LIFneurons. To that end, we consider LIFneurons in discrete time which share many commonalities with the binary Perceptron (see also . 1A)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_624",
    "chunk_position": 624
  },
  {
    "id": "2404.14964v3",
    "text": ". To that end, we consider LIFneurons in discrete time which share many commonalities with the binary Perceptron (see also . 1A). To illustrate these similarities let us consider a single LIF neuron with index idescribed by the following discrete-time dynamics: Iin 1 s Iin X jwij Sjn (13) Uin 1 ( m Uin (1m)Iin) (1Sin) (14) Sin , (15) where wijdescribes the synaptic weights between the neuron iand the input neuron j,s exp t s and m exp t m , where tis the time step, sis the synaptic time constant, and mis the membrane time constant",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_625",
    "chunk_position": 625
  },
  {
    "id": "2404.14964v3",
    "text": ". While the first two characterize the linear temporal dynamics of the synaptic current Iand membrane potential U, the last the non-linearity of the neuron, its spiking output S. Thus in this formulation, we can think of a LIFneuron as a binary Perceptron whose inputs are first processed through a linear filter cascade, i.e., the neuronal dynamics, and additionally have a reset mechanism given by the last factor in Eq. 14. In the stochastic case, this filter does not change. In fact, we keep the same for synaptic current (Eq. (13)) and membrane potential (Eq. (14))",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_626",
    "chunk_position": 626
  },
  {
    "id": "2404.14964v3",
    "text": ". 14. In the stochastic case, this filter does not change. In fact, we keep the same for synaptic current (Eq. (13)) and membrane potential (Eq. (14)). However, instead of a deterministic Heaviside function as in Eq. (15), we use a stochastic spike generation mechanism with escape noise, which is independent for each neuron igiven its membrane potential pin (16) Sin Be. (17) Again, this mechanism is in direct with the stochastic Perceptron case (cf. Eq. (7)) and permissive to computing smoothed stochastic derivatives",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_627",
    "chunk_position": 627
  },
  {
    "id": "2404.14964v3",
    "text": ". Eq. (7)) and permissive to computing smoothed stochastic derivatives. The derivative of the current and the membrane potential with respect to the weights induce their own dynamics: d dwij Iin 1 sd dwij Iin Sjn, d dwij Uin 1 d dwij Uin (1m)(1Sin)d dwij Iin (m Uin (1m)Iin)d dwij Sin, (18) where we used the product rule to include the derivative of the reset term. To compute the smoothed stochastic derivative of the spike train, we use the affine combination of the left and right smoothed stochastic derivatives of a Bernoulli random variable according to Arya et al",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_628",
    "chunk_position": 628
  },
  {
    "id": "2404.14964v3",
    "text": ". 18 to get d dwij Sind dpin Be z 1d d Uind dwij Uin d d Uin z SDd dwij Uin. (19) Again, we find that this exactly recovers SGs as introduced in Zenke et al. 13 for SGNwhen conditioning on the deterministic spiking path and, hence, confirms the between SGs and smoothed stochastic derivatives. Thus, our analysis establishes a link between the escape noise function of stochastic LIFneurons and the shape of the SD in multi-layer SNN s. A similar connection between escape noise and exact derivatives in expectation was previously described in shallow SNNs 20, 21. 11 A B DC 6",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_629",
    "chunk_position": 629
  },
  {
    "id": "2404.14964v3",
    "text": ". A similar connection between escape noise and exact derivatives in expectation was previously described in shallow SNNs 20, 21. 11 A B DC 6. SGs successfully train stochastic SNN s on a spike train matching task. (A) Spike raster plots of the input and target spike trains of the spike train matching task. Time is shown on the x-axis, the neuron indices are on the y-axis and each dot is a spike. The task is to convert the given frozen Poisson input spike pattern (left) into a structured output spike pattern depicting the Radcliffe Camera in Oxford (right)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_630",
    "chunk_position": 630
  },
  {
    "id": "2404.14964v3",
    "text": ". (B)Top: Output spike raster plots after training of the deterministic (left) and the stochastic SNN s (right). Although both methods faithfully reproduce the overall target structure, the deterministic network is slightly better at matching the exact timing. Bottom: Hidden layer spike raster plots. Despite the similar output, the two networks show visibly different hidden layer activity patterns. (C)L2 loss (top) and van Rossum distance (bottom) throughout training for the two network models",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_631",
    "chunk_position": 631
  },
  {
    "id": "2404.14964v3",
    "text": ". (C)L2 loss (top) and van Rossum distance (bottom) throughout training for the two network models. While the deterministic network outperforms the stochastic one in terms of L2distance, the difference is negligible for the van Rossum distance. (D)Average Fano factor throughout training in the hidden and output layer of the stochastic network. Although the stochastic network reduces its variability during training to match the deterministic target, its hidden layer still displays substantial variability at the end of training. Going further, we can use Eq. (19) to write Eq",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_632",
    "chunk_position": 632
  },
  {
    "id": "2404.14964v3",
    "text": ". Going further, we can use Eq. (19) to write Eq. (18) such that it only depends on current and membrane potential derivatives d dwij Uin 1 (m Uin (1m)Iin) z derivative through reset d Uin dwi (1m) (1Sin)d Iin dwij and can be computed forward in time. Most SNN simulators avoid BPthrough the reset term 1, 13 as this empirically improves performance for poorly scaled SGs 16. Therefore, it is common practice to set the right-hand term in the parenthesis in Eq. (20), i.e., the derivative through the reset term, to zero",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_633",
    "chunk_position": 633
  },
  {
    "id": "2404.14964v3",
    "text": ". Therefore, it is common practice to set the right-hand term in the parenthesis in Eq. (20), i.e., the derivative through the reset term, to zero. Conversely, strict adherence to stoch AD would suggest keeping the reset term when back-propagating. However, similar to Zenke et al. 16, we find no difference in performance whether we back-propagate through the reset or not when the SGis scaled with1 S, while without this scaling, BPthrough the reset negatively affects performance",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_634",
    "chunk_position": 634
  },
  {
    "id": "2404.14964v3",
    "text": ". Overall, we have shown that our results obtained from the analysis of Perceptrons are transferable to SNN s and hence confirm again the between SGs and smoothed stochastic derivatives in the stoch AD framework. 6 Surrogate gradients are ideally suited for training stochastic SNNs Above we have seen that SGdescent is theoretically justified by stoch AD albeit better supported for stochastic spiking neuron models. This finding also suggests that SGdescent is suitable for training stochastic SNN s, with deterministic 12 0 100 200 Epoch0.81.0Accuracy stoch. (train) stoch. (valid)det. (train) det",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_635",
    "chunk_position": 635
  },
  {
    "id": "2404.14964v3",
    "text": ". (train) stoch. (valid)det. (train) det. (valid) C D Neuron 611 Neuron 379 Neuron 323 030trial idx. 0 0.7 time s 0 0.7 time s 0 0.7 time s EB Input Hid. 0 Hid. 1 Hid. 2 Readout time s time s A 0 100 200 Epoch0.50.8Fano factor Hid. 0 Hid. 1 Hid. 2 train valid test other012Loss det. stoch. 7. SGs can successfully train stochastic convolutional spiking neural networks ( Conv SNN s). (A) Snapshot of network activity for two correctly classified sample inputs from the Spiking Heidelberg Digits ( SHD ) dataset",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_636",
    "chunk_position": 636
  },
  {
    "id": "2404.14964v3",
    "text": ". (A) Snapshot of network activity for two correctly classified sample inputs from the Spiking Heidelberg Digits ( SHD ) dataset. Top: readout unit activity over time, the colored line indicates the activity of the unit that corresponds to the correct class. Below: Spike raster plots of the three convolutional hidden layers. The spike raster plots of the inputs are shown at the bottom in red and gray. Time is on the x-axis, the neuron index is on the y-axis and each dot represents a spike",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_637",
    "chunk_position": 637
  },
  {
    "id": "2404.14964v3",
    "text": ". Time is on the x-axis, the neuron index is on the y-axis and each dot represents a spike. (B)Learning curves for training (dashed) and validation (solid) accuracy for the stochastic and the deterministic case (average over n 6trialsstd). (C)The mean Fano factor of the different layers in the stochastic network throughout training ( std,n 3).(D)The first three pairs of boxes show train, validation, and test loss of the Conv SNN as in (A) for the stochastic and the deterministic case for n 6random initializations. The rightmost boxes show test loss for the opposite activation function",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_638",
    "chunk_position": 638
  },
  {
    "id": "2404.14964v3",
    "text": ". The rightmost boxes show test loss for the opposite activation function. This means the network trained deterministically is tested with stochastic activation and vice versa. (E)Raster plots over trials of the spiking activity of three randomly chosen units from the second hidden layer. The units show clear trial-to-trial variability reminiscent of cortical activity. SNN s being a special case (cf. . 3). Next, we wanted to confirm this insight numerically",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_639",
    "chunk_position": 639
  },
  {
    "id": "2404.14964v3",
    "text": ". SNN s being a special case (cf. . 3). Next, we wanted to confirm this insight numerically. To that end, we trained deterministic and stochastic SNNs with SG descent on a deterministic spike train matching and a classification task. For the spike train matching task, we assumed 200 input neurons with frozen Poisson spike trains. We then set up a feed-forward SNN with one hidden layer, initialized in the fluctuation-driven regime 34",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_640",
    "chunk_position": 640
  },
  {
    "id": "2404.14964v3",
    "text": ". We then set up a feed-forward SNN with one hidden layer, initialized in the fluctuation-driven regime 34. We used supervised SG training in a deterministic and a stochastic of the same network to match 200 target spike trains which were given by a dithered of the Radcliffe Camera (. 6A; Methods .4.1). Both networks learned the task, albeit with visibly different hidden layer activity (. 6B). The deterministic outperformed the stochastic SNN in terms of L2-distance at 1 ms, i.e. the temporal resolution of the simulation",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_641",
    "chunk_position": 641
  },
  {
    "id": "2404.14964v3",
    "text": ". 6B). The deterministic outperformed the stochastic SNN in terms of L2-distance at 1 ms, i.e. the temporal resolution of the simulation. However, when comparing their outputs according to the van Rossum distance 35 with an alpha-shaped filter kernel to the -kernel of the LIF neuron ( mem 10 ms , syn 5 ms ), we found no difference in loss between the stochastic and deterministic networks (. 6C). Finally, we observed that the Fano factor, a measure of stochasticity, of the stochastic SNN dropped during training to better accommodate the deterministic target (. 6D)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_642",
    "chunk_position": 642
  },
  {
    "id": "2404.14964v3",
    "text": ". 6D). In summary, we find that the stochastic network exhibits comparable performance to the deterministic network. Thus SGs are suitable for training stochastic SNNs. To verify that this finding generalizes to a more complex task, we trained a Conv SNN with three hidden layers on the SHD dataset 36 with maximum-over-time readout (Methods .4.2 and . 7A). Like above, the stochastic network learned to solve the task with comparable training and validation accuracy to the deterministic network (. 7B)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_643",
    "chunk_position": 643
  },
  {
    "id": "2404.14964v3",
    "text": ". Like above, the stochastic network learned to solve the task with comparable training and validation accuracy to the deterministic network (. 7B). Specifically, we found that the stochastic Conv SNN achieved a validation accuracy of 97.21.0 , (test 85.11.3 ), compared to 94.70.8 (test84.31.0) for the deterministic Conv SNN . Furthermore, in the case of the stochastic SNN , we left the escape noise active during validation and testing. However, one can see that the stochastic network performs well when tested in a deterministic environment (. 7D, other)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_644",
    "chunk_position": 644
  },
  {
    "id": "2404.14964v3",
    "text": ". However, one can see that the stochastic network performs well when tested in a deterministic environment (. 7D, other). Conversely, the deterministically trained network does not perform well when evaluated under stochastic spike generation. In contrast to the previous task, we found that the Fano factor remained high during training (. 7C), i.e. stochasticity is preserved. This is also reflected in a high trial-to-trial variability as shown in . 7E. One can see that the spiking activity for three example neurons shows a high variability across trials for the same input",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_645",
    "chunk_position": 645
  },
  {
    "id": "2404.14964v3",
    "text": ". 7E. One can see that the spiking activity for three example neurons shows a high variability across trials for the same input. Thus, even for a more complex task, SG descent is well suited for training stochastic SNNs, and stochasticity is preserved after training. 13 7 Discussion We have analyzed the theoretical foundations of the widely used SGdescent method for multi-layer SNN s and shown that SGs can be derived for stochastic networks from the stoch AD framework 18",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_646",
    "chunk_position": 646
  },
  {
    "id": "2404.14964v3",
    "text": ". The stoch AD framework provides a principled way to use ADin discrete stochastic systems, directly applicable to SGdescent in stochastic SNN s. Our analysis is based on smoothed stochastic derivatives introduced within the stoch AD framework and stipulates that the SDshould match the derivative of the escape noise function of the stochastic neuron model. We confirmed in simulations that SGdescent allows training well-performing stochastic SNN s with substantial variability comparable to neurobiology",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_647",
    "chunk_position": 647
  },
  {
    "id": "2404.14964v3",
    "text": ". We confirmed in simulations that SGdescent allows training well-performing stochastic SNN s with substantial variability comparable to neurobiology. Our work shows that SGs enjoy better theoretical support in stochastic SNN s, which removes some ambiguity of choice of the SD and also sheds light on deterministic SGs as a special case. One of our key findings is the relation between the functional shape of the escape noise in stochastic neuron models and the SD",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_648",
    "chunk_position": 648
  },
  {
    "id": "2404.14964v3",
    "text": ". One of our key findings is the relation between the functional shape of the escape noise in stochastic neuron models and the SD. This relation is evident in single neurons, where the solution found by SGs and stoch AD is to the exact derivative of the expected output. This relation has also been described in previous work on shallow networks 20, 21, where the derivative of the escape noise directly shows up when computing exact derivatives in expectation. In this article, we found that the relation extends to multi-layer SNN s when applying stoch AD as a theoretical framework",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_649",
    "chunk_position": 649
  },
  {
    "id": "2404.14964v3",
    "text": ". In this article, we found that the relation extends to multi-layer SNN s when applying stoch AD as a theoretical framework. The link between the SDshape and escape noise provides a rigorous mathematical justification for a particular SDchoice, which in previous work was typically chosen heuristically. The issue of non-existing derivatives is not unique to SNN s but is a well-known challenge when dealing with discrete random variables and their derivatives",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_650",
    "chunk_position": 650
  },
  {
    "id": "2404.14964v3",
    "text": ". It hence arises in various contexts, such as in general binary neural networks 11, sigmoid belief nets 37, or when dealing with categorical distributions 38, 39. To address this challenge in arbitrary functions, including discrete random variables, the score function estimator, also called REINFORCE 40, is often applied, as it provides unbiased estimates. This estimator can be computed through stochastic computation graphs 41. While Arya et al. 18 use coupled randomness to reduce variance, there are also methods building on reinforcement learning for low variance gradient estimation 42",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_651",
    "chunk_position": 651
  },
  {
    "id": "2404.14964v3",
    "text": ". 18 use coupled randomness to reduce variance, there are also methods building on reinforcement learning for low variance gradient estimation 42. Conversely, to address the challenge of BPthrough non-differentiable functions in neural networks instead, a commonly used solution is the STE 26, 27, which replaces the derivative of a stochastic threshold function with one, also called the identity STE 28. This solution relates closely to SGs used in SNN s",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_652",
    "chunk_position": 652
  },
  {
    "id": "2404.14964v3",
    "text": ". This solution relates closely to SGs used in SNN s. However, unlike the STE,SGs are commonly used in deterministic networks and comprise a nonlinear SDlike the derivative of a scaled fast sigmoid 15. While both are successful in practice, we still lack a complete theoretical understanding. Several studies have analyzed the theoretical aspects of the STE, looking at the problem from various angles. However, they have each analyzed different versions of the STE, and a complete of the underlying theory is still missing",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_653",
    "chunk_position": 653
  },
  {
    "id": "2404.14964v3",
    "text": ". However, they have each analyzed different versions of the STE, and a complete of the underlying theory is still missing. Although none of them analyzed STEs in SNN s, i.e., SGs, examining their results helps to put SGs into perspective. Bengio et al. 27 originally introduced the identity STE as a biased estimator, saying that it guarantees the correct sign only in shallow networks. Yin et al. 28 studied coarse gradients, including the STE",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_654",
    "chunk_position": 654
  },
  {
    "id": "2404.14964v3",
    "text": ". Yin et al. 28 studied coarse gradients, including the STE. They studied this either in a network with only one nonlinearity or in an activation quantized network with a quantized rectified linear unit ( Re LU ) as opposed to a Heaviside activation function, as we have in the case of SNN s. They analyzed training stability and investigated which choice of STEs leads to useful weight updates. They concluded that the identity STE does not, while the Re LU and clipped Re LU versions do. They further found that the identity STE might be repelled from certain minima. Liu et al",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_655",
    "chunk_position": 655
  },
  {
    "id": "2404.14964v3",
    "text": ". They further found that the identity STE might be repelled from certain minima. Liu et al. 43 showed that the identity STE provides a first-order approximation to the gradient, which was previously shown by Tokui et al. 44 specifically for Bernoulli random variables. However, other STEs were not considered, such as e.g. the sigmoidal STE, which would correspond to our SG. Another approach to dealing with discrete distributions was pursued by Maddison et al. 38 and Jang et al. 39",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_656",
    "chunk_position": 656
  },
  {
    "id": "2404.14964v3",
    "text": ". Another approach to dealing with discrete distributions was pursued by Maddison et al. 38 and Jang et al. 39. They proposed a solution similar to the reparametrization trick but combined with a smooth relaxation of the categorical distribution. This relaxation amounts to training a network with continuous representations by sampling from a Gumbel-Softmax. At the same time, after training, the temperature of the Gumbel-Softmax distribution is set to zero to obtain a discrete output",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_657",
    "chunk_position": 657
  },
  {
    "id": "2404.14964v3",
    "text": ". At the same time, after training, the temperature of the Gumbel-Softmax distribution is set to zero to obtain a discrete output. Hence, while providing discrete output after training, such a network is trained with continuous neuronal outputs rather than spikes. Interestingly, most of these solutions deal with discrete functions in the stochastic case, such as stochastic binary Perceptrons or Bernoulli random variables, where noise is used for smoothing. But as we saw above, probabilistic approaches in the context of SPM s cannot be applied to multi-layer SNN s and BPwithout approximations",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_658",
    "chunk_position": 658
  },
  {
    "id": "2404.14964v3",
    "text": ". But as we saw above, probabilistic approaches in the context of SPM s cannot be applied to multi-layer SNN s and BPwithout approximations. In the past, stochastic SNN models have been implemented mainly for theoretical and biological plausibility reasons 20, 21. For example Brea et al. 21 showed a link between the escape noise and the voltage-triplet rule 45 in shallow networks. Furthermore, there were also approaches that enabled the training of multi-layer stochastic SNN s with AD, such as the Multilayer Spiker proposed by Gardner et al",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_659",
    "chunk_position": 659
  },
  {
    "id": "2404.14964v3",
    "text": ". 22, which uses the spike train itself as the SDof a spike train. Today, SNN s are mostly implemented as networks of deterministic LIFneurons with a Heaviside function as spiking non-linearity, as they tend to have higher performance. Recently, however, Ma et al. 46 found empirically that stochastic SNN s with different types of escape noise can be trained with SGdescent to high performance. While they showed a connection to deterministic SGdescent, they did not discuss the implications for multi-layer networks 14 with stateful LIFneurons",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_660",
    "chunk_position": 660
  },
  {
    "id": "2404.14964v3",
    "text": ". Again, the effectiveness of SGdescent in stochastic SNN s was confirmed, as suggested by the connection to the stoch AD framework found here. Thus, our work not only provides empirical confirmation of the effectiveness of SGs in training stochastic SNN s but also a comprehensive theoretical explanation for their success in multi-layer networks. To gain analytical insight into the theory behind SGs, we had to make some simplifying assumptions. For example, we performed most of our theoretical analysis on Perceptrons which, unlike LIF neurons, have no memory",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_661",
    "chunk_position": 661
  },
  {
    "id": "2404.14964v3",
    "text": ". For example, we performed most of our theoretical analysis on Perceptrons which, unlike LIF neurons, have no memory. Nevertheless, our results readily transfer to LIFneurons in discrete time, as such networks can be understood as a special type of binary recurrent neural networks 15 where the output is given by a Heaviside step function. However, one difference is that, unlike Perceptrons, LIFmodels have a reset after the spike, which should also be considered when calculating gradients",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_662",
    "chunk_position": 662
  },
  {
    "id": "2404.14964v3",
    "text": ". Smoothed stochastic derivatives allow and suggest back-propagating through the reset mechanism, but previous work has shown that it is beneficial to exclude the reset mechanism from the backward pass, as this leads to more robust performance, especially when the SGis not scaled to one 1, 13, 16. Both options are possible when using SG-descent, and we have not noticed much difference in performance when scaling the SDs by1 SGto one. However, omitting this scaling negatively impacts performance when back-propagation through the reset (see Supplementary",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_663",
    "chunk_position": 663
  },
  {
    "id": "2404.14964v3",
    "text": ". However, omitting this scaling negatively impacts performance when back-propagation through the reset (see Supplementary . S2C and compare the asymptotic Super Spike in Zenke et al. 13). While omitting this scaling can potentially be counteracted by an optimizer using per-parameter learning rates, the prefactor due to SGin the unscaled case can become quite large as it accumulates across layers and thus still affects performance. Another limitation is that we derived SGs for stochastic SNN models, while they are most commonly used in deterministic networks",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_664",
    "chunk_position": 664
  },
  {
    "id": "2404.14964v3",
    "text": ". Another limitation is that we derived SGs for stochastic SNN models, while they are most commonly used in deterministic networks. This discrepancy may in fact point at a possible advantage. Our simulations suggested that stochastic networks generalize well or better than their deterministic counterparts. This difference may be due to stochastic path selection, which leads to lower selection bias compared to the deterministic case",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_665",
    "chunk_position": 665
  },
  {
    "id": "2404.14964v3",
    "text": ". This difference may be due to stochastic path selection, which leads to lower selection bias compared to the deterministic case. Moreover, when employing small weight updates, each update follows a different path in the tree of spike patterns, effectively averaging the updates over multiple trials, albeit with slightly different parameter sets. Such stochastic averaging helps to reduce the bias present in the smoothed stochastic derivatives. Alternatively, the difference may be due to escape noise acting as an implicit regularizer reminiscent of dropout that promotes better generalization",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_666",
    "chunk_position": 666
  },
  {
    "id": "2404.14964v3",
    "text": ". Next, while our work motivates a clear link between the SDand a stochastic neuron model with escape noise, it does not say which escape noise to use. This lack may seem like a weakness that merely moves the ambiguity of choice to a different component of the neuron model. Instead, doing so reduces two heuristic choices to one. Furthermore, the escape noise function can be constrained using biological data 47, 48. Moreover, the tasks we used to study stochastic SNN s were limited",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_667",
    "chunk_position": 667
  },
  {
    "id": "2404.14964v3",
    "text": ". Moreover, the tasks we used to study stochastic SNN s were limited. Historically, stochastic SNN s have been evaluated on tasks that require exact deterministic spike trains at readout such as in the spike train matching task 2022. However, despite being performed with stochastic SNN s such tasks actually punish stochasticity, at least in the readout. Therefore, stochastic SNN s respond by becoming more deterministic during training as we have observed when monitoring the Fano factor (cf. . 6)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_668",
    "chunk_position": 668
  },
  {
    "id": "2404.14964v3",
    "text": ". Therefore, stochastic SNN s respond by becoming more deterministic during training as we have observed when monitoring the Fano factor (cf. . 6). In our setting, neurons have the possibility to change the effective steepness of the escape noise function by increasing their weights, as it is dependent on N w, and thus get more deterministic",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_669",
    "chunk_position": 669
  },
  {
    "id": "2404.14964v3",
    "text": ". Therefore, we believe that tasks that do not punish stochasticity, such as classification tasks, are much more suitable for evaluating the performance of stochastic SNN s as they allow training of well-performing SNN s that still show substantial variability in their responses. Finally, our analysis of the deterministic SGs was also subject to some limitations. In particular, a comparison with the actual gradient, which is either zero or undefined, would not have worked. Instead, we analyzed the deviation of the SGfrom the actual gradient in differentiable networks that are non-spiking",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_670",
    "chunk_position": 670
  },
  {
    "id": "2404.14964v3",
    "text": ". Instead, we analyzed the deviation of the SGfrom the actual gradient in differentiable networks that are non-spiking. Doing so, we have shown that SGs in deterministic networks with well-defined gradients deviate from the actual gradients. In the limiting case f , these networks are to Perceptron networks with step activation functions. Thus, although we could not perform our analysis directly on deterministic SNN s, we consider the above approach to be a good approximation since it preserves the main properties of SGs",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_671",
    "chunk_position": 671
  },
  {
    "id": "2404.14964v3",
    "text": ". The work in this article motivates several exciting directions for future research. First, we found that although necessary for training, SGs are generally not gradients of a surrogate loss and thus not guaranteed to find a local minimum. Specifically, when replacing the non-existent derivative of a Heaviside with a SDthe resulting SGis generally not a gradient. It seems unlikely that choosing a different escape noise, or SDwill alter this result",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_672",
    "chunk_position": 672
  },
  {
    "id": "2404.14964v3",
    "text": ". It seems unlikely that choosing a different escape noise, or SDwill alter this result. Thus in situations requiring stronger theoretical guarantees as those bestowed by gradients, an alternative approach would be to start from a surrogate loss function. While this approach would, by design, yield a proper gradient, it would be more akin to optimizing a smoothed of the problem, as is the case when using the Gumbel Softmax or the concrete distribution 38, 39, rather than defining a surrogate for an ill-behaved gradient. However, this approach would also come with the known difficulties for AD",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_673",
    "chunk_position": 673
  },
  {
    "id": "2404.14964v3",
    "text": ". However, this approach would also come with the known difficulties for AD. It is presently not clear whether or how they can be overcome by future work. 15 We also saw that SGs generally decrease the loss but they do not necessarily find a local minimum of the loss. This discrepancy is due to the deviation from the actual gradient. Yet, learning in deterministic SNN s is only possible due to this deviation, because the actual gradient is almost always zero. In fact, deviations from the actual gradient are often also desirable even in conventional ANN training",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_674",
    "chunk_position": 674
  },
  {
    "id": "2404.14964v3",
    "text": ". In fact, deviations from the actual gradient are often also desirable even in conventional ANN training. Bias and variance of the gradient estimator can help to generalize 49. For instance, in ANN s with a rough loss landscape, mollifying, which induces a deviation from the gradient, is successful in practice 50. Regularizers or optimizers, which are often used by default, show another advantage of deviations. For example, many optimizers rely on momentum to speed up training or avoid getting caught in bad local minima",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_675",
    "chunk_position": 675
  },
  {
    "id": "2404.14964v3",
    "text": ". For example, many optimizers rely on momentum to speed up training or avoid getting caught in bad local minima. Thus, an interesting line of future work is to study what constitutes a good local minimum in the case of SNN s, which types of biases or deviations from the gradient help to find them, and how this relates to SGs. Another promising starting point for future work is tied to the stoch AD framework and questions pertaining to optimization effectiveness",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_676",
    "chunk_position": 676
  },
  {
    "id": "2404.14964v3",
    "text": ". Another promising starting point for future work is tied to the stoch AD framework and questions pertaining to optimization effectiveness. In this study, we linked SG-descent to smoothed stochastic derivatives, which are biased with respect to the non-smoothed based on the full stochastic triple. Thus far, the latter can only be computed in forward-mode AD, which is impractical for neural network training, as it comes with an increased computational cost",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_677",
    "chunk_position": 677
  },
  {
    "id": "2404.14964v3",
    "text": ". An intriguing direction for future research is, therefore, to investigate forward mode ADwith the full stochastic triple to determine whether training improvements due to the reduction in bias would justify the added computational cost. Furthermore, it would be worth exploring whether this computational cost could be reduced, for instance, by using mixed-mode AD, such as in DECOLLE 51 or online spatio-temporal learning 52 (see 53 for a review)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_678",
    "chunk_position": 678
  },
  {
    "id": "2404.14964v3",
    "text": ". Finally, an exciting direction for future research is to use our theoretically motivated methodology for training stochastic SNN s to address biological questions. In neurobiology, many neurons exhibit trial-to-trial variability. However, the role of this variability is not well understood. Here, functional stochastic SNN models will help to further our understanding of its role in the brain. Stochasticity may play possible roles in representational drift 54, efficient coding 55, and for biological implementations of latent generative models 56, 57",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_679",
    "chunk_position": 679
  },
  {
    "id": "2404.14964v3",
    "text": ". Thus, it would be interesting to study how learning influences representational changes in plastic functional stochastic SNN s and whether we can identify specific dynamic signatures that allow drawing conclusions about the underlying learning mechanisms in the brain. In conclusion, SGs remain a valuable tool for training both deterministic and stochastic SNN s. In this article, we saw thatstoch AD provides a theoretical backbone to SGlearning which naturally extends to stochastic SNN s. Further, it suggests that the choice of the SDshould be matched to the derivative of the escape noise",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_680",
    "chunk_position": 680
  },
  {
    "id": "2404.14964v3",
    "text": ". Further, it suggests that the choice of the SDshould be matched to the derivative of the escape noise. Training such stochastic SNN s is becoming increasingly relevant for applications with noisy data or noisy hardware substrates and is essential for theoretical neuroscience as it opens the door for studying functional SNN s with biologically realistic levels of trial-to-trial variability. 8 Methods Our Perceptron and SNN models were written in Python 3.10.4 and extended either on the stork library 34 which is based on Pytorch 58, or Jax 59. The can be found at surrogate-gradient-theory",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_681",
    "chunk_position": 681
  },
  {
    "id": "2404.14964v3",
    "text": ". The can be found at surrogate-gradient-theory . All models were implemented in discrete time. The following provide more details on the different neuron models before providing all the necessary information on the two learning tasks, including the architecture and parameters used for a given task. 8.1 Neuron models Perceptron. As mentioned in .1, the Perceptron is a simplified of the LIFneuron model, where there is no memory and no reset. The membrane dynamics Ul ifor Perceptron iin layer lare given for the special case of a single Perceptron by Eq",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_682",
    "chunk_position": 682
  },
  {
    "id": "2404.14964v3",
    "text": ". The membrane dynamics Ul ifor Perceptron iin layer lare given for the special case of a single Perceptron by Eq. (4) in the theoretical results section, or in general by Ul i X jwij Sjbi, where wijare the feed-forward weights, Sjare the binary outputs (spikes) of the previous layer, and biis an optional bias term. Leaky integrate-and-fire neuron. We used an LIFneuron model with exponential current-based synapses 19",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_683",
    "chunk_position": 683
  },
  {
    "id": "2404.14964v3",
    "text": ". Leaky integrate-and-fire neuron. We used an LIFneuron model with exponential current-based synapses 19. In discrete time, the membrane potential Ul inof neuron iin layer lat time step nis given by Ul in 1 mem Ul in (1mem)Il in (1Sl in), (21) where Il inis the input current and Sl inoutput spike of the neuron itself, which governs reset dynamics. With multiplicative reset as above, the neuron stays silent for one time step after each spike. The membrane decay variable 16 mem exp t mem is defined through the membrane time constant mem and the chosen time step t",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_684",
    "chunk_position": 684
  },
  {
    "id": "2404.14964v3",
    "text": ". The membrane decay variable 16 mem exp t mem is defined through the membrane time constant mem and the chosen time step t. The input current Il inis given by Il in 1 syn Il in X jwl ij Sl1 jn z feedforward X kvl ik Sl kn z recurrent, (22) where wijandvikare the feedforward and recurrent synaptic weights, respectively, corresponding to the previous layers spikes Sl1 jand the same layers spikes Sl k, respectively. The synaptic decay variable is again given as syn exp t syn , defined through the synaptic time constant syn",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_685",
    "chunk_position": 685
  },
  {
    "id": "2404.14964v3",
    "text": ". The synaptic decay variable is again given as syn exp t syn , defined through the synaptic time constant syn. At the beginning of each minibatch, the initial membrane potential value of all neurons was set to their resting potential Ul i0 Urest 0and the initial value for the synaptic current was zero as well, Il i0 0 . 8.2 Spike generation The generation of a spike follows the same mechanism in both, the LIFand the Perceptron neuron model. Depending on the membrane potential value, the activation function generates a spike or not",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_686",
    "chunk_position": 686
  },
  {
    "id": "2404.14964v3",
    "text": ". Depending on the membrane potential value, the activation function generates a spike or not. The following paragraphs highlight the differences between the deterministic and the stochastic cases. Deterministic spike generation. A spike is generated in every time step, in which the membrane potential crosses a threshold . Hence, the Heaviside step function is used to generate a spike deterministically at time step n: Sl in H Ul in . Stochastic spike generation",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_687",
    "chunk_position": 687
  },
  {
    "id": "2404.14964v3",
    "text": ". Hence, the Heaviside step function is used to generate a spike deterministically at time step n: Sl in H Ul in . Stochastic spike generation. A stochastic neuron with escape noise 19 may spike, even if the membrane potential is below the threshold, or vice versa not spike, even if the membrane potential is above the threshold. Therefore, in discrete time, the probability of spiking in time step nis pl in N Ul in (23) for each neuron. We used 1 1ex, if not stated otherwise. The hyperparameter Ndefines the steepness of the sigmoid, with N leading to deterministic spiking",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_688",
    "chunk_position": 688
  },
  {
    "id": "2404.14964v3",
    "text": ". We used 1 1ex, if not stated otherwise. The hyperparameter Ndefines the steepness of the sigmoid, with N leading to deterministic spiking. Spikes were then drawn from a Bernoulli distribution with probability pl in, hence Sl in Be. (24) The expected value of spiking the spike probability, so E Sl in pl in. To emphasize the similarity to deterministic spike generation, we can consider an formulation, where the stochasticity is absorbed into the threshold such that (23) and (24) are combined into Sl in H Ul in . (25) The threshold is defined by its mean , i.e",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_689",
    "chunk_position": 689
  },
  {
    "id": "2404.14964v3",
    "text": ". (25) The threshold is defined by its mean , i.e. the deterministic threshold, and a zero-mean noise . For to the with the Bernoulli random variable (Eq. (24)), the noise is distributed as the logi log 1 1x where xis drawn from a uniform distribution over the interval 0,1, i.e.x . 8.3 Differentiable example network The minimal example network (. 4A) was simulated using Jax 59. To implement the main essence of SGdescent in a differentiable network, we constructed a network of Perceptrons with sigmoid activation functions (see Eq. (10))",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_690",
    "chunk_position": 690
  },
  {
    "id": "2404.14964v3",
    "text": ". (10)). The SGwas implemented by a less steep sigmoid, which was used instead of the actual activation function on the backward pass. Integrated (surrogate) gradients. In general, the integral of the derivative of the loss should again the loss RL wdw L. The same holds true for the network output. In . 4, we computed this quantity for w0.2,0.2, as well as in two dimensions for the parameters v1andv2. In . 5, we did not compute this along a line but instead chose to compute this along a closed path in parameter space, i.e. a circle",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_691",
    "chunk_position": 691
  },
  {
    "id": "2404.14964v3",
    "text": ". In . 5, we did not compute this along a line but instead chose to compute this along a closed path in parameter space, i.e. a circle. To do so, we integrated the SGalong a circle in a two-dimensional hyperplane which is spanned by two randomly chosen orthonormal vectors d1andd2in parameter space. The position along the circle is defined by an angle and the circle is parametrized by aandbsuch thatarsiandbrcowithrthe radius of the circle. Hence, when integrating along the circle, the weights in the network changed according to the angle d1 d2",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_692",
    "chunk_position": 692
  },
  {
    "id": "2404.14964v3",
    "text": ". Hence, when integrating along the circle, the weights in the network changed according to the angle d1 d2. 17 8.4 Learning tasks We trained SNN s to evaluate the differences in learning between the stochastic and deterministic SNN models trained with SG-descent. For this purpose, we chose a spike train matching and a classification task to cover different output modalities. 8.4.1 Spike train matching If an SNN can learn a precise timing of spikes, it must be able to match its output spike times with some target output spike times",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_693",
    "chunk_position": 693
  },
  {
    "id": "2404.14964v3",
    "text": ". Therefore in the spike train matching task, we ask, whether both, the stochastic and the deterministic network can learn deterministic output spike trains. For the training, no optimizer is used and since we use a minibatch size of one, this means we apply batch gradient descent. The details about the used architecture, neuronal, and training parameters are given in the following paragraphs as well as in 2 and 3 for both, the stochastic and the deterministic versions of the network. The written in Jax. Task",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_694",
    "chunk_position": 694
  },
  {
    "id": "2404.14964v3",
    "text": ". The written in Jax. Task. The target spike trains to match were generated from a of the Radcliffe Camera in Oxford using dithering to create a binary image. We set this image as our target spike raster, where the x-axis corresponds to time steps and the y-axis is the neuron index. Hence, the image provides a binary spike train for each readout neuron. As an input, we used frozen Poisson-distributed spike trains with a per-neuron firing rate of 50 Hz. This created a one-image dataset that requires 200 input neurons, and 200 output neurons and has a duration of 198 time steps, i.e",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_695",
    "chunk_position": 695
  },
  {
    "id": "2404.14964v3",
    "text": ". This created a one-image dataset that requires 200 input neurons, and 200 output neurons and has a duration of 198 time steps, i.e. 198 ms when using t 1 ms . Network architecture. The above-described spike train matching task is an easy task, that could also be solved by an SNN without a hidden layer. However, since we knew that in the shallow case without a hidden layer, the only difference between our stochastic and our deterministic versions would lie in the loss, we decided to use a network with one hidden layer",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_696",
    "chunk_position": 696
  },
  {
    "id": "2404.14964v3",
    "text": ". Hence the used network architecture was a feed-forward network with 200 input units, 200 hidden units, and 200 readout units, run with t 1 ms for a total of 198 time steps (see also ). Weights were initialized in the fluctuation-driven regime 34 with a mean of zero and target membrane fluctuations U 1. For the stochastic networks, we averaged the update over ten trials, i.e., we sampled ten paths in the tree of spike patterns, used each of them to compute an SG, and took their average to perform the weight update before sampling another path for the next weight update (see . 3)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_697",
    "chunk_position": 697
  },
  {
    "id": "2404.14964v3",
    "text": ". 3). Reset at same time step. Matching a target binary image without further constraints with a spike train might require a neuron to spike in two adjacent time steps. However, this is not possible with the membrane dynamics as in Eq. (21), where after every spike the neuron stays silent for one time step. Hence for this task, we used slightly modified membrane dynamics Ul in 1 mem Ul i (1 mem)Il in, where the reset was applied at the same time step as the spike and thus high enough input in the next time step could make the neuron fire again. Loss functions",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_698",
    "chunk_position": 698
  },
  {
    "id": "2404.14964v3",
    "text": ". Loss functions. We trained the network using an L2loss function L21 NMLX i1TX n1 SL inSin2 to compute the distance between target spike train Sifor readout neuron iand output spike train SL inat the readout layer L, where MLis the number of readout neurons and Tis the number of time steps",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_699",
    "chunk_position": 699
  },
  {
    "id": "2404.14964v3",
    "text": ". Furthermore, we also monitor the van Rossum distance 35 Lv R1 2Zt SiSi (s)2 ds between the target spike train and the output spike train, which might be a better distance for spike trains, since it also takes temporal structure into account by punishing a spike that is five time steps off more than a spike that is only one time step off. It does so, by convolving the output and target spike train first with a temporal kernel , before computing the squared distance. We chose to be to the -kernel of the LIF neuron (t) 1 1mem syn exp t syn exp t mem withmem 10 ms , syn 5 ms . 18 Fano factor",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_700",
    "chunk_position": 700
  },
  {
    "id": "2404.14964v3",
    "text": ". We chose to be to the -kernel of the LIF neuron (t) 1 1mem syn exp t syn exp t mem withmem 10 ms , syn 5 ms . 18 Fano factor. The Fano factor Fwas calculated on the hidden layer and output spike trains Sas F2 S S. 8.4.2 Classification on SHD To evaluate performance differences, we also evaluated both the stochastic and the deterministic on a classification task using a multi-layer recurrent Conv SNN (as in Rossbroich et al. 34), hence also increasing the difficulty for the stochastic model, which now had to cope with multiple noisy layers",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_701",
    "chunk_position": 701
  },
  {
    "id": "2404.14964v3",
    "text": ". 34), hence also increasing the difficulty for the stochastic model, which now had to cope with multiple noisy layers. The details about the used architecture, neuronal, and training parameters are given in the following paragraphs as well as in 2 and 3 for both, the stochastic and the deterministic versions of the network. Task. For the classification task, we used the SHD dataset 36, which is a real-world auditory dataset containing spoken digits in German and English from different speakers, hence it has 20 classes. The dataset can be downloaded from . Cramer et al",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_702",
    "chunk_position": 702
  },
  {
    "id": "2404.14964v3",
    "text": ". The dataset can be downloaded from . Cramer et al. 36 preprocessed the recordings using a biologically inspired cochlea model to create input spike trains for n 700 neurons. The duration of the samples varies, hence we decided to consider only the first TSHD 700 ms of each sample, which covers 98 of the original spikes. For our numerical simulations, we binned the spike trains using a t 2 ms and hence we hadt TSHD 350 time steps in this task",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_703",
    "chunk_position": 703
  },
  {
    "id": "2404.14964v3",
    "text": ". For our numerical simulations, we binned the spike trains using a t 2 ms and hence we hadt TSHD 350 time steps in this task. 10 of the training data was used as a validation set, and for testing we used the officially provided test set, which contains only speakers that did not appear in the training set. Network architecture. We used a multi-layer recurrent Conv SNN for the SHD classification task (see also ). There were 700 input neurons, that directly get fed the input spikes from the SHD dataset",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_704",
    "chunk_position": 704
  },
  {
    "id": "2404.14964v3",
    "text": ". There were 700 input neurons, that directly get fed the input spikes from the SHD dataset. The network had three recurrently connected hidden layers and used one-dimensional convolution kernels. Weights were initialized in the fluctuation-driven regime with a mean of zero, target membrane fluctuations U 1, and the proportion of fluctuations due to the feed-forward input was 0.9. The network size as well as the parameters for the feed-forward and recurrent convolutional operations are summarized in . We performed only one trial per update for the stochastic case. Readout units",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_705",
    "chunk_position": 705
  },
  {
    "id": "2404.14964v3",
    "text": ". We performed only one trial per update for the stochastic case. Readout units. As opposed to the previous task, a classification task requires special readout units. The readout units did have the same membrane and current dynamics as a normal LIFneuron (. (21) an), but they did not spike. Furthermore, we used a different membrane time constant RO mem TSHD 700 ms for the readout units. This allowed us to read out their membrane potential for classification (see paragraph on the loss function). Activity regularization",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_706",
    "chunk_position": 706
  },
  {
    "id": "2404.14964v3",
    "text": ". This allowed us to read out their membrane potential for classification (see paragraph on the loss function). Activity regularization. To prevent tonic firing, we used activity regularization to constrain the upper bound of firing activity. To this end, we constructed an additional loss term as a soft upper bound on the average firing activity of each feature in our convolutional layers",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_707",
    "chunk_position": 707
  },
  {
    "id": "2404.14964v3",
    "text": ". To this end, we constructed an additional loss term as a soft upper bound on the average firing activity of each feature in our convolutional layers. Hence for every layer, we computed a term gl,k upper 1 Ml Ml X il,k iupper 2 , (26) where Mlnfeatures nchannels is the number of neurons in layer landl,k i PT n Sl,k inis the spike count of neuron iin layer lgiven input k. We chose the parameter upper 7 to constrain the average firing rate to 10 Hz",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_708",
    "chunk_position": 708
  },
  {
    "id": "2404.14964v3",
    "text": ". We chose the parameter upper 7 to constrain the average firing rate to 10 Hz. Hence, the total upper bound activity regularization loss is given by Lupper upper LX l1Fl X f1gl,k upper , (27) where upper 0.01was the regularization strength. Those parameters can also be found in . Loss function. As this is a classification task, we used a maximum-over-time readout. To that end, we used a standard cross-entropy loss LCE1 Kk X i1CX c1yk clog pk c (28) to sum over all samples Kand all classes C. The correct class is encoded in yk cas a one-hot encoded target for the input k",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_709",
    "chunk_position": 709
  },
  {
    "id": "2404.14964v3",
    "text": ". The correct class is encoded in yk cas a one-hot encoded target for the input k. To compute the single probabilities pk cfor each class c, we first read out the maximum membrane potential values of each readout neuron over simulation time to get the activities ak c max . (29) 19 From those activities, we computed the probabilities using a Softmax function pk cex PC cex. Optimizer. We used the squared mean over root mean squared cubed ( SMORMS3 ) optimizer 34, 60, which chooses a learning rate based on how noisy the gradient is",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_710",
    "chunk_position": 710
  },
  {
    "id": "2404.14964v3",
    "text": ". We used the squared mean over root mean squared cubed ( SMORMS3 ) optimizer 34, 60, which chooses a learning rate based on how noisy the gradient is. The SMORMS3 optimizer keeps track of the three values g,g2and m, which were initialized to gg2 0andm 1. They are updated after every minibatch as follows: r1 m 1 g (1 r)gr L g2 (1 r)g2r L 2 m 1 m1g2 g2. Given this, SMORMS3 computes the current effective learning rate as current min ,g2 g2 g2, where is the initially chosen learning rate and is a small constant to avoid division by zero. Therefore the parameter update is current L . Fano factor",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_711",
    "chunk_position": 711
  },
  {
    "id": "2404.14964v3",
    "text": ". Therefore the parameter update is current L . Fano factor. The Fano factor was calculated after taking a moving average over the spike train Swith a window of 10 timesteps (20 ms) to get Sbin. Subsequently, the Fano factor Fwas computed as F2 Sbin Sbin. . Noise and surrogate function parameters. Parameters used in our numerical simulations for feed-forward networks on the spike-train matching task and multi-layer recurrent Conv SNN s on the SHD classification task. We selected the learning rate based on the best validation accuracy (cf. Supplementary . S2B)",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_712",
    "chunk_position": 712
  },
  {
    "id": "2404.14964v3",
    "text": ". We selected the learning rate based on the best validation accuracy (cf. Supplementary . S2B). The Super Spike nonlinearity is the derivative of a fast sigmoid scaled by1 : 1 (x1)2",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_713",
    "chunk_position": 713
  },
  {
    "id": "2404.14964v3",
    "text": ". Supplementary . S2B). The Super Spike nonlinearity is the derivative of a fast sigmoid scaled by1 : 1 (x1)2. Spike train matching task Classification task stochastic deterministic stochastic deterministic Number of trials 10 1 1 1 Learning rate hid 1005 out 1005hid 1005 out 10040.01 0.01 Escape noise Function () step () step Parameter hid 10 out 10010 - Surrogate gradient Function () Super Spike Super Spike Super Spike Parameter hid 10 10 10 10 Acknowledgments The authors thank Frank Schfer, Guillaume Bellec, and Wulfram Gerstner for stimulating discussions",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_714",
    "chunk_position": 714
  },
  {
    "id": "2404.14964v3",
    "text": ". This work was supported by the Swiss National Science Foundation (Grant Number PCEFP3202981) and the Novartis Research Foundation. 20 . Network and training parameters. Parameters used in numerical simulations for feed-forward SNN s on the spike-train matching task and multi-layer recurrent Conv SNNs on the SHD classification task. Spike train matching task Classification task Dataset Radcliffe Camera SHD No. input neurons 200 700 No. hidden neurons 200 16-32-64 No. output neurons 200 20 No",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_715",
    "chunk_position": 715
  },
  {
    "id": "2404.14964v3",
    "text": ". hidden neurons 200 16-32-64 No. output neurons 200 20 No. training epochs 5000 200 Time step 1 ms 2 ms Duration 198 ms 700 ms Mini-batch size 1 400 Kernel size (ff) - 21-7-7 Stride (ff) - 10-3-3 Padding (ff) - 0-0-0 Kernel size (rec) - 5 Stride (rec) - 1 Padding (rec) - 2 Loss L2or van Rossum distance 35 Maximum over time Optimizer None (gradient descent) SMORMS360 Neuronal parameters Spike threshold 1 1 Resting potential 0 0 mem 10 ms 20 ms syn 5 ms 10 ms RO mem - 700 ms Reset at same time step at next time step Activity regularizer upper - 7 upper - 0.01 21 References 1 Eshraghian, J",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_716",
    "chunk_position": 716
  },
  {
    "id": "2404.14964v3",
    "text": ". K., Ward, M., Neftci, E., Wang, X., Lenz, G., Dwivedi, G., Bennamoun, M., Jeong, D. S., and Lu, W. D. Training Spiking Neural Networks Using Lessons From Deep Learning. In: Proceedings of the IEEE 111.9 (Sept. 2023), 10161054. : 1558-2256. . 2 Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning Representations by Back-Propagating Errors. In: Nature 323.6088 (Oct. 1986), 533536. : 0028-0836. . 3 Gtig, R. and Sompolinsky, H. The Tempotron: A Neuron That Learns Spike Timing Based Decisions. In: Nature Neuroscience 9.3 (Mar. 2006), 420428. : 1546-1726. . 4 Memmesheimer, R",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_717",
    "chunk_position": 717
  },
  {
    "id": "2404.14964v3",
    "text": ". The Tempotron: A Neuron That Learns Spike Timing Based Decisions. In: Nature Neuroscience 9.3 (Mar. 2006), 420428. : 1546-1726. . 4 Memmesheimer, R. -M., Rubin, R., lveczky, B. P., and Sompolinsky, H. Learning Precisely Timed Spikes. In: Neuron 82.4 (May 2014), 925938. : 0896-6273. . 5 Huh, D. and Sejnowski, T. J. Gradient Descent for Spiking Neural Networks. In: Advances in Neural Information Processing Systems . V ol. 31. Curran Associates, Inc., 2018. 6 Bohte, S. M., Kok, J. N., and La Poutr, H. Error-Backpropagation in Temporally Encoded Networks of Spiking Neurons",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_718",
    "chunk_position": 718
  },
  {
    "id": "2404.14964v3",
    "text": ". Curran Associates, Inc., 2018. 6 Bohte, S. M., Kok, J. N., and La Poutr, H. Error-Backpropagation in Temporally Encoded Networks of Spiking Neurons. In: Neurocomputing 48.1-4 (Oct. 2002), 1737. : 09252312. . 7 Mostafa, H. Supervised Learning Based on Temporal Coding in Spiking Neural Networks. In: IEEE Transactions on Neural Networks and Learning Systems (2017), 19. : 2162-237X, 2162-2388. TNNLS.2017.2726060 . 8 Wunderlich, T. C. and Pehle, C. Event-Based Backpropagation Can Compute Exact Gradients for Spiking Neural Networks. In: Scientific Reports 11.1 (June 2021), p. 12829. : 2045-2322.",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_719",
    "chunk_position": 719
  },
  {
    "id": "2404.14964v3",
    "text": ". In: Scientific Reports 11.1 (June 2021), p. 12829. : 2045-2322. . 9 Klos, C. and Memmesheimer, R. -M.Smooth Exact Gradient Descent Learning in Spiking Neural Networks . Sept. 2023. ar Xiv: 2309.14523 cs, q-bio . 10 Bohte, S. M. Error-Backpropagation in Networks of Fractionally Predictive Spiking Neurons. In: Artificial Neural Networks and Machine Learning ICANN 2011 . Ed. by Honkela, T., Duch, W., Girolami, M., and Kaski, S. Berlin, Heidelberg: Springer, 2011, 6068. : 978-3-642-21735-7. . 11 Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_720",
    "chunk_position": 720
  },
  {
    "id": "2404.14964v3",
    "text": ". Berlin, Heidelberg: Springer, 2011, 6068. : 978-3-642-21735-7. . 11 Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y . Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to 1 or -1. In: ar Xiv:1602.02830 cs (Feb. 2016). ar Xiv: 1602.02830. 12 Esser, S. K., Merolla, P. A., Arthur, J. V ., Cassidy, A. S., Appuswamy, R., Andreopoulos, A., Berg, D. J., Mc Kinstry, J. L., Melano, T., Barch, D. R., Nolfo, C. di, Datta, P., Amir, A., Taba, B., Flickner, M. D., and Modha, D. S",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_721",
    "chunk_position": 721
  },
  {
    "id": "2404.14964v3",
    "text": ". J., Mc Kinstry, J. L., Melano, T., Barch, D. R., Nolfo, C. di, Datta, P., Amir, A., Taba, B., Flickner, M. D., and Modha, D. S. Convolutional networks for fast, energy-efficient neuromorphic computing. In: Proceedings of the National Academy of Sciences of the United States of America 113.41 (Oct. 2016), 1144111446. : 0027-8424. . 13 Zenke, F. and Ganguli, S. Super Spike: Supervised Learning in Multilayer Spiking Neural Networks. In: Neural Computation 30.6 (May 2018), 15141541. : 1530888X. . ar Xiv: 1705.11146 . 14 Bellec, G., Salaj, D., Subramoney, A., Legenstein, R., and Maass, W",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_722",
    "chunk_position": 722
  },
  {
    "id": "2404.14964v3",
    "text": ". : 1530888X. . ar Xiv: 1705.11146 . 14 Bellec, G., Salaj, D., Subramoney, A., Legenstein, R., and Maass, W. Long Short-Term Memory and Learningto-learn in Networks of Spiking Neurons. In: Advances in Neural Information Processing Systems . V ol. 31. Curran Associates, Inc., 2018. 15 Neftci, E. O., Mostafa, H., and Zenke, F. Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-based Optimization to Spiking Neural Networks. In: IEEE Signal Processing Magazine 36.6 (Nov. 2019), 5163. : 15580792. . 16 Zenke, F. and V ogels, T. P",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_723",
    "chunk_position": 723
  },
  {
    "id": "2404.14964v3",
    "text": ". In: IEEE Signal Processing Magazine 36.6 (Nov. 2019), 5163. : 15580792. . 16 Zenke, F. and V ogels, T. P. The Remarkable Robustness of Surrogate Gradient Learning for Instilling Complex Function in Spiking Neural Networks. In: Neural Computation 33.4 (Mar. 2021), 899925. : 0899-7667. . 17 Jang, H., Simeone, O., Gardner, B., and Gruning, A. An Introduction to Probabilistic Spiking Neural Networks: Probabilistic Models, Learning Rules, and Applications. In: IEEE Signal Processing Magazine 36.6 (Nov. 2019), 6477. : 1558-0792. . 18 Arya, G., Schauer, M., Schfer, F., and Rackauckas, C",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_724",
    "chunk_position": 724
  },
  {
    "id": "2404.14964v3",
    "text": ". In: IEEE Signal Processing Magazine 36.6 (Nov. 2019), 6477. : 1558-0792. . 18 Arya, G., Schauer, M., Schfer, F., and Rackauckas, C. Automatic Differentiation of Programs with Discrete Randomness. In: Advances in Neural Information Processing Systems 35 (Dec. 2022), 1043510447. 19 Gerstner, W., Kistler, W. M., Naud, R., and Paninski, L. Neuronal Dynamics -From Single Neurons to Networks and Models of Cognition . Cambridge University Press, 2014. : 978-1-107-63519-7. 22 20 Pfister, J. -P., Toyoizumi, T., Barber, D., and Gerstner, W",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_725",
    "chunk_position": 725
  },
  {
    "id": "2404.14964v3",
    "text": ". Cambridge University Press, 2014. : 978-1-107-63519-7. 22 20 Pfister, J. -P., Toyoizumi, T., Barber, D., and Gerstner, W. Optimal Spike-Timing-Dependent Plasticity for Precise Action Potential Firing in Supervised Learning. In: Neural Computation 18.6 (June 2006), 13181348. : 0899-7667. . 21 Brea, J., Senn, W., and Pfister, J. -P. Matching Recall and Storage in Sequence Learning with Spiking Neural Networks. In: Journal of Neuroscience 33.23 (June 2013), 95659575. : 0270-6474. JNEUROSCI.4098-12.2013 . 22 Gardner, B., Sporea, I., and Grning, A",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_726",
    "chunk_position": 726
  },
  {
    "id": "2404.14964v3",
    "text": ". In: Journal of Neuroscience 33.23 (June 2013), 95659575. : 0270-6474. JNEUROSCI.4098-12.2013 . 22 Gardner, B., Sporea, I., and Grning, A. Learning Spatiotemporally Encoded Pattern Transformations in Structured Spiking Neural Networks. In: Neural computation 27.12 (Dec. 2015), 254886. : 1530888X. . 23 Hammouamri, I., Khalfaoui-Hassani, I., and Masquelier, T. Learning Delays in Spiking Neural Networks Using Dilated Convolutions with Learnable Spacings . June 2023. ar Xiv: 2306.17670 cs . 24 Herranz-Celotti, L. and Rouat, J. Stabilizing Spiking Neuron Training . 2024. ar Xiv: 2202.00282 cs.NE",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_727",
    "chunk_position": 727
  },
  {
    "id": "2404.14964v3",
    "text": ". June 2023. ar Xiv: 2306.17670 cs . 24 Herranz-Celotti, L. and Rouat, J. Stabilizing Spiking Neuron Training . 2024. ar Xiv: 2202.00282 cs.NE . 25 Gerstner, W. and Kistler, W. M. Spiking Neuron Models . Cambridge University Press, 2002. : 978-0-52189079-3. 26 Hinton, G. E. Lectures from the 2012 Coursera Course: Neural Networks for Machine Learning . 2012. 27 Bengio, Y ., Lonard, N., and Courville, A. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation . Aug. 2013. ar Xiv: 1308.3432 cs . 28 Yin, P., Lyu, J., Zhang, S., Osher, S., Qi, Y ., and Xin, J",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_728",
    "chunk_position": 728
  },
  {
    "id": "2404.14964v3",
    "text": ". Aug. 2013. ar Xiv: 1308.3432 cs . 28 Yin, P., Lyu, J., Zhang, S., Osher, S., Qi, Y ., and Xin, J. Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets. In: International Conference on Learning Representations . Sept. 2018. 29 Marschall, O., Cho, K., and Savin, C. A Unified Framework of Online Learning for Training Recurrent Neural Networks. In: ar Xiv:1907.02649 cs, q-bio, stat (July 2019). ar Xiv: 1907.02649. 30 Werfel, J., Xie, X., and Seung, H. S. Learning curves for stochastic gradient descent in linear feedforward networks",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_729",
    "chunk_position": 729
  },
  {
    "id": "2404.14964v3",
    "text": ". ar Xiv: 1907.02649. 30 Werfel, J., Xie, X., and Seung, H. S. Learning curves for stochastic gradient descent in linear feedforward networks. In: Advances in neural information processing systems . 2004, 11971204. 31 Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J., and Hinton, G. Backpropagation and the brain. en. In: Nature Reviews Neuroscience (Apr. 2020). Publisher: Nature Publishing Group, 112. : 1471-0048. 10.1038s41583-020-0277-3 . 32 Glasserman, P. Estimating Sensitivities. In: Monte Carlo Methods in Financial Engineering . Ed. by Glasserman, P",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_730",
    "chunk_position": 730
  },
  {
    "id": "2404.14964v3",
    "text": ". 10.1038s41583-020-0277-3 . 32 Glasserman, P. Estimating Sensitivities. In: Monte Carlo Methods in Financial Engineering . Ed. by Glasserman, P. New York, NY: Springer, 2003, 377420. : 978-0-387-21617-1. 33 Fu, M. C. Gradient Estimation. In: Handbooks in Operations Research and Management Science . Ed. by Henderson, S. G. and Nelson, B. L. V ol. 13. Simulation. Elsevier, Jan. 2006, 575616. S0927-0507(06)13019-4 . 34 Rossbroich, J., Gygax, J., and Zenke, F. Fluctuation-Driven Initialization for Spiking Neural Network Training. In:Neuromorphic Computing and Engineering 2.4 (Dec. 2022), p",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_731",
    "chunk_position": 731
  },
  {
    "id": "2404.14964v3",
    "text": ". Fluctuation-Driven Initialization for Spiking Neural Network Training. In:Neuromorphic Computing and Engineering 2.4 (Dec. 2022), p. 044016. : 2634-4386. 2634-4386ac97bb . 35 Rossum, M. C. W. van. A Novel Spike Distance. In: Neural Computation 13.4 (Apr. 2001), 751763. : 0899-7667. . 36 Cramer, B., Stradmann, Y ., Schemmel, J., and Zenke, F. The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks. In: IEEE Transactions on Neural Networks and Learning Systems 33.7 (July 2022), 27442757. : 2162-2388. . 37 Neal, R. M. Connectionist Learning of Belief Networks",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_732",
    "chunk_position": 732
  },
  {
    "id": "2404.14964v3",
    "text": ". : 2162-2388. . 37 Neal, R. M. Connectionist Learning of Belief Networks. In: Artificial Intelligence 56.1 (July 1992), 71113. : 0004-3702. . 38 Maddison, C. J., Mnih, A., and Teh, Y . W. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables . Mar. 2017. Xiv.1611.00712 . ar Xiv: 1611.00712 cs, stat . 39 Jang, E., Gu, S., and Poole, B. Categorical Reparameterization with Gumbel-Softmax. In: International Conference on Learning Representations . Nov. 2016. 40 Williams, R. J. Simple Statistical Gradient-Following for Connectionist Reinforcement Learning",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_733",
    "chunk_position": 733
  },
  {
    "id": "2404.14964v3",
    "text": ". Nov. 2016. 40 Williams, R. J. Simple Statistical Gradient-Following for Connectionist Reinforcement Learning. In: Machine Learning 8.3-4 (May 1992), 229256. : 0885-6125, 1573-0565. . 41 Schulman, J., Heess, N., Weber, T., and Abbeel, P. Gradient Estimation Using Stochastic Computation Graphs. In:Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada . Ed. by Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. 2015, 35283536",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_734",
    "chunk_position": 734
  },
  {
    "id": "2404.14964v3",
    "text": ". Ed. by Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. 2015, 35283536. 42 Weber, T., Heess, N., Buesing, L., and Silver, D. Credit Assignment Techniques in Stochastic Computation Graphs. In: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics . PMLR, Apr. 2019, 26502660. 23 43 Liu, L., Dong, C., Liu, X., Yu, B., and Gao, J. Bridging Discrete and Backpropagation: Straight-Through and Beyond . Oct. 2023. ar Xiv: 2304.08612 cs . 44 Tokui, S. and Sato, I. Evaluating the Variance of Likelihood-Ratio Gradient Estimators",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_735",
    "chunk_position": 735
  },
  {
    "id": "2404.14964v3",
    "text": ". Oct. 2023. ar Xiv: 2304.08612 cs . 44 Tokui, S. and Sato, I. Evaluating the Variance of Likelihood-Ratio Gradient Estimators. In: Proceedings of the 34th International Conference on Machine Learning . PMLR, July 2017, 34143423. 45 Clopath, C. and Gerstner, W. V oltage and Spike Timing Interact in STDP A Unified Model. In: Frontiers in Synaptic Neuroscience 2 (July 21, 2010). : 1663-3563. . 46 Ma, G., Yan, R., and Tang, H. Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks . May 2023. Xiv.2305.16044 . ar Xiv: 2305.16044 cs",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_736",
    "chunk_position": 736
  },
  {
    "id": "2404.14964v3",
    "text": ". Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks . May 2023. Xiv.2305.16044 . ar Xiv: 2305.16044 cs . 47 Jolivet, R., Rauch, A., Lscher, H. -R., and Gerstner, W. Predicting Spike Timing of Neocortical Pyramidal Neurons by Simple Threshold Models. In: Journal of Computational Neuroscience 21.1 (Aug. 2006), 3549. : 1573-6873. . 48 Pozzorini, C., Mensi, S., Hagens, O., Naud, R., Koch, C., and Gerstner, W. Automated High-Throughput Characterization of Single Neurons by Means of Simplified Spiking Models",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_737",
    "chunk_position": 737
  },
  {
    "id": "2404.14964v3",
    "text": ". Automated High-Throughput Characterization of Single Neurons by Means of Simplified Spiking Models. In: PLOS Computational Biology 11.6 (June 2015), e1004275. : 1553-7358. . 49 Ghosh, A., Liu, Y . H., Lajoie, G., Kording, K., and Richards, B. A. How gradient estimator variance and bias impact learning in neural networks. en. In: International Conference on Learning Representations (ICLR) (Sept. 2023). 50 Gulcehre, C., Moczulski, M., Visin, F., and Bengio, Y . Mollifying Networks. In: International Conference on Learning Representations . 2017. 51 Kaiser, J., Mostafa, H., and Neftci, E",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_738",
    "chunk_position": 738
  },
  {
    "id": "2404.14964v3",
    "text": ". Mollifying Networks. In: International Conference on Learning Representations . 2017. 51 Kaiser, J., Mostafa, H., and Neftci, E. Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE). In: Frontiers in Neuroscience 14 (2020). : 1662-453X. . 52 Bohnstingl, T., Wo zniak, S., Pantazi, A., and Eleftheriou, E. Online Spatio-Temporal Learning in Deep Neural Networks. In: IEEE Transactions on Neural Networks and Learning Systems 34.11 (2023), 88948908. 10.1109TNNLS.2022.3153985 . 53 Zenke, F. and Neftci, E. O. Brain-Inspired Learning on Neuromorphic Substrates",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_739",
    "chunk_position": 739
  },
  {
    "id": "2404.14964v3",
    "text": ". 10.1109TNNLS.2022.3153985 . 53 Zenke, F. and Neftci, E. O. Brain-Inspired Learning on Neuromorphic Substrates. In: Proceedings of the IEEE 109.5 (May 2021), 935950. : 1558-2256. . 54 Micou, C. and OLeary, T. Representational Drift as a Window into Neural and Behavioural Plasticity. In: Current Opinion in Neurobiology 81 (Aug. 2023), p. 102746. : 0959-4388. 102746 . 55 Denve, S. and Machens, C. K. Efficient and Balanced Networks. In: Nature Neuroscience 19.3 (Mar. 2016), 375382. : 1097-6256, 1546-1726. . 56 Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_740",
    "chunk_position": 740
  },
  {
    "id": "2404.14964v3",
    "text": ". In: Nature Neuroscience 19.3 (Mar. 2016), 375382. : 1097-6256, 1546-1726. . 56 Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. The Wake-Sleep Unsupervised Neural Networks. In: Science 268.5214 (May 1995), 11581161. . 57 Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-Resolution Image Synthesis with Latent Diffusion Models . Apr. 2022. Xiv.2112.10752 . ar Xiv: 2112.10752",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_741",
    "chunk_position": 741
  },
  {
    "id": "2404.14964v3",
    "text": ". High-Resolution Image Synthesis with Latent Diffusion Models . Apr. 2022. Xiv.2112.10752 . ar Xiv: 2112.10752 . 58 Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., De Vito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Py Torch: An Imperative Style, High-Performance Deep Learning Library. In: Advances in Neural Information Processing Systems . V ol. 32. Curran Associates, Inc., 2019. 59 Bradbury, J., Frosting, R., Hawkings, P., Johnson, M",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_742",
    "chunk_position": 742
  },
  {
    "id": "2404.14964v3",
    "text": ". V ol. 32. Curran Associates, Inc., 2019. 59 Bradbury, J., Frosting, R., Hawkings, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., Vander Plas, J., Wanderman-Milne, S., and Zhang, Q. JAX: Composable Transformation of Python Num Py Programs . 2018. 60 Funk, S. RMSprop Loses to SMORMS3 - Beware the Epsilon! Apr. 2015. 24 A Supplementary before learning A B C aer learning Time s S1. The bias induced in SNN training due to deterministic SGs is small",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_743",
    "chunk_position": 743
  },
  {
    "id": "2404.14964v3",
    "text": ". 2015. 24 A Supplementary before learning A B C aer learning Time s S1. The bias induced in SNN training due to deterministic SGs is small. (A) Spike raster plot of a network trained on the random manifolds ( Randman ) dataset 16; top: membrane potential of the readout units, middle: spike raster plot of the 128 hidden units, bottom: input data. Time is on the x-axis, the y-axis is the neuron index and each dot is a spike",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_744",
    "chunk_position": 744
  },
  {
    "id": "2404.14964v3",
    "text": ". Time is on the x-axis, the y-axis is the neuron index and each dot is a spike. (B)Cosine similarity of gradients obtained in 100 single trials in a stochastic SNN performing the Randman task in (A) with respect to the mean gradient over 100 trials before and after learning. Teal shows the cosine similarity between the deterministic and the mean stochastic gradient. (C)First two principal components of the gradients (with the mean added back) obtained in different trials with the stochastic network before (left) and after (right) training on the Randman task for 200 epochs",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_745",
    "chunk_position": 745
  },
  {
    "id": "2404.14964v3",
    "text": ". Teal is the deterministic, and silver is the mean gradient. One can see that the direction chosen by the deterministic update slightly deviates from the average direction of an update in a stochastic network. train valid test other0.70.80.91.0Accuracy det. stoch.A B C 103 101 Learning rate0.60.81.0Valid. acc. stoch.det. S2. Supplementary metrics on the Conv SNN experiments. (A) Train, validation and test accuracy for the three-layer Conv SNN trained on SHD for the stochastic and deterministic case",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_746",
    "chunk_position": 746
  },
  {
    "id": "2404.14964v3",
    "text": ". (A) Train, validation and test accuracy for the three-layer Conv SNN trained on SHD for the stochastic and deterministic case. In the case of the stochastic SNN , escape noise is applied during, training, validation, and testing. The box labeled other shows the performance, if the stochastic network is evaluated on the test set without any escape noise being present and vice versa for the deterministic. (B)Validation accuracy for the stochastic and deterministic networks after training for different learning rates",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_747",
    "chunk_position": 747
  },
  {
    "id": "2404.14964v3",
    "text": ". (B)Validation accuracy for the stochastic and deterministic networks after training for different learning rates. (C)Validation accuracy for different setups, that applied specific changes to the setup from the main text (compare . 7): diff. reset stoch. : stochastic network trained with BPalso through the reset term. Scaled sigmoid SG: stochastic network trained with sigmoid instead of fast sigmoid SG, but scaled by1 SG, as done in Zenke et al. 13 for the fast sigmoid. Sigmoid SG : same as (B), but without scaling and SG 1",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_748",
    "chunk_position": 748
  },
  {
    "id": "2404.14964v3",
    "text": ". 13 for the fast sigmoid. Sigmoid SG : same as (B), but without scaling and SG 1. B Example: stochastic derivative of a Perceptron as in stoch AD 18 Let us consider a stochastic binary Perceptron as in Eq. (7). Let us first consider only the derivative of the Bernoulli with respect to the probability of firing p, namelydy dpd dp Be, as we can later apply the chain rule after smoothing",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_749",
    "chunk_position": 749
  },
  {
    "id": "2404.14964v3",
    "text": ". For the right stochastic derivative (which takes into account jumps from 0to1), we assume 0, thus the differential d will be d() 1if1p 1p 0otherwise where is the actual sample drawn from the Bernoulli and we use this fixed randomness to compute the differential. Given a sample () 1 , there can be no jump, but if we start with () 0 , there is a probability of1 1pthat the output of ()will jump from zero to one",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_750",
    "chunk_position": 750
  },
  {
    "id": "2404.14964v3",
    "text": ". Given a sample () 1 , there can be no jump, but if we start with () 0 , there is a probability of1 1pthat the output of ()will jump from zero to one. A stochastic derivative is written as triple, where the first number is the almost sure part of the derivative, the second is the weight (probability) wof a finite jump in the derivative, and finally, we have the alternate value Y, which is the new value of the output if the jump occurred. Hence, in our case, the correct stochastic derivative would be (R, w R, YR) ( (0,1 1p,1) if () 0 (0,0,0) if () 1",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_751",
    "chunk_position": 751
  },
  {
    "id": "2404.14964v3",
    "text": ". Hence, in our case, the correct stochastic derivative would be (R, w R, YR) ( (0,1 1p,1) if () 0 (0,0,0) if () 1. 25 For the left stochastic derivative, we would only consider jumps from one to zero. So the left stochastic derivative is (L, w L, YL) ( (0,0,0) if () 0 (0,1 p,0) if () 1. To use stochastic derivatives with BP, one needs to smooth them first. This, however, is no longer an unbiased solution. The smoothed stochastic derivative is defined as eE)(see also Eq. (3)). Hence in our case, we have eR1 1p10andeL1 p11",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_752",
    "chunk_position": 752
  },
  {
    "id": "2404.14964v3",
    "text": ". The smoothed stochastic derivative is defined as eE)(see also Eq. (3)). Hence in our case, we have eR1 1p10andeL1 p11. We know from (7), that p(u)and we can compute the continuous part of the derivative, e.g.d dup(u)(1(u))by applying the chain rule. Put together, we end up with etot R1 1p10dp du 1 1(u)10(u)(1(u)) (u)10 for the smoothed right stochastic derivative, and etot L1 p11dp du 1 (u)11(u)(1(u)) (1(u))11 for the smoothed left stochastic derivative. Now since every affine combination of the left and the right smoothed S3",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_753",
    "chunk_position": 753
  },
  {
    "id": "2404.14964v3",
    "text": ". Now since every affine combination of the left and the right smoothed S3. Smoothed stochastic derivatives: Any affine combination of a smoothed left (light blue) and a smoothed right (teal) stochastic derivative is a valid stochastic derivative (red) given a specific realization",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_754",
    "chunk_position": 754
  },
  {
    "id": "2404.14964v3",
    "text": ". stochastic derivative is a valid smoothed stochastic derivative, we can choose our smoothed stochastic derivative to be (1p)etot Rpetot Lwhich evaluates to etot (1 (u))(u)10(u)(1(u))11 (u)(1(u)) Therefore, when using a specific affine combination of the left and the right smoothed stochastic derivatives, we can writedy du(u)(1(u)) (u)thereby exactly recovering SDs in a stochastic network. Furthermore, we find the same expression for the SDindependent of the outcome of the Bernoulli random variable",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_755",
    "chunk_position": 755
  },
  {
    "id": "2404.14964v3",
    "text": ". Furthermore, we find the same expression for the SDindependent of the outcome of the Bernoulli random variable. The SDand the smoothed left and right stochastic derivatives for one sample of a stochastic Perceptron are shown in . S3. 26",
    "source": "arXiv",
    "chunk_id": "2404.14964v3_756",
    "chunk_position": 756
  },
  {
    "id": "1901.09948v2",
    "text": "Surrogate Gradient Learning in Spiking Neural Networks Emre O. Neftciy,Member, IEEE, Hesham Mostafay,Member, IEEE, Friedemann Zenkey y All authors contributed . The order of authors is arbitrary. Abstract Spiking neural networks are natures versatile solution to fault-tolerant and energy efcient signal processing. To translate these benets into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_757",
    "chunk_position": 757
  },
  {
    "id": "1901.09948v2",
    "text": ". These developments have created an imminent need for methods and tools to enable such systems to solve realworld signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specic data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates stepby-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_758",
    "chunk_position": 758
  },
  {
    "id": "1901.09948v2",
    "text": ". To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specically, as a particularly exible and efcient method to overcome the aforementioned challenges. I. I NTRODUCTION Biological spiking neural networks (SNNs) are evolutions highly efcient solution to the problem of signal processing. Therefore, taking inspiration from the brain is a natural approach to engineering more efcient computing architectures",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_759",
    "chunk_position": 759
  },
  {
    "id": "1901.09948v2",
    "text": ". Therefore, taking inspiration from the brain is a natural approach to engineering more efcient computing architectures. In the area of machine learning, recurrent neural networks (RNNs), a class of stateful neural networks whose internal state evolves with time (Box. 1), have proven highly effective at solving real-time pattern recognition and noisy time series prediction problems 1. RNNs and biological neural networks share several properties, such as a similar general architecture, temporal dynamics and learning through weight adjustments",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_760",
    "chunk_position": 760
  },
  {
    "id": "1901.09948v2",
    "text": ". Based on these similarities, a growing body of work is now establishing formal Xiv:1901.09948v2 cs.NE 3 May 2019 2 between RNNs and networks of spiking leaky integrate-and-re (LIF) neurons which are widely used in computational neuroscience 25. RNNs are typically trained using an optimization procedure in which the parameters or weights are adjusted to minimize a given objective function",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_761",
    "chunk_position": 761
  },
  {
    "id": "1901.09948v2",
    "text": ". RNNs are typically trained using an optimization procedure in which the parameters or weights are adjusted to minimize a given objective function. Efciently training large-scale RNNs is challenging due to a variety of extrinsic factors, such as noise and non-stationarity of the data, but also due to the inherent difculties of optimizing functions with long-range temporal and spatial dependencies. In SNNs and binary RNNs, these difculties are compounded by the nondifferentiable dynamics implied by the binary nature of their outputs",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_762",
    "chunk_position": 762
  },
  {
    "id": "1901.09948v2",
    "text": ". In SNNs and binary RNNs, these difculties are compounded by the nondifferentiable dynamics implied by the binary nature of their outputs. While a considerable body of work has successfully demonstrated training of two-layer SNNs 68 without hidden units, and networks with recurrent synaptic connections 9, 10, the ability to train deeper SNNs with hidden layers has remained a major obstacle. Because hidden units and depth are crucial to efciently solve many real-world problems, overcoming this obstacle is vital",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_763",
    "chunk_position": 763
  },
  {
    "id": "1901.09948v2",
    "text": ". Because hidden units and depth are crucial to efciently solve many real-world problems, overcoming this obstacle is vital. As network models grow larger and make their way into embedded and automotive applications, their power efciency becomes increasingly important. Simplied neural network architectures that can run natively and efciently on dedicated hardware are now being devised. This includes, for instance, networks of binary neurons or neuromorphic hardware that emulate the dynamics of SNNs 11",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_764",
    "chunk_position": 764
  },
  {
    "id": "1901.09948v2",
    "text": ". This includes, for instance, networks of binary neurons or neuromorphic hardware that emulate the dynamics of SNNs 11. Both types of networks dispense with energetically costly oating-point multiplications, making them particularly advantageous for low-power applications compared to neural networks executed on conventional hardware. These new hardware developments have created an imminent need for tools and strategies enabling efcient inference and learning in SNNs and binary RNNs",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_765",
    "chunk_position": 765
  },
  {
    "id": "1901.09948v2",
    "text": ". In this article, we discuss and address the inherent difculties in training SNNs with hidden layers, and introduce various strategies and approximations used to successfully implement them. II. U NDERSTANDING SNN S AS RNN S We start by formally mapping SNNs to RNNs. Formulating SNNs as RNNs will allow us to directly transfer and apply existing training methods for RNNs and will serve as the conceptual framework for the rest of this article. Before we proceed, one word on terminology",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_766",
    "chunk_position": 766
  },
  {
    "id": "1901.09948v2",
    "text": ". Before we proceed, one word on terminology. We use the term RNNs in its widest sense to refer to networks whose state evolves in time according to a set of recurrent dynamical . Such dynamical recurrence can be due to the explicit presence of recurrent synaptic connections between neurons in the network. This is the common understanding of what a RNN is. But 3 importantly, dynamical recurrence can also arise in the absence of recurrent connections. This happens, for instance, when stateful neuron or synapse models are used which have internal dynamics",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_767",
    "chunk_position": 767
  },
  {
    "id": "1901.09948v2",
    "text": ". This happens, for instance, when stateful neuron or synapse models are used which have internal dynamics. Because the networks state at a particular time step recurrently depends on its state in previous time steps, these dynamics are intrinsically recurrent. In this article, we use the term RNN for networks exhibiting either, or both types of recurrence. Moreover, we introduce the term recurrently connected neural network (RCNN) for the subset of networks with explicit recurrent synaptic connections",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_768",
    "chunk_position": 768
  },
  {
    "id": "1901.09948v2",
    "text": ". Moreover, we introduce the term recurrently connected neural network (RCNN) for the subset of networks with explicit recurrent synaptic connections. We will now show that both admit the same mathematical treatment despite the fact that their dynamical properties may be vastly different. To this end, we will rst introduce the LIF neuron model with current-based synapses which has wide use in computational neuroscience 12. Next, we will reformulate this model in discrete time and show its formal to a RNN with binary activation functions",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_769",
    "chunk_position": 769
  },
  {
    "id": "1901.09948v2",
    "text": ". Next, we will reformulate this model in discrete time and show its formal to a RNN with binary activation functions. Readers familiar with the LIF neuron model can skip the following steps up to (5). Box 1: Recurrent neural networks (RNNs) xnnnnn RNNs are networks of inter-connected units, or neurons in which the network state at any point in time anis a function of both external input xnand the networks state at the previous time point an1. One popular RNN structure arranges neurons in multiple layers where every layer is recurrently connected and also receives input from the previous layer",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_770",
    "chunk_position": 770
  },
  {
    "id": "1901.09948v2",
    "text": ". More precisely, the dynamics of a network with Llayers is given by: n (n)forl 1;:::;L n n1 n1forl 1;:::;L nxn where nis the state vector of the neurons at layer l,is an activation function, and andare the recurrent and feedforward weight matrices of layer l, respectively. External inputs xntypically arrive at the rst layer. Non-scalar quantities are typeset in bold face",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_771",
    "chunk_position": 771
  },
  {
    "id": "1901.09948v2",
    "text": ". External inputs xntypically arrive at the rst layer. Non-scalar quantities are typeset in bold face. A LIF neuron in layer lwith indexican formally be described in differential form as memd i d i Urest) R i (1) where Uis the membrane potential, Urestis the resting potential, mem is the membrane time constant,Ris the input resistance, and Iis the input current 12. (1) shows that Uiacts as a leaky integrator of the input current Ii. Neurons emit spikes to communicate their output to other neurons when their membrane voltage reaches the ring threshold",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_772",
    "chunk_position": 772
  },
  {
    "id": "1901.09948v2",
    "text": ". Neurons emit spikes to communicate their output to other neurons when their membrane voltage reaches the ring threshold . After each spike, the membrane voltage Uiis reset to the resting potential Ures. Due to this reset, 4 (1) only describes the subthreshold dynamics of a LIF neuron, i.e.the dynamics in absence of spiking output of the neuron. (a) Input neurons Output neuron (b) 01U 0I 0 0.4 0.8 Time (ms) . 1: Example LIF neuron dynamics. (a) Schematic of network setup. Four input neurons connect to one postsynaptic neuron. (b) Input and output activity over time",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_773",
    "chunk_position": 773
  },
  {
    "id": "1901.09948v2",
    "text": ". (a) Schematic of network setup. Four input neurons connect to one postsynaptic neuron. (b) Input and output activity over time. Bottom panel: Raster plot showing the activity of the four input neurons. Middle panel: The synaptic current I. Top panel: The membrane potential Uof the output neuron as a function of time. Output spikes are shown as points at the top. During the rst 0.4 s the dynamics are strictly sub-threshold and individual postsynaptic potentials (PSPs) are clearly discernible",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_774",
    "chunk_position": 774
  },
  {
    "id": "1901.09948v2",
    "text": ". During the rst 0.4 s the dynamics are strictly sub-threshold and individual postsynaptic potentials (PSPs) are clearly discernible. Only when multiple PSPs start to sum up, the neuronal ring threshold (dashed) is reached and output spikes are generated. In SNNs, the input current is typically generated by synaptic currents triggered by the arrival of presynaptic spikes . When working with differential , it is convenient to denote a spike train as a sum of Dirac delta functions P s2 wheresruns over the ring times jof neuronjin layerl",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_775",
    "chunk_position": 775
  },
  {
    "id": "1901.09948v2",
    "text": ". Synaptic currents follow specic temporal dynamics themselves. A common rst-order approximation is to model their time course as an exponentially decaying current following each presynaptic spike. Moreover, we assume that synaptic currents sum linearly. The dynamics of these operations are given by d i dt synz exp. decay X j ij z feedforward X j ij z recurren where the sum runs over all presynaptic neurons jand ijare the corresponding afferent weights from the layer below. Further, the ijcorrespond to explicit recurrent connections within each layer",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_776",
    "chunk_position": 776
  },
  {
    "id": "1901.09948v2",
    "text": ". Further, the ijcorrespond to explicit recurrent connections within each layer. Because of this property we can simulate a single LIF neuron with two linear differential whose initial conditions change instantaneously whenever a spike occurs",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_777",
    "chunk_position": 777
  },
  {
    "id": "1901.09948v2",
    "text": ". Through this property, we can incorporate the reset term in (1) through an extra term that instantaneously decreases the membrane potential by the amount (Urest)whenever the neuron emits a spike: d i dt1 mem ( i Urest) R i (Urest) (3) 5 It is customary to approximate the solutions of (2) and (3) numerically in discrete time and to express the output spike train inof neuroniin layerlat time step nas a nonlinear function of the membrane voltage i in)where denotes the Heaviside step function and corresponds to the ring threshold. Without loss of generality, we set Urest 0,R 1, and 1",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_778",
    "chunk_position": 778
  },
  {
    "id": "1901.09948v2",
    "text": ". Without loss of generality, we set Urest 0,R 1, and 1. When using a small simulation time step t0, (2) is well approximated by in 1 in X j ij jn X j ij jn (4) with the decay strength exp t syn . Note that 0 1for nite and positive syn. Moreover, jn2f0;1g. We usento denote the time step to emphasize the discrete dynamics. We can now express (3) as in 1 in in in (5) with exp t mem . (4) and (5) characterize the dynamics of a RNN. Specically, the state of neuron i is given by the instantaneous synaptic currents Iiand the membrane voltage U",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_779",
    "chunk_position": 779
  },
  {
    "id": "1901.09948v2",
    "text": ". Specically, the state of neuron i is given by the instantaneous synaptic currents Iiand the membrane voltage U. The computations necessary to update the cell state can be unrolled in time as is best illustrated by the computational graph ( 2). 0000 1111 2222 1 1 . 2: Illustration of the computational graph of a SNN in discrete time. Time steps ow from left to right. Input spikes are fed into the network from the bottom and propagate upwards to higher layers. The synaptic currents Iare decayed by in each time step and fed into the membrane potentials U",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_780",
    "chunk_position": 780
  },
  {
    "id": "1901.09948v2",
    "text": ". The synaptic currents Iare decayed by in each time step and fed into the membrane potentials U. The Uare similarly decaying over time as characterized by . Spike trains Sare generated by applying a threshold nonlinearity to the membrane potentials Uin each time step. Spikes causally affect the network state (red connections). First, each spike causes the membrane potential of the neuron that emits the spike to be reset. Second, each spike may be communicated to the same neuronal population via recurrent connections",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_781",
    "chunk_position": 781
  },
  {
    "id": "1901.09948v2",
    "text": ". Second, each spike may be communicated to the same neuronal population via recurrent connections . Finally, it may also be communicated via to another downstream network layer or, alternatively, a readout layer on which a cost function is dened. We have now seen that SNNs constitute a special case of RNNs. However, so far we have not explained how their parameters are set to implement a specic computational function. This is the focus of the rest of this article, in which we present a variety of learning that 6 systematically change the parameters towards implementing specic functionalities",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_782",
    "chunk_position": 782
  },
  {
    "id": "1901.09948v2",
    "text": ". III. M ETHODS FOR TRAINING RNN S Powerful machine learning methods are able to train RNNs for a variety of tasks ranging from time series prediction, to language translation, to automatic speech recognition 1. In the following, we discuss the most common methods before analyzing their applicability to SNNs. There are several stereotypical ingredients that dene the training process. The rst ingredient is a cost or loss function which is minimized when the networks response corresponds to the desired behavior",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_783",
    "chunk_position": 783
  },
  {
    "id": "1901.09948v2",
    "text": ". The rst ingredient is a cost or loss function which is minimized when the networks response corresponds to the desired behavior. In time series prediction, for example, this loss could be the squared difference between the predicted and the true value. The second ingredient is a mechanism that updates the networks weights to minimize the loss. One of the simplest and most powerful mechanisms to achieve this is to perform gradient descent on the loss function",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_784",
    "chunk_position": 784
  },
  {
    "id": "1901.09948v2",
    "text": ". One of the simplest and most powerful mechanisms to achieve this is to perform gradient descent on the loss function. In network architectures with hidden units (i.e.units whose activity affect the loss indirectly through other units) the parameter updates contain terms relating to the activity and weights of the downstream units they project to. Gradient-descent learning solves this credit assignment problem by providing explicit expressions for these updates through the chain rule of derivatives",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_785",
    "chunk_position": 785
  },
  {
    "id": "1901.09948v2",
    "text": ". As we will now see, the learning of hidden unit parameters depends on an efcient method to compute these gradients. When discussing these methods, we distinguish between solving the spatial credit assignment problem which affects multi-layer perceptrons (MLPs) and RNNs in the same way and the temporal credit assignment problem which only occurs in RNNs. We now discuss common which provide both types of credit assignment. A. Spatial credit assignment To train MLPs, credit or blame needs to be assigned spatially across layers and their respective units",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_786",
    "chunk_position": 786
  },
  {
    "id": "1901.09948v2",
    "text": ". A. Spatial credit assignment To train MLPs, credit or blame needs to be assigned spatially across layers and their respective units. This spatial credit assignment problem is solved most commonly by the backpropagation (BP) of error algorithm (Box. 2). In its simplest form, this errors backwards from the output of the network to upstream neurons. Using BP to adjust hidden layer weights ensures that the weight update will reduce the cost function for the current training example, provided the learning rate is small enough",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_787",
    "chunk_position": 787
  },
  {
    "id": "1901.09948v2",
    "text": ". While this theoretical guarantee is desirable, it comes at the cost of certain communication requirements namely that gradients have to be communicated back through the network and increased memory requirements as the neuron states need to be kept in memory until the errors become available. 7 Box 2: The Gradient Backpropagation Rule for Neural Networks The task of learning is to minimize a cost function Lover the entire dataset",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_788",
    "chunk_position": 788
  },
  {
    "id": "1901.09948v2",
    "text": ". 7 Box 2: The Gradient Backpropagation Rule for Neural Networks The task of learning is to minimize a cost function Lover the entire dataset. In a neural network, this can be achieved by gradient descent, which modies the network parameters Win the direction opposite to the gradient: Wij Wij Wij;where Wij L Wij L yiyi aiai Wij withai P j Wijxjthe total input to the neuron, yiis the output of neuron i, anda small learning rate. The rst term is the error of neuron iand the second term reects the sensitivity of the neuron output to changes in the parameter",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_789",
    "chunk_position": 789
  },
  {
    "id": "1901.09948v2",
    "text": ". The rst term is the error of neuron iand the second term reects the sensitivity of the neuron output to changes in the parameter. In multilayer networks, gradient descent is expressed as the BP of the errors starting from the prediction (output) layer to the inputs. Using superscripts l 0;:::;L to denote the layer ( 0is input,Lis output): ij i j;wher i0 i X k W;(l) ik; (6) where0is the derivative of the activation function, and (L) i L iis the error of output neuron iand ixiandindicates the transpose",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_790",
    "chunk_position": 790
  },
  {
    "id": "1901.09948v2",
    "text": ". x000 x111 x222 Unrolled RNNThis update rule is ubiquitous in deep learning and known as the gradient BP . Learning is typically carried out in forward passes (evaluation of the neural network activities) and backward passes (evaluation of s). The same rule can be applied to RNNs. In this case the recurrence is unrolled meaning that an auxiliary network is created by making copies of the network for each time step",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_791",
    "chunk_position": 791
  },
  {
    "id": "1901.09948v2",
    "text": ". In this case the recurrence is unrolled meaning that an auxiliary network is created by making copies of the network for each time step. The unrolled network is simply a deep network with shared feedforward weights and recurrent weights , on which the standard BP applies: ij ij Ln t X m0(l) im jm;and ij ij Ln t X m1(l) is jm1 (l) in 0 in X kn W;l ik X kn 1V;l ik! ; (7) Applying BP to an unrolled network is referred to as backpropagation through time (BPTT). B. Temporal credit assignment When training RNNs, we also have to consider temporal interdependencies of network activity",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_792",
    "chunk_position": 792
  },
  {
    "id": "1901.09948v2",
    "text": ". B. Temporal credit assignment When training RNNs, we also have to consider temporal interdependencies of network activity. This requires solving the temporal credit assignment problem (. 2). There are two common methods to achieve this: 1) The backward method: This method applies the same strategies as with spatial credit assignment by unrolling the network in time (Box. 2). Backpropagation through time (BPTT) solves the temporal credit assignment problem by back-propagating errors through 8 the unrolled network. This method works backward through time after completing a forward pass",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_793",
    "chunk_position": 793
  },
  {
    "id": "1901.09948v2",
    "text": ". This method works backward through time after completing a forward pass. The use of standard BP on the unrolled network directly enables the use of autodifferentiation tools offered in modern machine learning toolkits 3, 13. 2) The forward method: In some situations, it is benecial to propagate all necessary information for gradient computation forward in time 14. This formulation is achieved by computing the gradient of a cost function Lnand maintaining the recursive structure of the RNN",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_794",
    "chunk_position": 794
  },
  {
    "id": "1901.09948v2",
    "text": ". This formulation is achieved by computing the gradient of a cost function Lnand maintaining the recursive structure of the RNN. For example, the forward gradient of the feed-forward weight Wbecomes: Wm ij Ln Wm ij X k Ln kn PL;m ijkn;with ijkn Wm ij kn ijkn 0( kn)0 X j0 ij0 ijj0n1 X j0 ij0 ijj0n1 lm in11 A: (8) Gradients with respect to recurrent weights ijcan be computed in a similar fashion 14. The backward optimization method is generally more efcient in terms of computation, but requires maintaining all the inputs and activations for each time step",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_795",
    "chunk_position": 795
  },
  {
    "id": "1901.09948v2",
    "text": ". Thus, its space complexity for each layer is , where Nis the number of neurons per layer and Tis the number of time steps. On the other hand, the forward method requires maintaining variables ijk, resulting in a space complexity per layer. While is not a favorable scaling compared to for large N, simplications of the computational graph can reduce the memory complexity of the forward method to 2, 15, or even 4. These simplications also reduce the computational complexity, rendering the scaling of forward comparable or better than BPTT",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_796",
    "chunk_position": 796
  },
  {
    "id": "1901.09948v2",
    "text": ". These simplications also reduce the computational complexity, rendering the scaling of forward comparable or better than BPTT. Such simplications are at the core of several successful approaches which we will describe in Sec. V. Furthermore, the forward method is more appealing from a biological point of view, since the learning rule can be made consistent with synaptic plasticity in the brain and three-factor rules, as discussed in -B. In summary, efcient to train RNNs exist. We will now focus on training SNNs. IV",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_797",
    "chunk_position": 797
  },
  {
    "id": "1901.09948v2",
    "text": ". In summary, efcient to train RNNs exist. We will now focus on training SNNs. IV. C REDIT ASSIGNMENT WITH SPIKING NEURONS : CHALLENGES AND SOLUTIONS So far we have discussed common solutions to training RNNs. Before these solutions can be applied to SNNs, however, two key challenges need to be overcome. The rst challenge concerns the non-differentiability of the spiking nonlinearity. (7) and (8) 9 reveal that the expressions for both the forward and the backward learning methods contain the derivative of the neural activation function 0 i ias a multiplicative factor",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_798",
    "chunk_position": 798
  },
  {
    "id": "1901.09948v2",
    "text": ". For a spiking neuron, however, we have ) (), whose derivative is zero everywhere except at U, where it is ill dened (. 3). This all-or-nothing behavior of the binary spiking nonlinearity stops gradients from owing and makes LIF neurons unsuitable for gradient based optimization. The same issue occurs in binary neurons and some of the solutions proposed here are inspired by the methods rst developed in binary networks 16, 17. 01 0 0.5 1 Derivative Memb rane p otential UDeriv. of step Piece-wise lin. Sup er Spik e SLA YER . 3: Commonly used surrogate derivatives",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_799",
    "chunk_position": 799
  },
  {
    "id": "1901.09948v2",
    "text": ". 01 0 0.5 1 Derivative Memb rane p otential UDeriv. of step Piece-wise lin. Sup er Spik e SLA YER . 3: Commonly used surrogate derivatives . The step function has zero derivative (violet) everywhere except at 0 where it is ill dened. Examples of surrogate derivatives which have been used to train SNNs. Green: Piece-wise linear 3, 18, 19. Blue: Derivative of a fast sigmoid 2. Yellow: Exponential 13. Note that the axes have been rescaled on a per-function-basis for illustration purposes. The second challenge concerns the implementation of the optimization",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_800",
    "chunk_position": 800
  },
  {
    "id": "1901.09948v2",
    "text": ". The second challenge concerns the implementation of the optimization . Standard BP can be expensive in terms of computation, memory and communication, and may be poorly suited to the constraints dictated by the hardware that implements it (e.g. a computer, a brain, or a neuromorphic device). Processing in dedicated neuromorphic hardware and, more generally, non-von Neumann computers may have specic locality requirements (Box. 3) that can complicate matters. On such hardware, the forward approach may therefore be preferable",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_801",
    "chunk_position": 801
  },
  {
    "id": "1901.09948v2",
    "text": ". 3) that can complicate matters. On such hardware, the forward approach may therefore be preferable. In practice, however, the scaling of both methods ( and) has proven unsuitable for many SNN models. For example, the size of the convolutional SNN models trained with BPTT for gesture classication 20 are GPU memory bounded. Additional simplifying approximations that reduce the complexity of the forward method will be discussed below. In the following , we describe approximate solutions to these challenges that make learning in SNNs more tractable",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_802",
    "chunk_position": 802
  },
  {
    "id": "1901.09948v2",
    "text": ". In the following , we describe approximate solutions to these challenges that make learning in SNNs more tractable. To overcome the rst challenge in training SNNs, which is concerned with the discontinuous spiking nonlinearity, several approaches have been devised with varying degrees of success",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_803",
    "chunk_position": 803
  },
  {
    "id": "1901.09948v2",
    "text": ". The most common approaches can be coarsely classied into the following categories: i) resorting to entirely biologically inspired local learning rules for the hidden units, ii) translating conventionally trained rate-based neural networks to SNNs, iii) smoothing the network model to be continuously differentiable, or iv) dening a surrogate gradient (SG) as a continuous relaxation 10 of the real gradients. Approaches pertaining biologically motivated local learning rules (i) and network translation (ii) have been reviewed extensively elsewhere 5, 21",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_804",
    "chunk_position": 804
  },
  {
    "id": "1901.09948v2",
    "text": ". Approaches pertaining biologically motivated local learning rules (i) and network translation (ii) have been reviewed extensively elsewhere 5, 21. In this article, we therefore focus on the latter two supervised approaches (iii iv) which we will refer to as the smoothed and the SG approach. First, we review existing literature on common smoothing approaches before turning to an in-depth discussion of how to build functional SNNs using SG methods. A",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_805",
    "chunk_position": 805
  },
  {
    "id": "1901.09948v2",
    "text": ". A. Smoothed spiking neural networks The dening characteristic of smoothed SNNs is that their formulation ensures well-behaved gradients which are directly suitable for optimization. Smooth models can be further categorized into (1) soft nonlinearity models, (2) probabilistic models, for which gradients are only well dened in expectation, or models which either rely entirely on (3) rate or (4) single-spike temporal",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_806",
    "chunk_position": 806
  },
  {
    "id": "1901.09948v2",
    "text": ". 1) Gradients in soft nonlinearity models: This approach can in principle be applied directly to all spiking neuron models which explicitly include a smooth spike generating process. This includes, for instance, the Hodgkin-Huxley, Morris-Lecar, and Fitz Hugh-Nagumo models 12. In practice this approach has only been applied successfully by Huh and Sejnowski 22 using an augmented integrate-and-re model in which the binary spiking nonlinearity was replaced by a continuous-valued gating function",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_807",
    "chunk_position": 807
  },
  {
    "id": "1901.09948v2",
    "text": ". The resulting network constitutes a RCNN which can be optimized using standard methods of BPTT or real-time recurrent learning (RTRL). Importantly, the soft threshold models compromise on one of the key features of SNN, namely the binary spike propagation. 2) Gradients in probabilistic models: Another example for smooth models are binary probabilistic models. In simple terms, stochasticity effectively smooths out the discontinuous binary nonlinearity which makes it possible to dene a gradient on expectation values",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_808",
    "chunk_position": 808
  },
  {
    "id": "1901.09948v2",
    "text": ". Binary probabilistic models have been objects of extensive study in the machine learning literature mainly in the context of (restricted) Boltzmann machines 23. Similarly, the propagation of gradients has been studied for binary stochastic models 17. Probabilistic models are practically useful because the log-likelihood of a spike train is a smooth quantity which can be optimized using gradient descent 24. Although this insight was rst discovered in networks without hidden units, the same ideas were later extended to multi-layer networks 25. Similarly, Guerguiev et al",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_809",
    "chunk_position": 809
  },
  {
    "id": "1901.09948v2",
    "text": ". Similarly, Guerguiev et al. 26 used probabilistic neurons to study biologically 11 plausible ways of propagating error or target signals using segregated dendrites (see -A). In a similar vein, variational learning approaches were shown to be capable of learning useful hidden layer representations in SNNs 2729. However, the injected noise necessary to smooth out the effect of binary nonlinearities often poses a challenge for optimization 28. How noise, which is found ubiquitously in neurobiology, inuences learning in the brain, remains an open question",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_810",
    "chunk_position": 810
  },
  {
    "id": "1901.09948v2",
    "text": ". How noise, which is found ubiquitously in neurobiology, inuences learning in the brain, remains an open question. 3) Gradients in rate-coding networks: Another common approach to obtain gradients in SNNs is to assume a rate-based coding scheme. The main idea is that spike rate is the underlying information-carrying quantity. For many plausible neuron models, the supra-threshold ring rate depends smoothly on the neuron input. This input-output dependence is captured by the so-called f-I curve of a neuron",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_811",
    "chunk_position": 811
  },
  {
    "id": "1901.09948v2",
    "text": ". This input-output dependence is captured by the so-called f-I curve of a neuron. In such cases, the derivative of the f-I curves is suitable for gradient-based optimization. There are several examples of this approach. For instance, Hunsberger and Eliasmith 30 as well as Neftci et al. 31 used an effectively rateinput scheme to demonstrate competitive performance on standard machine learning benchmarks such as CIFAR10 and MNIST. Similarly Lee et al. 32 demonstrated deep learning in SNNs by dening partial derivatives on low-pass ltered spike trains",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_812",
    "chunk_position": 812
  },
  {
    "id": "1901.09948v2",
    "text": ". Similarly Lee et al. 32 demonstrated deep learning in SNNs by dening partial derivatives on low-pass ltered spike trains. Rate-based approaches can offer good performance, but they may be inefcient. On the one hand, precise estimation of ring rates requires averaging over a number of spikes. Such averaging requires either relatively high ring rates or long averaging times because several repeats are needed to average out discretization noise. This problem can be partially addressed by spatial averaging over large populations of spiking neurons",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_813",
    "chunk_position": 813
  },
  {
    "id": "1901.09948v2",
    "text": ". This problem can be partially addressed by spatial averaging over large populations of spiking neurons. However, this may require the use of larger neuron numbers. Finally, the distinction between rate-coding and probabilistic networks can be blurry since many probabilistic network implementations use rate-coding at the output level. Both types of models are differentiable, but for different reasons: Probabilistic models are based on a ring probability densities 24. Importantly, the ring probability of a neuron is a continuous function",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_814",
    "chunk_position": 814
  },
  {
    "id": "1901.09948v2",
    "text": ". Importantly, the ring probability of a neuron is a continuous function. Although measuring probability changes requires trial averaging over several samples, it is the underlying continuity of the probability density which formally allows to dene differential improvements and thus to derive gradients. By exploiting this feature, probabilistic models have been used to learn precise output spike timing 24, 25. In contrast, deterministic networks always emit a xed integer number of spikes for a given input",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_815",
    "chunk_position": 815
  },
  {
    "id": "1901.09948v2",
    "text": ". In contrast, deterministic networks always emit a xed integer number of spikes for a given input. To nevertheless get at a notion of differential 12 improvement, one may consider the number of spikes over a given time interval within single trials. When averaging over sufciently large intervals, the resulting ring rates behave as a quasi continuous function of the input current. This smooth input output relationship is captured by the neuronal f-I curve which can be used for optimization 30, 31. Operating at the level of rates, however, comes at the expense temporal precision",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_816",
    "chunk_position": 816
  },
  {
    "id": "1901.09948v2",
    "text": ". Operating at the level of rates, however, comes at the expense temporal precision. 4) Gradients in single-spike-timing-coding networks: In an effort to optimize SNNs without potentially harmful noise injection and without reverting to a rate-based coding scheme, several studies have considered the outputs of neurons in SNNs to be a set of ring times. In such a temporal coding setting, individual spikes could carry signicantly more information than ratebased schemes that only consider the total number of spikes in an interval",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_817",
    "chunk_position": 817
  },
  {
    "id": "1901.09948v2",
    "text": ". The idea behind training temporal coding networks was pioneered in Spike Prop 33. In this work the analytic expressions of ring times for hidden units were linearized, allowing to analytically compute approximate hidden layer gradients. More recently, a similar approach without the need for linearization was used in 34 where the author computed the spike timing gradients explicitly for non-leaky integrate-and-re neurons. Intriguingly, the work showed competitive performance on conventional networks and benchmarks",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_818",
    "chunk_position": 818
  },
  {
    "id": "1901.09948v2",
    "text": ". Intriguingly, the work showed competitive performance on conventional networks and benchmarks. Although the spike timing formulation does in some cases yield well-dened gradients, it may suffer from certain limitations. For instance, the formulation of Spike Prop 33 required each hidden unit to emit exactly one spike per trial, because it is impossible to dene ring time for quiescent units. Ultimately, such a non-quiescence requirement could be at conict with powerefciency for which it is conceivably benecial to, for instance, only have a subset of neurons active for any given task. B",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_819",
    "chunk_position": 819
  },
  {
    "id": "1901.09948v2",
    "text": ". B. Surrogate gradients SG methods provide an alternative approach to overcoming the difculties associated with the discontinuous nonlinearity. Moreover, they hold opportunities to reduce the potentially high complexity associated with training SNNs. Their dening characteristic is that instead of changing the model denition as in the smoothed approaches, a SG is introduced. In the following we make two distinctions. We rst consider SGs which constitute a continuous relaxation of the non-smooth spiking nonlinearity for purposes of numerical optimization (. 4)",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_820",
    "chunk_position": 820
  },
  {
    "id": "1901.09948v2",
    "text": ". We rst consider SGs which constitute a continuous relaxation of the non-smooth spiking nonlinearity for purposes of numerical optimization (. 4). Such SGs do not explicitly change the optimization and can be used, for instance, in combination with BPTT. Further, we also consider SGs with more profound changes that 13 explicitly affect locality of the underlying optimization themselves to improve the computational andor memory access overhead of the learning process. One example of this approach that we will discuss involves replacing the global loss by a number of local loss functions",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_821",
    "chunk_position": 821
  },
  {
    "id": "1901.09948v2",
    "text": ". One example of this approach that we will discuss involves replacing the global loss by a number of local loss functions. Finally, the use of SGs allows to efciently train SNNs end-to-end without the need to specify which coding scheme is to be used in the hidden layers. Loss True Surrogate initial final interpolation positio (a) (b) . 4: Example of SG for a SNN classier. (a) Value of the loss function (gray) of an SNN classier along an interpolation path over the hidden layer parameters",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_822",
    "chunk_position": 822
  },
  {
    "id": "1901.09948v2",
    "text": ". (a) Value of the loss function (gray) of an SNN classier along an interpolation path over the hidden layer parameters . Specically, we linearly interpolated between the random initial and nal (post-optimization) weight matrices of the hidden layer inputs (network details: 2 input, 2 hidden, and 2 output units trained on a binary classication task). Note that the loss function (gray) displays characteristic plateaus with zero gradient which are detrimental for numerical optimization. ( b) Norm of hidden layer (surrogate) gradients in arbitrary units along the interpolation path",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_823",
    "chunk_position": 823
  },
  {
    "id": "1901.09948v2",
    "text": ". ( b) Norm of hidden layer (surrogate) gradients in arbitrary units along the interpolation path. To perform numerical optimization in this network we constructed a SG (violet) which, in contrast to the true gradient (gray), is non-zero. Note that we obtained the true gradient via the nite differences method which in itself is an approximation. Importantly, the SG approximates the true gradient, but retains favorable properties for optimization, i.e. continuity and niteness",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_824",
    "chunk_position": 824
  },
  {
    "id": "1901.09948v2",
    "text": ". Importantly, the SG approximates the true gradient, but retains favorable properties for optimization, i.e. continuity and niteness. The SG can be thought of as the gradient of a virtual surrogate loss function (violet curve in (a); obtained by numerical integration of the SG and scaled to match loss at initial and nal point). This surrogate loss remains virtual because it is generally not computed explicitly. In practice, suitable SGs are obtained directly from the gradients of the original network through sensible approximations",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_825",
    "chunk_position": 825
  },
  {
    "id": "1901.09948v2",
    "text": ". In practice, suitable SGs are obtained directly from the gradients of the original network through sensible approximations. This is a key difference with respect to some other approaches 22 in which the entire network is replaced explicitly by a surrogate network on which gradient descent can be performed using its true gradients. Like standard gradient-descent, SG learning can deal with the spatial and temporal credit assignment problem by either BPTT or by forward methods, e.g. through the use of eligibility traces (see -B for details)",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_826",
    "chunk_position": 826
  },
  {
    "id": "1901.09948v2",
    "text": ". through the use of eligibility traces (see -B for details). Alternatively, additional approximations can be introduced which may offer advantages specically for hardware implementations. In the following, we briey review existing work relying on SG methods before turning to a more in-depth treatment of the underlying principles and capabilities. 14 1) Surrogate derivatives for spiking nonlinearity: A set of works have used SG to specically overcome the challenge of the discontinuous spiking nonlinearity",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_827",
    "chunk_position": 827
  },
  {
    "id": "1901.09948v2",
    "text": ". In these works, typically a standard as BPTT is used with one minor modication: within the occurrence of the derivative of the spiking nonlinearity is replaced by the derivative of a smooth function. Implementing these approaches is straight-forward in most auto-differentiation-enabled machine learning toolkits. One of the rst uses of such a SG is described in Bohte 19 where the derivative of a spiking neuron non-linearity was approximated by the derivative of a truncated quadratic function, thus resulting in a rectifying linear unit (Re LU) as surrogate derivative (. 3)",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_828",
    "chunk_position": 828
  },
  {
    "id": "1901.09948v2",
    "text": ". 3). This is similar in avor to the solution proposed to optimize binary neural networks 16. The same idea underlies the training of large-scale convolutional networks with binary activations on classication problems using neuromorphic hardware 18. Zenke and Ganguli 2 proposed a three factor online learning rule using a fast sigmoid to construct a SG. Shrestha and Orchard 13 used an exponential function and reported competitive performance on a range of neuromorphic benchmark problems. Additionally, OConnor et al. 35 described a spike-based encoding method inspired by Sigma-Delta modulators",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_829",
    "chunk_position": 829
  },
  {
    "id": "1901.09948v2",
    "text": ". Additionally, OConnor et al. 35 described a spike-based encoding method inspired by Sigma-Delta modulators. They used their method to approximately encode both the activations and the errors in standard feedforward articial neural networks (ANNs), and apply standard backpropagation on these sparse approximate encodings. Surrogate derivatives have also been used to train spiking RCNNs where dynamical recurrence arises due to the use of LIF neurons as well as due to recurrent synaptic connections. Recently, Bellec et al",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_830",
    "chunk_position": 830
  },
  {
    "id": "1901.09948v2",
    "text": ". Recently, Bellec et al. 3 successfully trained RCNNs with slow temporal neuronal dynamics using a piecewise linear surrogate derivative. Encouragingly, the authors found that such networks can perform on par with conventional long short-term memory (LSTM) networks. Similarly, Wo zniak et al. 36 reported competitive performance on a series of temporal benchmark datasets. In summary, a plethora of studies have constructed SG using different nonlinearities and trained a diversity of SNN architectures. These nonlinearties, however, have a common underlying theme",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_831",
    "chunk_position": 831
  },
  {
    "id": "1901.09948v2",
    "text": ". These nonlinearties, however, have a common underlying theme. All functions are nonlinear and monotonically increasing towards the ring threshold (. 3). While a more systematic comparison of different surrogate nonlinearities is still pending, overall the diversity found in the present literature suggests that the success of the method is not crucially dependent on the details of the surrogate used to approximate the derivative",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_832",
    "chunk_position": 832
  },
  {
    "id": "1901.09948v2",
    "text": ". 2) Surrogate gradients affecting locality of the update rules: The majority of studies discussed in the previous a surrogate nonlinearity to prevent gradients from vanishing 15 L (a) BP L (b) FA L (c) DFA (d) Local Errors : Strategies for relaxing gradient BP requirements . Dashed lines indicate xed, random connections. (a) BP propagates errors through each layer using the transpose of the forward weights by alternating forward and backward passes. (b) Feedback Alignment 37 replaces the transposed matrix with a random one",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_833",
    "chunk_position": 833
  },
  {
    "id": "1901.09948v2",
    "text": ". (b) Feedback Alignment 37 replaces the transposed matrix with a random one. (c) Direct Feedback Alignment 38 directly propagates the errors from the top layer to the hidden layers. (d) Local errors 29 uses a xed, random, auxiliary cost function at each layer. (or exploding), but by relying on methods such as BPTT, they did not explicitly affect the structural properties of the learning rules",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_834",
    "chunk_position": 834
  },
  {
    "id": "1901.09948v2",
    "text": ". (or exploding), but by relying on methods such as BPTT, they did not explicitly affect the structural properties of the learning rules. There are, however, training approaches for SNNs which introduce more far-reaching modications which may completely alter the way error signals or target signals are propagated (or generated) within the network. Such approaches are typically used in conjunction with the aforementioned surrogate derivatives",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_835",
    "chunk_position": 835
  },
  {
    "id": "1901.09948v2",
    "text": ". Such approaches are typically used in conjunction with the aforementioned surrogate derivatives. There are two main motivations for such modications which are typically linked to physical constraints that make it impossible to implement the correct gradient descent algorithm. For instance, in neurobiology biophysical constraints make it impossible to implement BPTT without further approximations",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_836",
    "chunk_position": 836
  },
  {
    "id": "1901.09948v2",
    "text": ". For instance, in neurobiology biophysical constraints make it impossible to implement BPTT without further approximations. Studies interested in how the brain could solve the credit assignment problem focus on how simplied local could achieve similar performance while adhering to the constraints of the underlying biological wetware (Box. 3). Similarly, neuromorphic hardware may pose certain constraints with regard to memory or communications which impede the use of BPTT and call for simpler and often more local methods for training on such devices",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_837",
    "chunk_position": 837
  },
  {
    "id": "1901.09948v2",
    "text": ". As training SNNs using SGs advances to deeper architectures, it is foreseeable that additional problems, similar to the ones encountered in ANNs, will arise. For instance, several approaches currently rely on SGs derived from sigmoidal activation functions (. 3). However, the use of sigmoidal activation functions is implicated with vanishing gradient problems. Another set of challenges which may well need tackling in the future could be linked to the bias which SGs introduce into the learning dynamics",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_838",
    "chunk_position": 838
  },
  {
    "id": "1901.09948v2",
    "text": ". Another set of challenges which may well need tackling in the future could be linked to the bias which SGs introduce into the learning dynamics. In the following Applications Section, we will review a selection of promising SG approaches which introduce far larger deviations from the true gradients and still allow for learning at a greatly reduced complexity and computational cost. 16 V",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_839",
    "chunk_position": 839
  },
  {
    "id": "1901.09948v2",
    "text": ". 16 V. A PPLICATIONS In this section, we present a selection of illustrative applications of smooth or SGs to SNNs which exploit both the internal continuous-time dynamics of the neurons and their event-driven nature. The latter allows a network to remain quiescent until incoming spikes trigger activity. A. Feedback alignment and random error backpropagation One family of that relaxes some of the requirements of BP are feedback alignment or, more generally, random BP 3739",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_840",
    "chunk_position": 840
  },
  {
    "id": "1901.09948v2",
    "text": ". These are approximations to the gradient BP rule that side-step the non-locality problem by replacing weights in the BP rule with random ones (. 5b):(l) i0 i P k ki;where is a xed, random matrix with the same dimensions as W. The replacement of W;(l)with a random matrix breaks the dependency of the backward phase on , enabling the rule to be more local. One common variation is to replace the entire backward propagation by a random propagation of the errors to each layer (. 5c) 38: (l) i0 i P k ki;where is a xed, random matrix with appropriate dimensions",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_841",
    "chunk_position": 841
  },
  {
    "id": "1901.09948v2",
    "text": ". 5c) 38: (l) i0 i P k ki;where is a xed, random matrix with appropriate dimensions. Random BP approaches lead to remarkably little loss in classication performance on some benchmark tasks. Although a general theoretical understanding of random BP is still a subject of intense research, simulation studies have shown that, during learning, the network adjusts its feedforward weights such that they partially align with the (random) feedback weights, thus permitting them to convey useful error information 37",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_842",
    "chunk_position": 842
  },
  {
    "id": "1901.09948v2",
    "text": ". Building on these ndings, an asynchronous spikedriven adaptation of random BP using local synaptic plasticity rules with the dynamics of spiking neurons was demonstrated in 31. To obtain the SGs, the authors approximated the derivative of the neural activation function using a symmetric function that is zero everywhere except in the vicinity of zero, where it is constant. The derivative of this function exists and is piecewise constant",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_843",
    "chunk_position": 843
  },
  {
    "id": "1901.09948v2",
    "text": ". The derivative of this function exists and is piecewise constant. Networks using this learning rule performed remarkably well, and were shown to operate continuously and asynchronously without the alternation between forward and backward passes that is necessary in BP. One important limitation with random BP applied to SNNs was that the temporal dynamics of the neurons and synapses was not taken into account in the gradients. The following rule, Super Spike solves this problem",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_844",
    "chunk_position": 844
  },
  {
    "id": "1901.09948v2",
    "text": ". The following rule, Super Spike solves this problem. 17 Box 3: Local models of computation Locality of computations is characterized by the set variables available to the physical processing elements, and depends on the computational substrate. To illustrate the concept of locality, we assume two neurons, A and B, and would like Neuron Ato implement a function on domain D dened as: DDloc Dnloc;where Dlocf WBA;S;Ug and Dnlocf S;UBg: Here,Srefers to the output of neuron BT seconds ago, UA,UBare the respective membrane potentials, and WBAis the synaptic weight from Bto A",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_845",
    "chunk_position": 845
  },
  {
    "id": "1901.09948v2",
    "text": ". Variables under Dlocare directly available to Neuron A and are thus local to it. On the other hand, variable Sis temporally non-local and UBis spatially non-local to neuron A. Non-local information can be transmitted through special structures, for example dedicated encoders and decoders for UBand a form of working memory (WM) for S. Although locality in a model of computation can make its use challenging, it enables massively parallel computations with dynamical interprocess communications. B",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_846",
    "chunk_position": 846
  },
  {
    "id": "1901.09948v2",
    "text": ". B. Supervised learning with local three factor learning rules Super Spike is a biologically plausible three factor learning rule. In contrast to many existing three factor rules which fall into the category of smoothed approaches 2429, Super Spike is a SG approach which combines several approximations to render it more biologically plausible 2. Although the underlying motivation of the study is geared toward a deeper understanding of learning in biological neural networks, the learning rule may prove interesting for hardware implementations because it does not rely on BPTT",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_847",
    "chunk_position": 847
  },
  {
    "id": "1901.09948v2",
    "text": ". Specically, the rule uses synaptic eligibility traces to solve the temporal credit assignment problem. We now provide a short account on why Super Spike can be seen as one of the forward-intime optimization procedures. Super Spike was derived for temporal supervised learning tasks in which a given output neuron learns to spike at predened times",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_848",
    "chunk_position": 848
  },
  {
    "id": "1901.09948v2",
    "text": ". Super Spike was derived for temporal supervised learning tasks in which a given output neuron learns to spike at predened times. To that end, Super Spike minimizes the van Rossum distance with kernel between a set of output spike train Sand their corresponding target spike trains S L1 2Zt 1ds1 2Zt 1((SS ))2ds1 2X )2(9) where the last approximation corresponds to transitioning to discrete time. To perform online gradient descent, we need to compute the gradients of Ln. Here we rst encounter the derivative Wij Skn",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_849",
    "chunk_position": 849
  },
  {
    "id": "1901.09948v2",
    "text": ". To perform online gradient descent, we need to compute the gradients of Ln. Here we rst encounter the derivative Wij Skn. Because the (discrete) convolution is a linear operator, this expression simplies to Skn Wij. In Super Spike is implemented as a dynamical system (see 2 for details)",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_850",
    "chunk_position": 850
  },
  {
    "id": "1901.09948v2",
    "text": ". In Super Spike is implemented as a dynamical system (see 2 for details). To 18 compute derivatives of the neurons output spiketrain of the form Sin Wijwe differentiate the network dynamics ( (4) and (5)) and obtain Skn 1 Wij 0(Ukn 1)Ukn 1 Wij (10) Ukn 1 Wij Ukn Wij Ikn Wij Skn Wi Ikn 1 Wij Ikn Wij Sjn (12) The above dene a dynamical system which, given the starting conditions Sk0 Uk0 Ik0 0 , can be simulated online and forward in time to produce all relevant derivatives. Crucially, to arrive at useful SGs, Super Spike makes two approximations",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_851",
    "chunk_position": 851
  },
  {
    "id": "1901.09948v2",
    "text": ". Crucially, to arrive at useful SGs, Super Spike makes two approximations. First, 0is replaced by a smooth surrogate derivative 0(Un)(cf. . 3). Second, the reset term with the negative sign in (11) is dropped, which empirically leads to better results. With these denitions in hand, the nal weight updates are given by Wijnein 0(Ukn)Ukn Wij (13) whereei. These weight updates depend only on local quantities (Box. 3). Above, we have considered a simple two-layer network (cf. . 2) without recurrent connections",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_852",
    "chunk_position": 852
  },
  {
    "id": "1901.09948v2",
    "text": ". 3). Above, we have considered a simple two-layer network (cf. . 2) without recurrent connections. If we were to apply the same strategy to compute updates in a RCNN or a network with an additional hidden layer, the would become more complicated and non-local. Super Spike applied to multi-layer networks sidesteps this issue by propagating error signals from the output layer directly to the hidden units as in random BP (cf. -A; . 5c; 3739)",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_853",
    "chunk_position": 853
  },
  {
    "id": "1901.09948v2",
    "text": ". -A; . 5c; 3739). Thus, Super Spike achieves temporal credit assignment by propagating all relevant quantities forward in time, while it relies on random BP to perform spatial credit assignment. While the work by Zenke and Ganguli 2 was centered around feed-forward networks, Bellec et al. 15 show that similar biologically plausible three factors rule can also be used to train RCNN efciently. C. Learning using local errors In practice, the performance of Super Spike does not scale favorably for large multilayer networks",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_854",
    "chunk_position": 854
  },
  {
    "id": "1901.09948v2",
    "text": ". C. Learning using local errors In practice, the performance of Super Spike does not scale favorably for large multilayer networks. The scalability of Super Spike can be improved by introducing local errors, as described here. 19 . 6: Deep Continuous Local Learning (DCLL) with spikes 4, applied to the event-based DVSGestures dataset. The feed-forward weights (green) of a three layer convolutional SNN are trained with SG using local errors generated using xed random projections to a local classier",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_855",
    "chunk_position": 855
  },
  {
    "id": "1901.09948v2",
    "text": ". Learning in DCLL scales linearly with the number of neurons thanks to local rate-based cost functions formed by spike-based basis functions. The circular arrows indicate recurrence due to the statefulness of the LIF dynamics (no recurrent synaptic connections were used here) and are not trained. This SNN outperforms BPTT methods 13, requiring fewer training iterations 4 compared to other approaches. Multi-layer neural networks are hierarchical feature extractors",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_856",
    "chunk_position": 856
  },
  {
    "id": "1901.09948v2",
    "text": ". Multi-layer neural networks are hierarchical feature extractors. Through successive linear projections and point-wise non-linearities, neurons become tuned (respond most strongly) to particular spatio-temporal features in the input. While the best features are those that take into account the subsequent processing stages and which are learned to minimize the nal error (as the features learned using BP do), high-quality features can also be obtained by more local methods. The non-local component of the weight update (Eq. (6)) is the error term (l) in",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_857",
    "chunk_position": 857
  },
  {
    "id": "1901.09948v2",
    "text": ". The non-local component of the weight update (Eq. (6)) is the error term (l) in. Instead of obtaining this error term through BP, we require that it be generated using information local to the layer. One way of achieving this is to dene a layer-wise loss (n)and use this local loss to obtain the errors. In such a local learning setting, the local errors (l)becomes: (l) in 0 ind d in(n)where(n)n;n)(14) with na pseudo-target for layer l, anda xed random matrix that projects the activity vector at layer lto a vector having the same dimension as the pseudo-target",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_858",
    "chunk_position": 858
  },
  {
    "id": "1901.09948v2",
    "text": ". In essence, this formulation assumes that an auxiliary random layer is attached to layer land the goal is to modify so as to minimize the discrepancy between the auxiliary random layers output and the pseudo-target. The simplest choice for the pseudo-target is to use the top-layer target. This forces each layer to learn a set of features that are able to match the top-layer target after 20 undergoing a xed random linear projection",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_859",
    "chunk_position": 859
  },
  {
    "id": "1901.09948v2",
    "text": ". This forces each layer to learn a set of features that are able to match the top-layer target after 20 undergoing a xed random linear projection. Each layer builds on the features learned by the layer below it, and we empirically observe that higher layers are able to learn higher-quality features that allow their random and xed auxiliary layers to better match the target 40. A related approach was explored with spiking neural networks 41, where separate networks provided high-dimensional temporal signals to improve learning",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_860",
    "chunk_position": 860
  },
  {
    "id": "1901.09948v2",
    "text": ". Local errors were recently used in SNNs in combination with the Super Spike (cf. -B) forward method to overcome the temporal credit assignment problem 4. As in Super Spike, the SNN model is simplied by using a feedforward structure, and omitting the refractory dynamics in the optimization. However, the cost function was dened to operate locally on the instantaneous rates of each layer",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_861",
    "chunk_position": 861
  },
  {
    "id": "1901.09948v2",
    "text": ". However, the cost function was dened to operate locally on the instantaneous rates of each layer. This simplication results in a forward method whose space complexity scales as (instead of for the forward method, for Super Spike, or for the backward method), while still making use of spiking neural dynamics. Thus the method constitutes a highly efcient synaptic plasticity rule for multi-layer SNNs",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_862",
    "chunk_position": 862
  },
  {
    "id": "1901.09948v2",
    "text": ". Thus the method constitutes a highly efcient synaptic plasticity rule for multi-layer SNNs. Furthermore, the simplications enable the use of existing automatic differentiation methods in machine learning frameworks to systematically derive synaptic plasticity rules from task-relevant cost functions and neural dynamics (see 4 and included tutorials), making DCLL easy to implement. This approach was benchmarked on the DVS Gestures dataset (. 6), and performs on par with standard BP or BPTT rules. D",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_863",
    "chunk_position": 863
  },
  {
    "id": "1901.09948v2",
    "text": ". This approach was benchmarked on the DVS Gestures dataset (. 6), and performs on par with standard BP or BPTT rules. D. Learning using gradients of spike times Difculties in training SNNs stem from the discrete nature of the quantities of interest such as the number of spikes in a particular interval. The derivatives of these discrete quantities are zero almost everywhere which necessitates the use of SG methods. Alternatively, we can choose to use spike-based quantities that have well dened, smooth derivatives. One such quantity is spike times",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_864",
    "chunk_position": 864
  },
  {
    "id": "1901.09948v2",
    "text": ". Alternatively, we can choose to use spike-based quantities that have well dened, smooth derivatives. One such quantity is spike times. This capitalizes on the continuous-time nature of SNNs and results in highly sparse network activity as the emission time of even a single spike can encode signicant information. Just as importantly, spike times are continuous quantities that can be made to depend smoothly on the neurons input",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_865",
    "chunk_position": 865
  },
  {
    "id": "1901.09948v2",
    "text": ". Just as importantly, spike times are continuous quantities that can be made to depend smoothly on the neurons input. Working with spike times is thus a complementary approach to SG but which achieves the same goal: obtaining a smooth chain of derivatives between the networks outputs and inputs. For this example, we use non-leaky integrate and re neurons described by: d Ui dt Iiwith Ii X j Wij X exp ((ttr i)) (15) 21 wheretr iis the time of the rthspike from neuron j, and is the Heaviside step function",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_866",
    "chunk_position": 866
  },
  {
    "id": "1901.09948v2",
    "text": ". Consider the simple exclusive or (XOR) problem in the temporal domain: A network receives two spikes, one from each of two different sources. Each spike can either be early or late. The network has to learn to distinguish between the case in which the spikes are either both early or both late, and the case where one spike is early and the other is late (. 7a). When designing a SNN, there is signicant freedom in how the network input and output are encoded",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_867",
    "chunk_position": 867
  },
  {
    "id": "1901.09948v2",
    "text": ". 7a). When designing a SNN, there is signicant freedom in how the network input and output are encoded. In this case, we use a rst-to-spike which we have two output neurons and the binary classication result is represented by the output neuron that spikes rst. 7b shows the networks response after training (see 34 for details on the training process). For the rst input class (earlylate or lateearly), one output neuron spikes rst and for the other class (earlyearly or latelate), the other output neuron spikes rst",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_868",
    "chunk_position": 868
  },
  {
    "id": "1901.09948v2",
    "text": ". hidden layer intput layer class 0class 0class 1class 1output layer input patternstime time time time (a) 0 1 2 3 4 5 6 70.5 0.00.51.0vmem 0 1 2 3 4 5 6 70.5 0.00.51.0 0 1 2 3 4 5 6 70.5 0.00.51.0vmem 0 1 2 3 4 5 6 70.5 0.00.51.0 0 1 2 3 4 5 6 70.5 0.00.51.0vmem 0 1 2 3 4 5 6 70.5 0.00.51.0 0 1 2 3 4 5 6 7 0.5 0.00.51.0vmem 0 1 2 3 4 5 6 7 0.5 0.00.51.0Hidden layer neurons Output layer neurons earlylate lateearly latelate earlyearly (b) . 7: Temporal XOR problem . (a) An SNN with one hidden layer",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_869",
    "chunk_position": 869
  },
  {
    "id": "1901.09948v2",
    "text": ". 7: Temporal XOR problem . (a) An SNN with one hidden layer. Each input neuron emits one spike which can either be late or early resulting in four possible input patterns that should be classied into two classes. (b) For the four input spike patterns (one per row), the right plots show the membrane potentials of the two output neurons, while the left plots show the membrane potentials of the four hidden neurons. Arrows at the top of the plot indicate output spikes from the layer, while arrows at the bottom indicate input spikes",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_870",
    "chunk_position": 870
  },
  {
    "id": "1901.09948v2",
    "text": ". Arrows at the top of the plot indicate output spikes from the layer, while arrows at the bottom indicate input spikes. The output spikes of the hidden layer are the input spikes of the output layer. The classication result is encoded in the identity of the output neuron that spikes rst. . VI. C ONCLUSION We have outlined how SNNs can be studied within the framework of RNNs and discussed successful approaches for training them",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_871",
    "chunk_position": 871
  },
  {
    "id": "1901.09948v2",
    "text": ". . VI. C ONCLUSION We have outlined how SNNs can be studied within the framework of RNNs and discussed successful approaches for training them. We have specically focused on SG approaches for two reasons: SG approaches are able to train SNNs to unprecedented performance levels on a 22 range of real-world problems",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_872",
    "chunk_position": 872
  },
  {
    "id": "1901.09948v2",
    "text": ". This transition marks the beginning of an exciting time in which SNNs will become increasingly interesting for applications which were previously dominated by RNNs; SGs provide a framework that ties together ideas from machine learning, computational neurosciences, and neuromorphic computing. From the viewpoint of computational neuroscience, the approaches presented in this paper are appealing because several of them are related to threefactor plasticity rules which are an important class of rules believed to underlie synaptic plasticity in the brain",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_873",
    "chunk_position": 873
  },
  {
    "id": "1901.09948v2",
    "text": ". Finally, for the neuromorphic community, SG methods provide a way to learn under various constraints on communication and storage which makes SG methods highly relevant for learning on custom low-power neuromorphic devices. The spectacular successes of modern ANNs were enabled by and hardware advances that made it possible to efciently train large ANNs on vast amounts of data. With temporal coding, SNNs are universal function approximators that are potentially far more powerful than ANNs with sigmoidal nonlinearities",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_874",
    "chunk_position": 874
  },
  {
    "id": "1901.09948v2",
    "text": ". With temporal coding, SNNs are universal function approximators that are potentially far more powerful than ANNs with sigmoidal nonlinearities. Unlike large-scale ANNs, which had to wait for several decades until the necessary computational resources were available for training them, we currently have the necessary resources, whether in the form of mainstream compute devices such as CPUs or GPUs, or custom neuromorphic devices, to train and deploy large SNNs. The fact that SNNs are less widely used than ANNs is thus primarily due to the issue of trainability",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_875",
    "chunk_position": 875
  },
  {
    "id": "1901.09948v2",
    "text": ". The fact that SNNs are less widely used than ANNs is thus primarily due to the issue of trainability. In this article, we have provided an overview of various exciting developments that are gradually addressing the issues encountered when training SNNs. Fully addressing these issues would have immediate and wide-ranging implications, both technologically, and in relation to learning in biological brains",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_876",
    "chunk_position": 876
  },
  {
    "id": "1901.09948v2",
    "text": ". ACKNOWLEDGMENTS This work was supported by the Intel Corporation (EN); the National Science Foundation under grant 1640081 (EN); the Swiss National Science Foundation Early Postdoc Mobility Grant P2ZHP2 164960 (HM) ; the Wellcome Trust 110124Z15Z (FZ). REFERENCES 1 I. Goodfellow, Y . Bengio, and A. Courville, Deep learning . MIT press, 2016. 2 F. Zenke and S. Ganguli, Super Spike: Supervised Learning in Multilayer Spiking Neural Networks, Neural Computation , 30, no. 6, 15141541, Apr. 2018. 3 G. Bellec, D. Salaj, A. Subramoney, R. Legenstein, and W",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_877",
    "chunk_position": 877
  },
  {
    "id": "1901.09948v2",
    "text": ". 6, 15141541, Apr. 2018. 3 G. Bellec, D. Salaj, A. Subramoney, R. Legenstein, and W. Maass, Long short-term memory and Learning-to-learn in networks of spiking neurons, in Advances in Neural Information Processing 23 Systems 31 , S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds. Curran Associates, Inc., 2018, 795805. 4 J. Kaiser, H. Mostafa, and E. Neftci, Synaptic plasticity for deep continuous local learning, ar Xiv ar Xiv:1812.10766 , 2018. 5 A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and A",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_878",
    "chunk_position": 878
  },
  {
    "id": "1901.09948v2",
    "text": ". 5 A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and A. Maida, Deep learning in spiking neural networks, Neural Networks , Dec. 2018. 6 R. G utig, To spike, or when to spike? Current Opinion in Neurobiology , 25, 134139, Apr. 2014. 7 R.-M. Memmesheimer, R. Rubin, B. Olveczky, and H. Sompolinsky, Learning Precisely Timed Spikes, Neuron , 82, no. 4, 925938, May 2014. 8 N. Anwani and B. Rajendran, Norm AD-normalized approximate descent based supervised learning rule for spiking neurons, in Neural Networks (IJCNN), 2015 International Joint Conference on . IEEE, 2015, 18. 9 A",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_879",
    "chunk_position": 879
  },
  {
    "id": "1901.09948v2",
    "text": ". IEEE, 2015, 18. 9 A. Gilra and W. Gerstner, Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network, e Life Sciences , 6, p. e28295, Nov. 2017. Online. Available: 10 W. Nicola and C. Clopath, Supervised learning in spiking neural networks with FORCE training, Nature Communications , 8, no. 1, p. 2208, Dec. 2017. 11 K. Boahen, A neuromorphs prospectus, Computing in Science Engineering , 19, no. 2, 1428, Mar. 2017. 12 W. Gerstner, W. M. Kistler, R. Naud, and L. Paninski, Neuronal dynamics: From single neurons to networks and models of cognition",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_880",
    "chunk_position": 880
  },
  {
    "id": "1901.09948v2",
    "text": ". 2017. 12 W. Gerstner, W. M. Kistler, R. Naud, and L. Paninski, Neuronal dynamics: From single neurons to networks and models of cognition . Cambridge University Press, 2014. 13 S. B. Shrestha and G. Orchard, SLAYER: Spike Layer Error Reassignment in Time, in Advances in Neural Information Processing Systems 31 , S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds. Curran Associates, Inc., 2018, 14191428. 14 R. J. Williams and D. Zipser, A learning continually running fully recurrent neural networks, Neural computation , 1, no. 2, 270280, 1989. 15 G",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_881",
    "chunk_position": 881
  },
  {
    "id": "1901.09948v2",
    "text": ". 14 R. J. Williams and D. Zipser, A learning continually running fully recurrent neural networks, Neural computation , 1, no. 2, 270280, 1989. 15 G. Bellec, F. Scherr, E. Hajek, D. Salaj, R. Legenstein, and W. Maass, Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets, ar Xiv:1901.09049 cs, Jan. 2019. Online. Available: 16 M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y . Bengio, Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to 1 or -1, ar Xiv:1602.02830 cs , Feb",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_882",
    "chunk_position": 882
  },
  {
    "id": "1901.09948v2",
    "text": ". Bengio, Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to 1 or -1, ar Xiv:1602.02830 cs , Feb. 2016, ar Xiv: 1602.02830. 17 Y . Bengio, N. L eonard, and A. Courville, Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation, ar Xiv:1308.3432 cs , Aug. 2013, ar Xiv: 1308.3432. 24 18 S. K. Esser, P. A. Merolla, J. V . Arthur, A. S. Cassidy, R. Appuswamy, A. Andreopoulos, D. J. Berg, J. L. Mc Kinstry, T. Melano, D. R. Barch, C. di Nolfo, P. Datta, A. Amir, B. Taba, M. D. Flickner, and D. S",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_883",
    "chunk_position": 883
  },
  {
    "id": "1901.09948v2",
    "text": ". Appuswamy, A. Andreopoulos, D. J. Berg, J. L. Mc Kinstry, T. Melano, D. R. Barch, C. di Nolfo, P. Datta, A. Amir, B. Taba, M. D. Flickner, and D. S. Modha, Convolutional networks for fast, energy-efcient neuromorphic computing, Proc Natl Acad Sci U S A , 113, no. 41, 11 44111 446, Oct. 2016. 19 S. M. Bohte, Error-Backpropagation in Networks of Fractionally Predictive Spiking Neurons, in Articial Neural Networks and Machine Learning ICANN 2011 , ser. Lecture Notes in Computer Science. Springer, Berlin, Heidelberg, Jun. 2011, 6068. 20 S. B. Shrestha and G",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_884",
    "chunk_position": 884
  },
  {
    "id": "1901.09948v2",
    "text": ". Lecture Notes in Computer Science. Springer, Berlin, Heidelberg, Jun. 2011, 6068. 20 S. B. Shrestha and G. Orchard, Slayer: Spike layer error reassignment in time, ar Xiv ar Xiv:1810.08646 , 2018. 21 L. F. Abbott, B. De Pasquale, and R.-M. Memmesheimer, Building functional networks of spiking model neurons, Nat Neurosci , 19, no. 3, 350355, Mar. 2016. 22 D. Huh and T. J. Sejnowski, Gradient Descent for Spiking Neural Networks, in Advances in Neural Information Processing Systems 31 , S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett, Eds",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_885",
    "chunk_position": 885
  },
  {
    "id": "1901.09948v2",
    "text": ". Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett, Eds. Curran Associates, Inc., 2018, 14401450. 23 D. Ackley, G. Hinton, and T. Sejnowski, A learning Boltzmann machines, Cognitive Science: A Multidisciplinary Journal , 9, no. 1, 147169, 1985. 24 J.-P. Pster, T. Toyoizumi, D. Barber, and W. Gerstner, Optimal Spike-Timing-Dependent Plasticity for Precise Action Potential Firing in Supervised Learning, Neural Computation , 18, no. 6, 13181348, Apr. 2006. 25 B. Gardner, I. Sporea, and A",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_886",
    "chunk_position": 886
  },
  {
    "id": "1901.09948v2",
    "text": ". 6, 13181348, Apr. 2006. 25 B. Gardner, I. Sporea, and A. Gr uning, Learning Spatiotemporally Encoded Pattern Transformations in Structured Spiking Neural Networks, Neural Comput , 27, no. 12, 25482586, Oct. 2015. 26 J. Guerguiev, T. P. Lillicrap, and B. A. Richards, Towards deep learning with segregated dendrites, e Life Sciences , 6, p. e22901, Dec. 2017. 27 J. Brea, W. Senn, and J.-P. Pster, Matching Recall and Storage in Sequence Learning with Spiking Neural Networks, J. Neurosci. , 33, no. 23, 95659575, Jun. 2013. 28 D. J. Rezende and W",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_887",
    "chunk_position": 887
  },
  {
    "id": "1901.09948v2",
    "text": ". Neurosci. , 33, no. 23, 95659575, Jun. 2013. 28 D. J. Rezende and W. Gerstner, Stochastic variational learning in recurrent spiking networks, Front. Comput. Neurosci , 8, p. 38, 2014. 29 H. Mostafa and G. Cauwenberghs, A learning framework for winner-take-all networks with stochastic synapses, Neural computation , 30, no. 6, 15421572, 2018. 30 E. Hunsberger and C. Eliasmith, Spiking deep networks with lif neurons, ar Xiv ar Xiv:1510.08829 , 2015. 31 E. O. Neftci, C. Augustine, S. Paul, and G",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_888",
    "chunk_position": 888
  },
  {
    "id": "1901.09948v2",
    "text": ". Hunsberger and C. Eliasmith, Spiking deep networks with lif neurons, ar Xiv ar Xiv:1510.08829 , 2015. 31 E. O. Neftci, C. Augustine, S. Paul, and G. Detorakis, Event-driven random back-propagation: Enabling neuromorphic deep learning machines, Frontiers in Neuroscience , 11, p. 324, 2017. 32 J. H. Lee, T. Delbruck, and M. Pfeiffer, Training deep spiking neural networks using backpropagation, Frontiers in Neuroscience , 10, 2016. 25 33 S. M. Bohte, J. N. Kok, and H. La Poutre, Error-backpropagation in temporally encoded networks of spiking neurons, Neurocomputing , 48, no. 1, 1737, 2002. 34 H",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_889",
    "chunk_position": 889
  },
  {
    "id": "1901.09948v2",
    "text": ". N. Kok, and H. La Poutre, Error-backpropagation in temporally encoded networks of spiking neurons, Neurocomputing , 48, no. 1, 1737, 2002. 34 H. Mostafa, Supervised learning based on temporal coding in spiking neural networks, IEEE transactions on neural networks and learning systems , 29, no. 7, 32273235, 2018. 35 P. OConnor, E. Gavves, and M. Welling, Temporally efcient deep learning with spikes, ar Xiv ar Xiv:1706.04159 , 2017. 36 S. Wo zniak, A. Pantazi, and E. Eleftheriou, Deep networks incorporating spiking neural dynamics, ar Xiv ar Xiv:1812.07040 , 2018. 37 T. P. Lillicrap, D",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_890",
    "chunk_position": 890
  },
  {
    "id": "1901.09948v2",
    "text": ". Pantazi, and E. Eleftheriou, Deep networks incorporating spiking neural dynamics, ar Xiv ar Xiv:1812.07040 , 2018. 37 T. P. Lillicrap, D. Cownden, D. B. Tweed, and C. J. Akerman, Random synaptic feedback weights support error backpropagation for deep learning, Nature Communications , 7, 2016. 38 A. Nkland, Direct feedback alignment provides learning in deep neural networks, in Advances in Neural Information Processing Systems 29 , D. D. Lee, M. Sugiyama, U. V . Luxburg, I. Guyon, and R. Garnett, Eds. Curran Associates, Inc., 2016, 10371045. 39 P. Baldi and P",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_891",
    "chunk_position": 891
  },
  {
    "id": "1901.09948v2",
    "text": ". D. Lee, M. Sugiyama, U. V . Luxburg, I. Guyon, and R. Garnett, Eds. Curran Associates, Inc., 2016, 10371045. 39 P. Baldi and P. Sadowski, A theory of local learning, the learning channel, and the optimality of backpropagation, Neural Networks , 83, 5174, 2016. 40 H. Mostafa, V . Ramesh, and G. Cauwenberghs, Deep supervised learning using local errors, Frontiers in neuroscience , 12, p. 608, 2018. 41 W. Nicola and C. Clopath, Supervised learning in spiking neural networks with force training, Nature communications , 8, no. 1, p. 2208, 2017.",
    "source": "arXiv",
    "chunk_id": "1901.09948v2_892",
    "chunk_position": 892
  },
  {
    "id": "2105.05609v1",
    "text": "Deep Spiking Convolutional Neural Network for Single Object Localization Based On Deep Continuous Local Learning Sami Barchid Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRISt AL F-59000 Lille, France sami.barchiduniv-lille.fr Jose Mennesson IMT Lille-Douai, Institut Mines-T elecom, Centre for Digital Systems Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRISt AL F-59000 Lille, France jose.mennessonimt-lille-douai.fr Chaabane Dj eraba Univ",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_893",
    "chunk_position": 893
  },
  {
    "id": "2105.05609v1",
    "text": ". Lille, CNRS, Centrale Lille, UMR 9189 CRISt AL F-59000 Lille, France jose.mennessonimt-lille-douai.fr Chaabane Dj eraba Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRISt AL F-59000 Lille, France chabane.djerabauniv-lille.fr Abstract With the advent of neuromorphic hardware, spiking neural networks can be a good energy-efcient alternative to articial neural networks. However, the use of spiking neural networks to perform computer vision tasks remains limited, mainly focusing on simple tasks such as digit recognition. It remains hard to deal with more complex tasks (e.g",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_894",
    "chunk_position": 894
  },
  {
    "id": "2105.05609v1",
    "text": ". It remains hard to deal with more complex tasks (e.g. segmentation, object detection) due to the small number of works on deep spiking neural networks for these tasks. The objective of this paper is to make the rst step towards modern computer vision with supervised spiking neural networks. We propose a deep convolutional spiking neural network for the localization of a single object in a grayscale image. We propose a network based on DECOLLE, a spiking model that enables local surrogate gradient-based learning",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_895",
    "chunk_position": 895
  },
  {
    "id": "2105.05609v1",
    "text": ". We propose a network based on DECOLLE, a spiking model that enables local surrogate gradient-based learning. The encouraging results reported on Oxford-IIIT-Pet validates the exploitation of spiking neural networks with a supervised learning approach for more elaborate vision tasks in the future. Index Terms Deep Spiking Neural Network, SNN, Convolution, Object Localization I. I NTRODUCTION Computer vision has shown great progress with the advent of Articial Neural Networks (ANN) and deep learning, which achieves state-of-the-art performance for most vision tasks 1",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_896",
    "chunk_position": 896
  },
  {
    "id": "2105.05609v1",
    "text": ". However, modern deep learning approaches require a high computational complexity, which leads to high energy consumption. Although many works focus on reducing the computational complexity of ANNs 2, they are still energyintensive compared to the biological brain, which only requires around two dozens of Watts for its full activity. Therefore, bio-inspired methods are good candidates to design low-power solutions and solve the problem of energy consumption",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_897",
    "chunk_position": 897
  },
  {
    "id": "2105.05609v1",
    "text": ". Therefore, bio-inspired methods are good candidates to design low-power solutions and solve the problem of energy consumption. Spiking Neural Networks (SNNs), often known as the third generation of neural networks 3, are strongly inspired from biological neurons 4. They consist of spiking neurons, which communicate through discrete spatio-temporal events calledspikes. As opposed to ANNs, they are amenable to implementation in efcient and low-power neuromorphic hardware 5. However, their performance is still behind ANNs",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_898",
    "chunk_position": 898
  },
  {
    "id": "2105.05609v1",
    "text": ". However, their performance is still behind ANNs. It can be explained by the non-differentiable nature of spikes, making it impossible to use the standard back-propagation . Concerning computer vision using SNNs, their applications in computer vision are still limited compared to ANNs. One of the reasons is that many research papers mainly focus on simple tasks like digit recognition 7. Therefore, we argue that there is a lack of works aiming at more complex computer vision tasks with SNNs (e.g. object detection)",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_899",
    "chunk_position": 899
  },
  {
    "id": "2105.05609v1",
    "text": ". Therefore, we argue that there is a lack of works aiming at more complex computer vision tasks with SNNs (e.g. object detection). Our objective in this paper is to validate the ability of SNNs trained with recent supervised methods 8 to deal with more complex vision tasks. To do so, we propose a Deep Convolutional SNN (DCSNN) to perform object localization of one object in grayscale images, as a rst step towards a fully functional object detection solution",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_900",
    "chunk_position": 900
  },
  {
    "id": "2105.05609v1",
    "text": ". Our proposed DCSNN is based on Deep Continuous Local Learning (DECOLLE) 9, a spiking model based on supervised local synaptic plasticity rule. We report proof-of-concept results on Oxford-IIIT-Pet 10 and discuss the perspective of this preliminary work. The rest of the paper is organized as follows: discusses the existing works on SNNs related to vision tasks and training methods. formulates the preliminary notions on SNNs and briey introduces DECOLLE. describes our DCSNN approach based on DECOLLE. reports our experimental proof-of-concept for object localization using DCSNN",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_901",
    "chunk_position": 901
  },
  {
    "id": "2105.05609v1",
    "text": ". describes our DCSNN approach based on DECOLLE. reports our experimental proof-of-concept for object localization using DCSNN. concludes our work and discusses the perspectives for our approach. II. R ELATED WORKS Since spiking neurons are not differentiable, SNNs cannot directly benet from the successful back-propagation algo-ar Xiv:2105.05609v1 cs.CV 12 May 2021 rithm 6. Nevertheless, many other learning approaches exist and can vary according to the strategy employed. Some works 11 focus on converting a trained ANN into a functional SNN without losing too much performance",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_902",
    "chunk_position": 902
  },
  {
    "id": "2105.05609v1",
    "text": ". Some works 11 focus on converting a trained ANN into a functional SNN without losing too much performance. Consequently, the converted SNN can benet from the best of both worlds, by achieving good performance of ANNs and being able to benet from neuromorphic hardwares efciency. This approach tends to achieve the best results in accuracy metrics 8. In addition, there already exist fully functional converted SNN that can deal with modern vision tasks 12, such as Object Detection 13. However, the training of the original ANN remains energy-intensive and the SNN is not trained directly",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_903",
    "chunk_position": 903
  },
  {
    "id": "2105.05609v1",
    "text": ". However, the training of the original ANN remains energy-intensive and the SNN is not trained directly. This method can be seen as an optimization technique for ANNs rather than a learning rule for SNNs. On the other hand, many works try to exploit bio-plausible learning rules, well-known in neuroscience, to train SNNs on computer vision tasks. One of the most used learning rules of this type is the Spike-Timing-Dependent Plasticity (STDP) 14. It updates the weights of the SNNs according to the relative spike times of preand post-synaptic neurons",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_904",
    "chunk_position": 904
  },
  {
    "id": "2105.05609v1",
    "text": ". It updates the weights of the SNNs according to the relative spike times of preand post-synaptic neurons. This kind of approach is unsupervised and local, which means that they are suitable for neuromorphic hardware. However, building deep SNNs with bio-plausible learning remains an open problem, due to the need to design additional neural mechanisms 7. Methods based on this strategy are usually limited to small networks and deal with simple tasks (e.g. digit recognition) because STDP-like approaches are still unable to effectively learn from complex natural images 15",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_905",
    "chunk_position": 905
  },
  {
    "id": "2105.05609v1",
    "text": ". digit recognition) because STDP-like approaches are still unable to effectively learn from complex natural images 15. However, recent works show great performance of SNNs trained with STDP when coupled to event-based cameras (e.g. DVS cameras). For instance, 16 performs image segmentation from an event-based camera with a spike-based approach trained with STDP. As for training SNNs with supervised learning, many recent papers propose to adapt the back-propagation SNNs",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_906",
    "chunk_position": 906
  },
  {
    "id": "2105.05609v1",
    "text": ". As for training SNNs with supervised learning, many recent papers propose to adapt the back-propagation SNNs. To do so, a popular direction is to back-propagate a surrogate gradient as a continuous relaxation of the real gradient 9, 1719. Although this strategy enables the use of supervised learning to train SNNs, there is still no works on vision tasks other than simple classication tasks. We argue that surrogate gradient approaches can be exploited to perform more complex vision tasks and, consequently, pave the way towards functional solutions",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_907",
    "chunk_position": 907
  },
  {
    "id": "2105.05609v1",
    "text": ". Among the existing works on surrogate gradient learning, DECOLLE 9 is a recent method that has many advantages: 1)it enables local learning, making it implementable in neuromorphic hardware; 2)it has low memory complexity compared to other methods; 3)its simplicity makes it amenable to implementation on popular machine learning frameworks. For these reasons, we decide to use DECOLLE for our work in order to propose the rst work towards modern vision tasks with supervised SNN. Sec. III briey introduces DECOLLE approach.III. B ACKGROUND A",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_908",
    "chunk_position": 908
  },
  {
    "id": "2105.05609v1",
    "text": ". Sec. III briey introduces DECOLLE approach.III. B ACKGROUND A. Spiking Neural Networks The main difference between articial and spiking neurons is the representation of information. In a spiking neuron, information is represented with a series of short electrical impulses known as spikes, as opposed to the numerical representation of articial neurons. In addition, there are various neuron models to represent a spiking neuron. One of the most popular and simplest neuron models is the Leaky Integrate-and-Fire (LIF) neuron",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_909",
    "chunk_position": 909
  },
  {
    "id": "2105.05609v1",
    "text": ". One of the most popular and simplest neuron models is the Leaky Integrate-and-Fire (LIF) neuron. It acts as a leaky integrator of input spikes to its internal membrane potential. When the membrane potential Vexceeds a dened ring threshold, the neuron emits an output spike to its postsynaptic neurons, and its membrane potential is reset to a resting potential (here, set to 0)",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_910",
    "chunk_position": 910
  },
  {
    "id": "2105.05609v1",
    "text": ". The model can be formulated as follows: leakd V dt V V 0when , where leakis the membrane time constant (which describes the leaky dynamics) and is the input current at the current timet.is dened as the summation of the presynaptic spikes weighted by the related synaptic weights. The input current at a time tis formulated as follows : X i2)(2) where Nis the set of presynaptic neurons and wiis the synaptic weight of the presynaptic neuron i.Siis the spike train of the presynaptic neuron iwhere a spike occurs at time tjfor thejthspike",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_911",
    "chunk_position": 911
  },
  {
    "id": "2105.05609v1",
    "text": ". A spike event Scan be described as a Kronecker delta function : S ( 1;ifttj 0;otherwis . 1 shows the dynamics of the formulated LIF model. SNNs are trained by updating the synaptic weights between connected neurons, similarly to ANNs. An SNN can be structured in the same way as an ANN, with neurons organized into successive layers (e.g. convolutional layers). Because developing SNNs directly on neuromorphic hardware can be a daunting task, SNNs are rstly simulated on GPUCPU hardware. The simulation requires to discretize the continuous dynamics of spiking neurons into successive timesteps",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_912",
    "chunk_position": 912
  },
  {
    "id": "2105.05609v1",
    "text": ". The simulation requires to discretize the continuous dynamics of spiking neurons into successive timesteps. For the simulation, we dene Ttimesteps of 1ms duration. B. Deep Continuous Local Learning DECOLLE 9 is a supervised approach based on surrogate gradient learning for SNNs. Since the synaptic weights are updated after each timestep, the DECOLLE rule is used for online learning. In DECOLLE, a readout layer initialized with xed random weights is attached to each LIF layer of the network. At each . 1. The LIF neuron model dynamics (inspired from 19)",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_913",
    "chunk_position": 913
  },
  {
    "id": "2105.05609v1",
    "text": ". At each . 1. The LIF neuron model dynamics (inspired from 19). The presynaptic spikes are weighted by the synaptic weights before being integrated in the neuron. The integrated spikes increases the leaky membrane potential until a dened threshold is reached. When the membrane potential exceeds the threshold, the neuron emits an output spike and the membrane potential is reset. timestep, a readout layer multiplies the neural activation of its related LIF layer with its xed random weights",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_914",
    "chunk_position": 914
  },
  {
    "id": "2105.05609v1",
    "text": ". timestep, a readout layer multiplies the neural activation of its related LIF layer with its xed random weights. The output of a readout layer is a prediction that is used to solve the targeted task and compute a local error. Consequently, each layer minimizes its own local error function. LIF layers build on the features of the previous layers trained with their own local error function. The layers indirectly learn features that are useful for their successors. The advantage of this approach is the locality of the error optimization, which makes it usable on neuromorphic hardware",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_915",
    "chunk_position": 915
  },
  {
    "id": "2105.05609v1",
    "text": ". The advantage of this approach is the locality of the error optimization, which makes it usable on neuromorphic hardware. Another advantage is the low memory complexity required to simulate a DECOLLE network with popular autodifferentiation frameworks (e.g. Py Torch). IV. M ETHODOLOGY A. Problem Formulation The targeted task is to perform object localization on strictly one object in a grayscale image. We dene the problem as follows : given a grayscale image I2RHW, a bounding box of coordinates Bfxmin;ymin;xmax;ymaxg1surrounding the object in the foreground is predicted",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_916",
    "chunk_position": 916
  },
  {
    "id": "2105.05609v1",
    "text": ". To do so, we propose , a DCSNN based on DECOLLE, which consists of L convolutional LIF layers. B. Readout Layers As stated in Sec. III, a readout layer is attached to each layer of spiking neurons in a DECOLLE model. According to our targeted problem formulated in Sec. IV-A, the DCSNN must predict the coordinates of a bounding box B. Therefore, the readout layers are simply linear layers in order to predict the four needed coordinates of B. In other words, our DCSNN produces a series of Lbounding box predictions, i.e. f Big L 1",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_917",
    "chunk_position": 917
  },
  {
    "id": "2105.05609v1",
    "text": ". In other words, our DCSNN produces a series of Lbounding box predictions, i.e. f Big L 1. 1These coordinates correspond to the upper-left corner (xmin; ymin )and the lower-right corner (xmax; ymax )of the predicted bounding box B.C. Network Architecture Our proposed architecture is structured following the popular encoder-decoder paradigm used in common ANN models 20. . 2 shows the overall architecture of our model. It consists of an encoder composed of 3 convolutional LIF layers, which subsequently increase the semantic information and decrease the spatial information",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_918",
    "chunk_position": 918
  },
  {
    "id": "2105.05609v1",
    "text": ". Since the deep parts of an encoder tend to lose the spatial information of low-level layers 20, a decoder module is used to recover the spatial details. A decoder of 3 convolutional LIF layers is employed, where the output spikes from the encoders layers are passed to the decoder through residual connections. The residual connections are essentially addition operations between two spiking feature maps. D. Static Image Neural Coding As an SNN uses spikes to transmit information, numerical values cannot be fed directly in a spike-based network",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_919",
    "chunk_position": 919
  },
  {
    "id": "2105.05609v1",
    "text": ". D. Static Image Neural Coding As an SNN uses spikes to transmit information, numerical values cannot be fed directly in a spike-based network. Pixels must beforehand be encoded into spike trains using a function known as neural coding 21. Among the existing neural coding strategies, we choose rate coding, which means pixel values are represented as a frequency of spikes during a dened time (i.e. a high pixel value results in a high frequency of spikes)",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_920",
    "chunk_position": 920
  },
  {
    "id": "2105.05609v1",
    "text": ". a high pixel value results in a high frequency of spikes). Specically, each numerical value of the pixels is converted using a Poisson process, which creates a spike train occurring at a target frequency with no spike-to-spike correlation. However, the main drawback of rate coding can be mentioned: a large number of spikes are needed to encode information, which introduces latency into the network 15. E. Output Conversion Strategy Since the objective of object localization is to predict the bounding box coordinates, the output spikes of an SNN must be converted back to numerical values",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_921",
    "chunk_position": 921
  },
  {
    "id": "2105.05609v1",
    "text": ". In DECOLLE, this problem is partially solved thanks to the readout layers, which return a numerical prediction at each of the Ttimesteps. The remaining issue is the nal prediction computed among the Tpredictions. Since a ratestatic image delivers its total information after the Ttimesteps passed, we choose the prediction of the last timestep as the nal one. Among the Lreadout layers, the only predictions taken into account are from the last layer of our architecture because deep hierarchical representations have an improved representation in comparison to the lower layers. V",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_922",
    "chunk_position": 922
  },
  {
    "id": "2105.05609v1",
    "text": ". V. P ROOF -OF-CONCEPT EXPERIMENT A. Experimental Protocol We evaluate our method on the Oxford-IIIT-Pet dataset 10. It consists of 7349 RGB images. Each image represents a single catdog in the foreground. The dataset originally contains a foreground segmentation for each image. We convert this segmentation mask into a single bounding box surrounding the represented pet. For our experiment, we convert the images into grayscale. The training split is composed of 6000 randomly selected images and the testing split contains the . 2. Our proposed DCSNN architecture",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_923",
    "chunk_position": 923
  },
  {
    "id": "2105.05609v1",
    "text": ". The training split is composed of 6000 randomly selected images and the testing split contains the . 2. Our proposed DCSNN architecture. White, blue and red cubes represent LIF convolutional layer, maxpool layer, and upsampling layer respectively. Black arrows are residual connections between feature maps (i.e. addition operations). For simplication purposes, the readout linear layers (specic to DECOLLE 9 model) are not shown. remaining images. Since there is strictly one bounding box per image, we evaluate our model with the mean intersection over union metric (m Io U). B",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_924",
    "chunk_position": 924
  },
  {
    "id": "2105.05609v1",
    "text": ". remaining images. Since there is strictly one bounding box per image, we evaluate our model with the mean intersection over union metric (m Io U). B. Implementation Details We implement our SNN model by adapting the original DECOLLE 9 implementation made with Py Torch. Our experiment is performed during 100 epochs in a machine with one NVIDIA 2080Ti GPU. The batch size is set to 16. We use the Ada Max optimizer 22 with 1 0, 2 0:95and a smooth L1 loss. The learning rate is initialized to109. Images are resized to 176x240 pixels with data augmentation applied",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_925",
    "chunk_position": 925
  },
  {
    "id": "2105.05609v1",
    "text": ". The learning rate is initialized to109. Images are resized to 176x240 pixels with data augmentation applied. Data augmentation includes random horizontal ipping and random brightness changes. As for the hyper-parameters specic to the DECOLLE method, we keep the same values as in the original paper 9. C. Results Our experiment achieves promising results of 63.2 m Io U on the test set. In addition, we visually verify the consistency of the predicted bounding boxes. . 3 shows representative examples of predictions on the test set. The bounding boxes are mostly predicted with good accuracy (i.e",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_926",
    "chunk_position": 926
  },
  {
    "id": "2105.05609v1",
    "text": ". . 3 shows representative examples of predictions on the test set. The bounding boxes are mostly predicted with good accuracy (i.e. between 55 and 70 m Io U). However, rare images are still poorly predicted (15 m Io U). We observe that the problem comes from images where the object is small or not in the foreground. To explain it, we have two hypotheses: 1) Data Imbalance : our method suffers from a common data imbalance problem. Most images in Oxford-IIIT-Pet are close-up of pets",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_927",
    "chunk_position": 927
  },
  {
    "id": "2105.05609v1",
    "text": ". Most images in Oxford-IIIT-Pet are close-up of pets. Therefore, the model generalizes these examples and does not learn the rare and hard examples of pets in the background. 2) Output Conversion : our conversion strategy (dened (a) Predictions (b) Ground truths . 3. Qualitative results. in Sec. IV-E) is still not optimal and leads to a loss of information. Consequently, the predicted coordinates are not perfectly represented",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_928",
    "chunk_position": 928
  },
  {
    "id": "2105.05609v1",
    "text": ". in Sec. IV-E) is still not optimal and leads to a loss of information. Consequently, the predicted coordinates are not perfectly represented. The reported results of this experiment validate the proof-ofconcept and conrm the ability of state-of-the-art supervised learning rules to train SNNs in order to perform more complex vision tasks. Even if the evaluated experiment is still limited compared to state-of-the-art deep learning tasks, we argue that it can be a rst step towards modern computer vision tasks. VI",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_929",
    "chunk_position": 929
  },
  {
    "id": "2105.05609v1",
    "text": ". VI. D ISCUSSION AND CONCLUSION In this work, we designed an encoder-decoder DCSNN architecture based on DECOLLE 9, a supervised local synaptic plasticity rule, to perform object localization on a static image dataset. By evaluating our model on static grayscale natural images, we proved the ability of SNNs trained with state-of-the-art supervised to deal with modern computer vision tasks. To the best of our knowledge, this is the rst work on a complex vision task using supervised SNNs and static images",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_930",
    "chunk_position": 930
  },
  {
    "id": "2105.05609v1",
    "text": ". To the best of our knowledge, this is the rst work on a complex vision task using supervised SNNs and static images. Finally, we proposed an intuitive technique to obtain a nal prediction from the numerous readout layers predictions. Many introduced mechanisms can be further investigated in future works, such as the optimal neural coding or the output conversion strategy. Moreover, more vision tasks can be studied using supervised SNNs",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_931",
    "chunk_position": 931
  },
  {
    "id": "2105.05609v1",
    "text": ". Moreover, more vision tasks can be studied using supervised SNNs. Even though our proposed DCSNN shows encouraging results with static images converted with rate coding, this strategy does not fully exploit the asynchronous event-based capacity of SNNs. On the other hand, event-based sensors 23 can lead to sparser and fully asynchronous vision models. Consequently, in future works, we want to further explore the performance of our architecture while using event-based cameras, which enable the treatment of spatio-temporal data instead of only static images",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_932",
    "chunk_position": 932
  },
  {
    "id": "2105.05609v1",
    "text": ". In addition, using event-based cameras could lead to a fully spike-based object tracking solution. REFERENCES 1 W. Liu, Z. Wang, X. Liu, N. Zeng, Y . Liu, and F. E. Alsaadi, A survey of deep neural network architectures and their applications, Neurocomputing , 234, 1126, 2017. 2 Y . Cheng, D. Wang, P. Zhou, and T. Zhang, A survey of model compression and acceleration for deep neural networks, ar Xiv ar Xiv:1710.09282 , 2017. 3 W. Maass, Networks of spiking neurons: the third generation of neural network models, Neural networks , 10, no. 9, 16591671, 1997. 4 A. L. Hodgkin and A. F",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_933",
    "chunk_position": 933
  },
  {
    "id": "2105.05609v1",
    "text": ". 9, 16591671, 1997. 4 A. L. Hodgkin and A. F. Huxley, A quantitative description of membrane current and its application to conduction and excitation in nerve, The Journal of physiology , 117, no. 4, 500544, 1952. 5 B. Linares-Barranco, T. Serrano-Gotarredona, L. A. Camu nas-Mesa, J. A. Perez-Carrasco, C. Zamarre no-Ramos, and T. Masquelier, On spike-timing-dependent-plasticity, memristive devices, and building a self-learning visual cortex, Frontiers in neuroscience , 5, p. 26, 2011. 6 J. H. Lee, T. Delbruck, and M",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_934",
    "chunk_position": 934
  },
  {
    "id": "2105.05609v1",
    "text": ". 26, 2011. 6 J. H. Lee, T. Delbruck, and M. Pfeiffer, Training deep spiking neural networks using backpropagation, Frontiers in neuroscience , 10, p. 508, 2016. 7 P. Falez, P. Tirilly, I. M. Bilasco, P. Devienne, and P. Boulet, Multilayered spiking neural network with target timestamp threshold adaptation and stdp, in 2019 International Joint Conference on Neural Networks (IJCNN) . IEEE, 2019, 18. 8 A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and A. Maida, Deep learning in spiking neural networks, Neural Networks , 111, 4763, 2019. 9 J. Kaiser, H. Mostafa, and E",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_935",
    "chunk_position": 935
  },
  {
    "id": "2105.05609v1",
    "text": ". Masquelier, and A. Maida, Deep learning in spiking neural networks, Neural Networks , 111, 4763, 2019. 9 J. Kaiser, H. Mostafa, and E. Neftci, Synaptic plasticity dynamics for deep continuous local learning (decolle), Frontiers in Neuroscience , 14, p. 424, 2020. 10 O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V . Jawahar, Cats and dogs, in IEEE Conference on Computer Vision and Pattern Recognition , 2012. 11 Y . Cao, Y . Chen, and D. Khosla, Spiking deep convolutional neural networks for energy-efcient object recognition, International Journal of Computer Vision , 113, no. 1, 5466, 2015",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_936",
    "chunk_position": 936
  },
  {
    "id": "2105.05609v1",
    "text": ". 1, 5466, 2015. 12 A. Sengupta, Y . Ye, R. Wang, C. Liu, and K. Roy, Going deeper in spiking neural networks: Vgg and residual architectures, Frontiers in neuroscience , 13, p. 95, 2019. 13 S. Kim, S. Park, B. Na, and S. Yoon, Spiking-yolo: spiking neural network for energy-efcient object detection, in Proceedings of the AAAI Conference on Articial Intelligence , 34, no. 07, 2020, 11 27011 277. 14 N. Caporale and Y . Dan, Spike timingdependent plasticity: a hebbian learning rule, Annu. Rev. Neurosci. , 31, 2546, 2008. 15 P",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_937",
    "chunk_position": 937
  },
  {
    "id": "2105.05609v1",
    "text": ". 14 N. Caporale and Y . Dan, Spike timingdependent plasticity: a hebbian learning rule, Annu. Rev. Neurosci. , 31, 2546, 2008. 15 P. Falez, Improving Spiking Neural Networks Trained with Spike Timing Dependent Plasticity for Image Recognition, Theses, Universit e de Lille, Oct. 2019.16 P. Kirkland, G. Di Caterina, J. Soraghan, and G. Matich, Spikeseg: Spiking segmentation via stdp saliency mapping, in 2020 International Joint Conference on Neural Networks (IJCNN) . IEEE, 2020, 18. 17 F. Zenke and S",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_938",
    "chunk_position": 938
  },
  {
    "id": "2105.05609v1",
    "text": ". IEEE, 2020, 18. 17 F. Zenke and S. Ganguli, Superspike: Supervised learning in multilayer spiking neural networks, Neural computation , 30, no. 6, 1514 1541, 2018. 18 E. O. Neftci, H. Mostafa, and F. Zenke, Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks, IEEE Signal Processing Magazine , 36, no. 6, 5163, 2019. 19 C. Lee, S. S. Sarwar, P. Panda, G. Srinivasan, and K. Roy, Enabling spike-based backpropagation for training deep neural network architectures, Frontiers in neuroscience , 14, 2020. 20 T.-Y",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_939",
    "chunk_position": 939
  },
  {
    "id": "2105.05609v1",
    "text": ". Roy, Enabling spike-based backpropagation for training deep neural network architectures, Frontiers in neuroscience , 14, 2020. 20 T.-Y . Lin, P. Doll ar, R. Girshick, K. He, B. Hariharan, and S. Belongie, Feature pyramid networks for object detection, in Proceedings of the IEEE conference on computer vision and pattern recognition , 2017, 21172125. 21 A. Borst and F. E. Theunissen, Information theory and neural coding, Nature neuroscience , 2, no. 11, 947957, 1999. 22 D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, ar Xiv ar Xiv:1412.6980 , 2014. 23 G. Gallego, T",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_940",
    "chunk_position": 940
  },
  {
    "id": "2105.05609v1",
    "text": ". 11, 947957, 1999. 22 D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, ar Xiv ar Xiv:1412.6980 , 2014. 23 G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, S. Leutenegger, A. Davison, J. Conradt, K. Daniilidis et al. , Eventbased vision: A survey, ar Xiv ar Xiv:1904.08405 , 2019.",
    "source": "arXiv",
    "chunk_id": "2105.05609v1_941",
    "chunk_position": 941
  },
  {
    "id": "2401.15212v1",
    "text": "Speed-based Filtration and DBSCAN of Event-based Camera Data with Neuromorphic Computing Charles P. Rizzo Department of EECS University of Tennessee Knoxville, TN, USA crizzoutk.edu Catherine D. Schuman Department of EECS University of Tennessee Knoxville, TN, USA cschumanutk.edu James S. Plank Department of EECS University of Tennessee Knoxville, TN, USA jplankutk.edu Abstract Spiking neural networks are powerful computational elements that pair well with event-based cameras (EBCs)",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_942",
    "chunk_position": 942
  },
  {
    "id": "2401.15212v1",
    "text": ". In this work, we present two spiking neural network architectures that process events from EBCs: one that isolates and filters out events based on their speeds, and another that clusters events based on the DBSCAN algorithm. Index Terms spiking neural networks, neuromorphic computing, event-based cameras, dbscan I. I NTRODUCTION AND RELATED WORKS In neuromorphic computing, spiking neural networks are leveraged as compute modules that typically perform some sort of control-based task or data classification",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_943",
    "chunk_position": 943
  },
  {
    "id": "2401.15212v1",
    "text": ". Spiking neural networks (SNNs), unlike traditional von-Neumann style architectures, feature both the memory and computation elements in the network together. By using spikes to propagate information throughout the network over time, neuromorphic computing draws direct inspiration from how the biological brain functions. By mimicking the human brain in function, neuromorphic computing aims to realize computational elements that are smaller in size and require less power than both conventional computing architectures and artificial neural networks",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_944",
    "chunk_position": 944
  },
  {
    "id": "2401.15212v1",
    "text": ". Event-based cameras are novel vision sensors dubbed neuromorphic cameras because of the way they asynchronously emit events, which is similar to SNNs spike-based computing architecture. An event-based cameras events are analogous to an SNNs spikes. The cameras pixels asynchronously emit events whenever they individually measure a change in light intensity that is greater than some threshold. The cameras are low-power, have high dynamic range, and dont suffer This material is based upon work supported by the U.S",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_945",
    "chunk_position": 945
  },
  {
    "id": "2401.15212v1",
    "text": ". The cameras are low-power, have high dynamic range, and dont suffer This material is based upon work supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, under award number DE-SC0022566, and in part on research sponsored by Air Force Research Laboratory under agreement number FA8750-21-11018. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes, notwithstanding any copyright notation thereon",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_946",
    "chunk_position": 946
  },
  {
    "id": "2401.15212v1",
    "text": ". The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes, notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies or endorsements, either expressed or implied, of Air Force Research Laboratory or the U.S. Government.motion blur like conventional frame-based cameras. Their high temporal resolution ( s) can make them difficult to work with as they can produce large amounts of data quickly",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_947",
    "chunk_position": 947
  },
  {
    "id": "2401.15212v1",
    "text": ". Their high temporal resolution ( s) can make them difficult to work with as they can produce large amounts of data quickly. Fortunately, because spiking neural networks are temporal in nature and feature an event-driven compute architecture, they pair naturally with event-based cameras. Traditionally, as with artificial neural networks (ANNs), spiking neural networks are trained on an instance of a problem so that over time as the networks continue to train, their performance improves",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_948",
    "chunk_position": 948
  },
  {
    "id": "2401.15212v1",
    "text": ". What complicates spiking neural network training is the nondifferentiability of the spike operation which renders traditional backpropagation useless. In response to this, surrogate gradient based learning like Slayer 1 have helped bridge the gap between spiking neural networks and backpropagation. There are other training methods, though, like the EONS 2 and LEAP 3 genetic that apply genetic mutators to populations of networks over time",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_949",
    "chunk_position": 949
  },
  {
    "id": "2401.15212v1",
    "text": ". There are other training methods, though, like the EONS 2 and LEAP 3 genetic that apply genetic mutators to populations of networks over time. Other methods to train SNNs like reservoir computing 4 or Whetstone 5, an converts an ANN to an SNN during its training by slowly converting its activation functions to thresholding activation functions have also been used with success. These methods facilitate SNN training by circumventing the need for the backpropagation of error throughout the network to train its weights",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_950",
    "chunk_position": 950
  },
  {
    "id": "2401.15212v1",
    "text": ". These methods facilitate SNN training by circumventing the need for the backpropagation of error throughout the network to train its weights. A far less popular approach for developing spiking neural networks is to hand-tool or architect them. This is unsurprising, however, as it requires in depth knowledge of a neuroprocessors architecture in order to fully leverage its neuronal, synaptic, and other network-level features. Manually mapping functions to spiking neural networks is a difficult and time-consuming process. Even still, it is done notably in some works like those by Severa et al",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_951",
    "chunk_position": 951
  },
  {
    "id": "2401.15212v1",
    "text": ". Even still, it is done notably in some works like those by Severa et al. 6, Plank et al. 7, and Rizzo et al. 8 for example. Hand-tooled spiking neural networks are typically small and scalable, precisely defined behavior-wise, and can be leveraged as network components in an ensemble",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_952",
    "chunk_position": 952
  },
  {
    "id": "2401.15212v1",
    "text": ". In this work, we present and precisely define two hand-tooled spiking neural network architectures that perform the following functions on event-based camera data: 1) Speed filtration where events can be filtered out forar Xiv:2401.15212v1 cs.NE 26 Jan 2024 moving too quickly or too slowly 2) Density-based spatial clustering of events based on the DBSCAN 9 . M ETHOD A. General Network Specification Our spiking neurons implement the simple integrate and fire neuron model where a neuron fires when its potential exceeds its threshold t",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_953",
    "chunk_position": 953
  },
  {
    "id": "2401.15212v1",
    "text": ". The synapses of the network are simple featuring only discrete weights wand delays d. Additionally, the neurons have the option of enabling total leak, where at the end of a neurons cycle, if there is any charge remaining, it is all leaked away and the neurons potential reset to its resting potential. Other features common in spiking architectures like spike timing dependent plasticity (STDP) 10 for learning are not necessary for this work. In this way, our network specifications are elegant requiring bare-bones hardware resources for each network",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_954",
    "chunk_position": 954
  },
  {
    "id": "2401.15212v1",
    "text": ". In this way, our network specifications are elegant requiring bare-bones hardware resources for each network. Additionally, a neuron can simultaneously serve as both an input and an output neuron on which input spikes may be applied and firings decoded into decisions. B. Speed Filter The speed filter function is a technique that filters out events based on their speed. An input event xs speed is determined by the amount of events that occur within some radius epsilon () of event xover the course of two timesteps",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_955",
    "chunk_position": 955
  },
  {
    "id": "2401.15212v1",
    "text": ". An input event xs speed is determined by the amount of events that occur within some radius epsilon () of event xover the course of two timesteps. If some number of events occur within event xs radius that is greater than the speed threshold t, we say event xis moving fast. However, if the number of events is less than or to the speed threshold t, we filter out event xas moving too slow. Alternatively, we can identify events that are moving slower than the speed threshold tand filter out events moving faster than it",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_956",
    "chunk_position": 956
  },
  {
    "id": "2401.15212v1",
    "text": ". Alternatively, we can identify events that are moving slower than the speed threshold tand filter out events moving faster than it. This filtering technique is functionally to the spiking threshold pooling (or counting) networks presented in 8 but without the downsampling component. Even though we are technically thresholding neighboring event density around some event x, we are doing it over time and with the understanding that events tend to represent regions or objects that are in motion. This is why we denote this technique as the speed filter",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_957",
    "chunk_position": 957
  },
  {
    "id": "2401.15212v1",
    "text": ". This is why we denote this technique as the speed filter. The speed filter is influenced by 11, in which Nagaraj et al. filter events from a DVS camera by constructing a spiking neuron layer whose dimensionality is the same as the camera. For a camera whose resolution is 640 480, this is over 307,000 spiking neurons. Additionally, they define the networks connectivity by connecting each neuron to nine pixels: the pixel it represents and the eight other pixels that are a Chebyshev distance of 1 away from it",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_958",
    "chunk_position": 958
  },
  {
    "id": "2401.15212v1",
    "text": ". This means that their speed filtration spiking neural network solution has over 2.7 million synapses",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_959",
    "chunk_position": 959
  },
  {
    "id": "2401.15212v1",
    "text": ". Instead, we present the same functionality as a modular network that can be parallelized and deployed to hardware systems that cannot dedicate such an immense quantity of resources to only event filtration.1) Network Specification: For the network representation of the speed filter, there are three parameters that contribute to its architecture: 1) The speed threshold ts 2) The radius epsilon 3) Whether we filter out events that are moving faster or slower than the speed threshold First, let us assume we filter out events that are moving slower than the speed threshold ts",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_960",
    "chunk_position": 960
  },
  {
    "id": "2401.15212v1",
    "text": ". In defining sas the Chebyshev distance, there are (2s1)2input neurons labeled Ii,j and a single output neuron O. The input neurons Ii,jhave thresholds of 0, and the output neuron Ohas a threshold to the speed threshold ts. It is worth noting that distance metrics other than the Chebyshev distance may be used (e.g. Euclidean, Mahalanobis, etc.), and only the number of input neurons is affected by this choice. The connectivity of the network is defined by one set of synapses: 1)Ii,jto Owith a weight of 1 and a delay of 0",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_961",
    "chunk_position": 961
  },
  {
    "id": "2401.15212v1",
    "text": ". The connectivity of the network is defined by one set of synapses: 1)Ii,jto Owith a weight of 1 and a delay of 0 . 1: Speed filter network architecture with s 1 that filters out events moving slower than speed threshold ts 10. 1 illustrates what this architecture looks like for s 1and speed threshold ts 10. Lets suppose an event at pixel x,yoccurs at time nand we want to determine whether or not to filter it out based on its speed. The current events pixel is mapped to input neuron Is,s, and the other adjacent pixels ranging from xxs, xs, yys, yssimply map to the input neurons Ij0,2,i0,2s",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_962",
    "chunk_position": 962
  },
  {
    "id": "2401.15212v1",
    "text": ". The network cycles required to process the event at pixel x, y are detailed below: 1)Cycle 0 : Apply events that occur at pixels xx s, xs, yys, ysat time n - 1 to inputs Ij,i. 2)Cycle 1 : Apply events that occur at pixels xx s, xs, yys, ysat time nto inputs Ij,i. Current event x,yis applied to Is,sduring this cycle. 3)Cycle 2 : Output neuron Oaccumulates events as charge from events applied at cycle 0. If output neuron Os potential exceeds its threshold ts, it fires. 4)Cycle 3 : Output neuron Oaccumulates events as charge from events applied at cycle 1",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_963",
    "chunk_position": 963
  },
  {
    "id": "2401.15212v1",
    "text": ". 4)Cycle 3 : Output neuron Oaccumulates events as charge from events applied at cycle 1. If output neuron Os potential exceeds its threshold ts, it fires. The network requires a total of four cycles to process one event at pixel location x, yafter which the networks state may be cleared so that it can process the next event. shows an example spike 1 where the event is not filtered out, and shows an example spike 1 where the event is filtered out. If however we want to filter out events that are moving faster than the speed threshold ts, we must slightly tweak the network architecture",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_964",
    "chunk_position": 964
  },
  {
    "id": "2401.15212v1",
    "text": ". If however we want to filter out events that are moving faster than the speed threshold ts, we must slightly tweak the network architecture. In addition to the architecture already described, we add two neurons and three synapses. The first neuron is an additional bias input neuron we denote as Ib whose threshold t 0 . The second additional neuron is the new output neuron that we denote as Onwhose threshold t 2 . In our previous architecture, neuron Owas our output neuron, but in this architecture, it is a hidden neuron",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_965",
    "chunk_position": 965
  },
  {
    "id": "2401.15212v1",
    "text": ". In our previous architecture, neuron Owas our output neuron, but in this architecture, it is a hidden neuron. The connectivity is the same as described above but with the following additions: 1)Ibto Ib: This is a self-edge whose synaptic weight is 1 and delay is 0. This ensures that once neuron Ibfires, it continues to fire at each cycle until the end of the network simulation. 2)Ibto Onwith a weight of 1 and a delay of 0 3)Oto Onwith an inhibitory synaptic weight of -1 and delay of 0 2 shows this modified architecture when 1 and the speed threshold t 10",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_966",
    "chunk_position": 966
  },
  {
    "id": "2401.15212v1",
    "text": ". The dashed line in 2 and latter represents an inhibitory synapse. . 2: Speed filter network architecture with s 1 that filters out events moving faster than speed threshold ts 7. Again, lets suppose an event at pixel x,yoccurs at time n and we want to determine whether or not to filter it out based on its speed being too fast. The network cycles required to process the event at pixel x, y are detailed below:1)Cycle 0 : Apply events that occur at pixels xx s, xs, yys, ysat time n - 1 to inputs Ij,i",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_967",
    "chunk_position": 967
  },
  {
    "id": "2401.15212v1",
    "text": ". Additionally, we apply a spike to the new bias neuron Ib 2)Cycle 1 : Apply events that occur at pixels xx s, xs, yys, ysat time nto inputs Ij,i. Current event x,yis applied to Is,sduring this cycle. Bias input neuron Ibfires sending charge to itself and to output neuron On. 3)Cycle 2 : Hidden neuron Oaccumulates charge from events applied at cycle 0. If hidden neuron Os potential exceeds its threshold ts, it fires with an inhibitory spike to output neuron On. Bias input neuron Ibfires sending charge to itself and to output neuron On",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_968",
    "chunk_position": 968
  },
  {
    "id": "2401.15212v1",
    "text": ". Bias input neuron Ibfires sending charge to itself and to output neuron On. 4)Cycle 3 : Hidden neuron Oaccumulates charge from events applied at cycle 1. If hidden neuron Os potential exceeds its threshold ts, it fires with an inhibitory spike to output neuron On. Bias input neuron Ibfires sending charge to itself and to output neuron On. 5)Cycle 4 : If no inhibitory spikes arrive at output neuron Onfrom hidden neuron O,Onfires meaning that we do not filter out the event",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_969",
    "chunk_position": 969
  },
  {
    "id": "2401.15212v1",
    "text": ". 5)Cycle 4 : If no inhibitory spikes arrive at output neuron Onfrom hidden neuron O,Onfires meaning that we do not filter out the event. If instead one or more inhibitory units of charge arrive at neuron Onfrom neuron O, the output neuron will not fire, and the event atx,yis filtered out. Filtering out events that are moving faster than the threshold tsrequires that the network run for one additional cycle per event or five total cycles per event. shows an example spike 2 where the event is filtered out, and shows an example spike 2 where the event is filtered out",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_970",
    "chunk_position": 970
  },
  {
    "id": "2401.15212v1",
    "text": ". shows an example spike 2 where the event is filtered out, and shows an example spike 2 where the event is filtered out. In summary, the following resources are required: To filter out events moving slower than speed threshold ts: Neuron Count :(2s 1)2 1neurons Synapse Count :(2s 1)2synapses Runtime per Event : 4 cycles To filter out events moving faster than speed threshold ts: Neuron Count :(2s 1)2 3neurons Synapse Count :(2s 1)2 3synapses Runtime per Event : 5 cycles C",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_971",
    "chunk_position": 971
  },
  {
    "id": "2401.15212v1",
    "text": ". Density-Based Spatial Clustering of Applications with Noise Filter Density-based spatial clustering of applications with noise (DBSCAN) 9 is a spatial clustering algorithm. It works by clustering all data points as either a core point, a border point, or a noise point. It clusters data based on the premise that spatially close data belongs to the same cluster. The clusters are not rigidly shaped and are composed of core points with border points forming the boundary of the cluster",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_972",
    "chunk_position": 972
  },
  {
    "id": "2401.15212v1",
    "text": ". The clusters are not rigidly shaped and are composed of core points with border points forming the boundary of the cluster. For our use case, an event is a data point, and it is classified as either core, border, or noise based on the following: An event is a core event if in its radius epsilon ( ), at least threshold events occur : Example for speed filter shown in 1. Current event of interest is applied to I 1,1at timestep 1. Because more than the speed threshold ts 10 events are applied between timesteps 0 and 1, output Ofires at timestep 3",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_973",
    "chunk_position": 973
  },
  {
    "id": "2401.15212v1",
    "text": ". Because more than the speed threshold ts 10 events are applied between timesteps 0 and 1, output Ofires at timestep 3. The event is not filtered out and passes through the network for downstream processing. Timestep Apply spike to Fire I0,1 I1,0 I1,1 I1,2 I2,0 I2,1 I2,2 O 0 I0,1, I1,0, I1,2, I2,1, I2,2 - - - - - - - - 1 I1,0, I1,1, I1,2, I2,0, I2,1, I2,2 - - - 2 - - - 3 - - - - - - - - : Example for speed filter shown in 1. Current event of interest is applied to I 1,1at timestep 1",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_974",
    "chunk_position": 974
  },
  {
    "id": "2401.15212v1",
    "text": ". Current event of interest is applied to I 1,1at timestep 1. Because less than or to the speed threshold ts 10 events are applied between timesteps 0 and 1, output Odoes not fire. The event is filtered out. Timestep Apply spike to Fire I0,1 I1,0 I1,1 I1,2 I2,0 I2,1 I2,2 O 0 I0,1, I1,0, I1,2, I2,1 - - - - - - - - 1 I1,0, I1,1, I1,2, I2,0, I2,1 - - - - 2 - - - - 3 - - - - - - - - - : Example for speed filter inverse functionality shown in 2. Current event of interest is applied to I 1,1at timestep 1",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_975",
    "chunk_position": 975
  },
  {
    "id": "2401.15212v1",
    "text": ". Current event of interest is applied to I 1,1at timestep 1. A spike is applied to the bias neuron I bat timetstep 0, and it fires into itself and output O nin each successive timestep. Because more than the speed threshold t 7 events are applied between timesteps 0 and 1, hidden neuron Ofires at timestep 3. This fire directly inhibits output O nfrom firing, and the event of interest is filtered out",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_976",
    "chunk_position": 976
  },
  {
    "id": "2401.15212v1",
    "text": ". This fire directly inhibits output O nfrom firing, and the event of interest is filtered out. Timestep Apply spike to Fire Ib I1,0 I1,1 I1,2 I2,0 I2,1 O O n 0 Ib, I1,0, I1,2, I2,1 - - - - - - - - 1 I1,0, I1,1, I1,2, I2,0, I2,1 - - - - 2 - - - 3 - - - - - - - 4 - - - - - - - - : Example for speed filter inverse functionality shown in 2. Current event of interest is applied to I 1,1at time step 1. A spike is applied to the bias neuron I bat timetstep 0, and it fires into itself and output O nin each successive timestep",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_977",
    "chunk_position": 977
  },
  {
    "id": "2401.15212v1",
    "text": ". A spike is applied to the bias neuron I bat timetstep 0, and it fires into itself and output O nin each successive timestep. Because less than or to the speed threshold t 7 events are applied between timesteps 0 and 1, hidden neuron Odoes not fire. Instead, by timestep 4, I bhas contributed enough charge to output O nfor it to fire, allowing the event to be passed through and not filtered out",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_978",
    "chunk_position": 978
  },
  {
    "id": "2401.15212v1",
    "text": ". Timestep Apply spike to Fire Ib I1,0 I1,1 I1,2 I2,0 I2,1 O O n 0 Ib, I1,0, I1,2, I2,1 - - - - - - - - 1 I1,0, I1,1, I1,2 - - - - 2 - - - - - 3 - - - - - - - - 4 - - - - - - - An event is a border event if in its radius epsilon ( ), between one and threshold tevents occur where at least one of the events is a known core event An event is a noise event if neither of the prior conditions are met Our network architecture has been validated against scikitlearns 12 DBSCAN a few different datasets including a few sequences from the DSEC dataset 13",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_979",
    "chunk_position": 979
  },
  {
    "id": "2401.15212v1",
    "text": ". 1) Network Specification: For the network representation of the DBSCAN algorithm, there are only two parameters that contribute to its architecture: 1) The threshold tdor min points value 2) The radius epsilon ddefined as the Chebyshev distance Optionally, we can choose to recall and label noise events,but we argue that it makes more sense to filter them out as noise. Additionally, for our implementation of the DBSCAN , the clusters are not labelled and identified. The events are only classified as either core or border events while noise events are filtered out",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_980",
    "chunk_position": 980
  },
  {
    "id": "2401.15212v1",
    "text": ". The events are only classified as either core or border events while noise events are filtered out. We define two sets of input neurons Ii,jand Ia,b. Each set of input neurons, like with the speed filter network, is composed o2neurons, which results in a total of (2d1)22 input neurons. The input neurons have thresholds t 0 . There are four hidden neurons and two output neurons. The output neuron Ocis the core output neuron and its threshold t 0. The border output neuron is denoted as Oband its threshold t 1",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_981",
    "chunk_position": 981
  },
  {
    "id": "2401.15212v1",
    "text": ". The output neuron Ocis the core output neuron and its threshold t 0. The border output neuron is denoted as Oband its threshold t 1. There are also four hidden neurons denoted as Hi0,3whose thresholds differ and will be explained in a later example that walks through the network activity",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_982",
    "chunk_position": 982
  },
  {
    "id": "2401.15212v1",
    "text": ". The connectivity is defined as follows: 1)Ii,jto H0with a weight of 1 and delay of 0 2)Ii,jto H1with a weight of 1 and delay of 0 3)Ia,bto H3with a weight of 1 and delay of 0 4)H0to H2,H1to Oc,H2to Ob,Octo Oc,H3to Ob with a weight of 1 and delay of 0 5)H1to H2,Octo Obwith a weight of -1 and delay of 0 To better illustrate the connectivity, 3 shows a network that performs the DBSCAN an d value of 1 and a threshold tor min points value of 3. . 3: DBSCAN network architecture with d 1 and threshold td 3",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_983",
    "chunk_position": 983
  },
  {
    "id": "2401.15212v1",
    "text": ". . 3: DBSCAN network architecture with d 1 and threshold td 3. Like with our speed filter network example, lets suppose an event occurs at pixel x,yand time n, and we want to determine whether this event is a core point event, a border point event, or a noise point event. Again, the current events pixel is mapped to input neuron Idj,di, and the other adjacent pixels ranging from xxd, xd, yyd, yd simply map to the input neurons Ij0,2d,i0,2d",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_984",
    "chunk_position": 984
  },
  {
    "id": "2401.15212v1",
    "text": ". Functionally, the inputs Ia,bare to the input neurons Ii,j; however, instead of operating on the event of interest at pixel x,y,Ia,boperates on each event that occurs in the radius dfrom the event at pixel x,y. This is necessary because while inputs Ii,jclassify whether the event at x,yis a core, border, or noise point, the inputs Ia,bmust evaluate whether any of x,ys neighboring events are core point events to validate a border point classification from inputs Ii,jso as to accurately perform the DBSCAN",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_985",
    "chunk_position": 985
  },
  {
    "id": "2401.15212v1",
    "text": ". Additionally, hidden neuron H3has total leak enabled so that at the end of every cycle, if there is any remaining potential on the neuron, it is leaked away so that we may continue processing each neighboring event each cycle without delay. The network activity is roughly detailed below:: Example spike the I i,jinput neurons for 4a. There are no adjacent events, so we omit the the I a,bneurons activity as there is none",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_986",
    "chunk_position": 986
  },
  {
    "id": "2401.15212v1",
    "text": ". There are no adjacent events, so we omit the the I a,bneurons activity as there is none. Timestep Apply spike to Fire I1,1 H0 H1 0 I1,1 - - - 1 - - - 2 - - - - 3 - - - - 4 - - - - 5 - - - - 6 - - - - 1)Cycle 0 : Apply events that occur at pixels xx d, xd, yyd, ydat time nto inputs Ij,i where the event at pixel x,ymaps to input I dj,di. Additionally, apply the first neighboring event x,y to Ia,b where x,y maps to I db,da; for each subsequent cycle, apply the next event in x,ys neighboring list of events to the Ia,binput neurons",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_987",
    "chunk_position": 987
  },
  {
    "id": "2401.15212v1",
    "text": ". 2)Cycle 2 : If hidden neuron H1fires, our event at x,yis a core event. It in turn fires into output neuron Ocwhich will continuously generate output spikes and send spikes with inhibitory charge to output neuron Ob to prevent a border point event classification. If hidden neuron H1does not fire and only H0fires, we have a potential border point event classification that must be corroborated by the inputs Ia,band hidden neuron H3over the course of the next td1 cycles. If neither H0nor H1fire, the event at pixel x,yis classified as noise and filtered out",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_988",
    "chunk_position": 988
  },
  {
    "id": "2401.15212v1",
    "text": ". If neither H0nor H1fire, the event at pixel x,yis classified as noise and filtered out. 3)Cycle 3 to Cycle (min points - 1) : Input neurons Ia,b continue processing events in list of neighboring events xxd, xd, yyd, y. Supposing one of the events occurs at pixel location x,y, that event is applied at Idb,daand events that occur at pixels xxd, xd, yyd, yd are mapped to and applied to Ib,asimilarly to how the event at pixel x,yand its neighboring events in radius d are mapped to and applied to Ij,i. Three more detailed examples are shown in V to IX",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_989",
    "chunk_position": 989
  },
  {
    "id": "2401.15212v1",
    "text": ". Three more detailed examples are shown in V to IX. Two are needed per example (with the exception of the noise classification), one for the I i,jinputs and one for the I a,b inputs. The examples that they illustrate are shown in 4. They show an events classification as either noise, border, or core based on its adjacent activity. This process of classifying events as either core point events, border point events, or noise point events requires threshold (or min points) t4cycles per event",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_990",
    "chunk_position": 990
  },
  {
    "id": "2401.15212v1",
    "text": ". Classifying an event as noise can be accomplished in 3 cycles, classifying an event as a core point can be done in 4 cycles, but classifying an event as a border point event can require up to threshold (or min points) td 4cycles. This is due to the fact that classifying an event : Example spike the I i,jinput neurons for 4b. Since min points 3, and there are a total of 4 events within an epsilon dof pixel x,, both hidden neurons H 0and H 1fire. H 1inhibits neuron H 2from firing and causes core output neuron O cto fire",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_991",
    "chunk_position": 991
  },
  {
    "id": "2401.15212v1",
    "text": ". H 1inhibits neuron H 2from firing and causes core output neuron O cto fire. O cfires every cycle afterward because of its self-edge and continuously inhibits output neuron O bfrom firing, as it still accumulates charge from the I a,bneurons. At the end of simulation, only output O cwould have fired, so the classification would be a core neuron classification",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_992",
    "chunk_position": 992
  },
  {
    "id": "2401.15212v1",
    "text": ". At the end of simulation, only output O cwould have fired, so the classification would be a core neuron classification. Timestep Apply spike to Fire I0,0 I0,2 I1,1 I2,0 H0 H1 H2 Oc Ob 0 I0,0, I0,2, I1,1, I2,0 - - - - - - - - - 1 - - - - - - 2 - - - - - - - - 3 - - - - - - - - - 4 - - - - - - - - - 5 - - - - - - - - - 6 - - - - - - - - - : Example spike the I a,binput neurons for 4b. Since tdor min points 3, theres a maximum of 3 events that will need to be processed by the I a,bneurons. We process the events adjacent to x,yin row-major order excluding event x,y",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_993",
    "chunk_position": 993
  },
  {
    "id": "2401.15212v1",
    "text": ". We process the events adjacent to x,yin row-major order excluding event x,y. Because hidden neuron H 3has total leak enabled, we can process all adjacent events in succession with no downtime. For all three adjacent events, there are only two events to apply at the I a,bneurons. This means that no adjacent event will cause neuron H 3to fire. Timestep Apply spike to Fire I0,0 I0,2 I1,1 I2,0 I2,2 H3 Ob 0 I1,1, I2,2 - - - - - - - 1 I1,1, I2,0 - - - - - 2 I1,1, I0,2 - - - - - 3 - - - - - - 4 - - - - - - - - 5 - - - - - - - - 6 - - - - - - - - : Example spike the I i,jinput neurons for 4c",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_994",
    "chunk_position": 994
  },
  {
    "id": "2401.15212v1",
    "text": ". We apply the two events to the I i,jneurons which causes hidden neuron H 0to fire. Hidden neuron H 1does not fire as its threshold has not been exceeded. H 2then fires sending charge to O bwhich fires because it has already received its supplementary required charge from the I a,bneurons at timestep 3 as is shown in . Timestep Apply spike to Fire I0,2 I1,1 H0 H2 Ob 0 I1,1, I0,2 - - - - - 1 - - - - 2 - - - - - 3 - - - - - 4 - - - - - 5 - - - - - - 6 - - - - - - : Spike the I a,binput neurons for 4c",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_995",
    "chunk_position": 995
  },
  {
    "id": "2401.15212v1",
    "text": ". We process the sole neighboring event, and apply its three adjacent events to the I a,bneurons which causes hidden neuron H 3to fire. Hidden neuron H 3sends priming charge to output neuron O bwhich doesnt fire until cycle 4, which is when it receives its supplemental charge from the I i,jneurons as is shown in . Timestep Apply spike to Fire I0,0 I0,2 I1,1 I2,0 H3 Ob 0 I0,0,I0,2I1,1, I2,0 - - - - - - 1 - - - 2 - - - - - - 3 - - - - - - - 4 - - - - - - 5 - - - - - - - 6 - - - - - - - (a) Example for classification of a noise event. (b) Example for classification of a core event",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_996",
    "chunk_position": 996
  },
  {
    "id": "2401.15212v1",
    "text": ". (b) Example for classification of a core event. (c) Example for classification of a border event. . 4as a border point event requires the parallel classification of at least threshold (or min points) td1neighboring events. The intuition behind this is that if we believe an event at x,y is a border point event, there are threshold (or min points) td1or less events in its radius d. We must then process in parallel up to a possible threshold (or min points) td1 events at the Ia,bneurons to determine if any of them are a core event point",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_997",
    "chunk_position": 997
  },
  {
    "id": "2401.15212v1",
    "text": ". In summary, the following resources are required: Neuron Count :(2d 1)22 6 neurons Synapse Count :(2d 1)23 7 synapses Runtime per Event : threshold (or min points) td 4 cycles III. D ISCUSSION When applying spiking neural networks to neuromorphic hardware, there are several practicalities that should be taken into consideration. One of these is the encoding function used to apply the data to the inputs of the neural network. Fortunately, for event-based camera data, this operation is fairly straightforward",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_998",
    "chunk_position": 998
  },
  {
    "id": "2401.15212v1",
    "text": ". Fortunately, for event-based camera data, this operation is fairly straightforward. An event directly translates to a one-valued spike applied to the appropriate input neuron, regardless of its polarity. Additionally, after processing an event, the networks leftover state needs to be cleared before processing the next event. This soft reset requires a few cycles to reinitialize a networks state before processing the next event. Decoding the output neurons spikes also must be addressed",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_999",
    "chunk_position": 999
  },
  {
    "id": "2401.15212v1",
    "text": ". Decoding the output neurons spikes also must be addressed. For both presented networks, the popular Winner-take-all (WTA) decoding scheme results in the desired network functionalities. In the WTA decoding scheme, whichever output neuron fires the most is the winner, and its classification or action is chosen. Applying one of the filtering networks to a dataset is straightforward. However, when two or more filtering networks are applied successively, it is more complicated",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1000",
    "chunk_position": 1000
  },
  {
    "id": "2401.15212v1",
    "text": ". However, when two or more filtering networks are applied successively, it is more complicated. One option is to time multiplex the observation space and route its inputs through one network, collect that networks outputs off of the neuromorphic chip, and then route the first networks outputs as the second or latter networks inputs. This approach works, but is burdened by data movement costs on and off chip. Instead, we propose compiling the networks together",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1001",
    "chunk_position": 1001
  },
  {
    "id": "2401.15212v1",
    "text": ". This approach works, but is burdened by data movement costs on and off chip. Instead, we propose compiling the networks together. As an example, lets assume we would like to pass all of the events through a speed filter and then a subsequent DBSCAN filter to isolate objects moving at a certain speed, toss out any noise and cluster the events. Because we need to pass every event through a speed filter network before applying it to the DBSCAN filter network, the simplest implementation would require a speed filter network for each input of the DBSCAN filter network",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1002",
    "chunk_position": 1002
  },
  {
    "id": "2401.15212v1",
    "text": ". In trading space for time, the compiled network would be composed of the DBSCAN network preceded by (2d 1)22speed filter networks. In theory, assuming we are filtering out events that are moving slower than the speed filter threshold ts, the runtime of the compiled network would be the sum of both network components give or take a few cycles. Additionally, its possible to avoid duplication of the speed filter network for every DBSCAN filter network (a) 50 ms aggregation of events from the interlaken 00c sequence",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1003",
    "chunk_position": 1003
  },
  {
    "id": "2401.15212v1",
    "text": ". (b) Same as 5a but after the events have been passed through a speed filter and then a DBSCAN filter. The blue events are core events, and the green events are border events. . 5 input neuron in favor of trading time for space. Instead, one speed filter network each for the I i,jand I a,bneurons can be used as long as the input events and inter-network activity are properly time multiplexed through clever use of synaptic delays. We leave the detailed characterization and verification of these compiled network approaches to a latter work",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1004",
    "chunk_position": 1004
  },
  {
    "id": "2401.15212v1",
    "text": ". We leave the detailed characterization and verification of these compiled network approaches to a latter work. 5 shows the output of a 50ms aggregation of events from the interlaken 00c sequence from the DSEC 13 dataset both before ( 5a) and after ( 5b) a speed filter and DBSCAN filter are applied to all of the events in the aggregation. This illustrates a use case of both the speed filter network and DBSCAN filter network together by showing the reduction in overall event noise and the clustering of events",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1005",
    "chunk_position": 1005
  },
  {
    "id": "2401.15212v1",
    "text": ". The parameters used to generate this output for the speed filter network are t s 9 ands 1 using the speed filter inverse functionality (i.e. filter out events that are moving too fast). The parameters used for the DBSCAN network are td 10 andd 3.IV. C ONCLUSIONS Overall, these architectures are scalable and can be adjusted as desired for target hardware. The speed filter network can be used to isolate or filter out events moving at certain speeds with respect to the camera, and the DBSCAN filter network can be used to spatially cluster events while getting rid of noise",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1006",
    "chunk_position": 1006
  },
  {
    "id": "2401.15212v1",
    "text": ". Furthermore, these networks can be used together serially as a system of networks that performs a complex functionality. The end result is a highly scalable, customizable, and parallelizable neuromorphic system capable of event denoising and clustering. REFERENCES 1 S. B. Shrestha and G. Orchard, Slayer: Spike layer error reassignment in time, Advances in neural information processing systems , 31, 2018. 2 C. D. Schuman, J. P. Mitchell, R. M. Patton, T. E. Potok, and J. S",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1007",
    "chunk_position": 1007
  },
  {
    "id": "2401.15212v1",
    "text": ". 2 C. D. Schuman, J. P. Mitchell, R. M. Patton, T. E. Potok, and J. S. Plank, Evolutionary optimization for neuromorphic systems, in NICE: Neuro-Inspired Computational Elements Workshop , 2020. 3 M. A. Coletti, E. O. Scott, and J. K. Bassett, Library for evolutionary in python (leap), in Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion , 2020, 1571 1579. 4 J. J. M. Reynolds, J. S. Plank, and C. D",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1008",
    "chunk_position": 1008
  },
  {
    "id": "2401.15212v1",
    "text": ". 4 J. J. M. Reynolds, J. S. Plank, and C. D. Schuman, Intelligent reservoir generation for liquid state machines using evolutionary optimization, in 2019 International Joint Conference on Neural Networks (IJCNN) , 2019, 18. 5 W. Severa, C. M. Vineyard, R. Dellana, S. J. Verzi, and J. B. Aimone, Training deep neural networks for binary communication with the whetstone method, Nature Machine Intelligence , 1, no. 2, 8694, Feb 2019. Online. Available: 1038s42256-018-0015-y 6 W. Severa, O. Parekh, K. D. Carlson, C. D. James, and J. B",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1009",
    "chunk_position": 1009
  },
  {
    "id": "2401.15212v1",
    "text": ". 2, 8694, Feb 2019. Online. Available: 1038s42256-018-0015-y 6 W. Severa, O. Parekh, K. D. Carlson, C. D. James, and J. B. Aimone, Spiking network for scientific computing, in 2016 IEEE International Conference on Rebooting Computing (ICRC) , 2016, 18. 7 J. S. Plank, C. Zheng, C. D. Schuman, and C. Dean, Spiking neuromorphic networks for binary tasks, in International Conference on Neuromorphic Computing Systems (ICONS) . ACM, 2021, 18. 8 C. P. Rizzo, C. D. Schuman, and J. S",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1010",
    "chunk_position": 1010
  },
  {
    "id": "2401.15212v1",
    "text": ". ACM, 2021, 18. 8 C. P. Rizzo, C. D. Schuman, and J. S. Plank, Neuromorphic downsampling of event-based camera output, in Proceedings of the 2023 Annual Neuro-Inspired Computational Elements Conference . ACM, April 2023. Online. Available: 11453584954.3584962 9 M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al. , A density-based discovering clusters in large spatial databases with noise, inkdd, 96, no. 34, 1996, 226231. 10 S. Song, K. D. Miller, and L. F. Abbott, Competitive hebbian learning through spike-timing-dependent synaptic plasticity, Nature Neuroscience , 3, no. 9, 919926, Sep 2000",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1011",
    "chunk_position": 1011
  },
  {
    "id": "2401.15212v1",
    "text": ". F. Abbott, Competitive hebbian learning through spike-timing-dependent synaptic plasticity, Nature Neuroscience , 3, no. 9, 919926, Sep 2000. Online. Available: 11 M. Nagaraj, C. M. Liyanagedera, and K. Roy, Dotie-detecting objects through temporal isolation of events using a spiking architecture, in2023 IEEE International Conference on Robotics and Automation (ICRA) . IEEE, 2023, 48584864. 12 F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg et al",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1012",
    "chunk_position": 1012
  },
  {
    "id": "2401.15212v1",
    "text": ". 12 F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg et al. , Scikit-learn: Machine learning in python, the Journal of machine Learning research , 12, 28252830, 2011. 13 M. Gehrig, W. Aarents, D. Gehrig, and D. Scaramuzza, Dsec: A stereo event camera dataset for driving scenarios, IEEE Robotics and Automation Letters , 2021.",
    "source": "arXiv",
    "chunk_id": "2401.15212v1_1013",
    "chunk_position": 1013
  },
  {
    "id": "2205.04263v2",
    "text": "ar Xiv:2205.04263v2 eess.SP 1 Jun 2022Spiking Neural Network for IMDD Optical Communication Elias Arnold1,, Georg B ocherer2,, Eric M uller1, Philipp Spilger1, Johannes Schemmel1, Stefano Calabr o2, Maxim Kuschnerov2 1Electronic Visio, Kirchho-Institute for Physics, He idelberg University, Germany 2Huawei Technologies Duesseldorf Gmb H, Munich Research Cen ter, Germany Abstract A spiking neural network (SNN) model suitable for electronic neuromorphic hardware is designed for an IMDD link. The SNN achieves the s ame bit-error-rate as an articial neural network, outperforming linear ion",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1014",
    "chunk_position": 1014
  },
  {
    "id": "2205.04263v2",
    "text": ". The SNN achieves the s ame bit-error-rate as an articial neural network, outperforming linear ion. 1 Introduction Low cost and low power optical transceivers are indispensable for s upporting the exponentially growing data center trac caused by cloud-based services. The h igh power consumption of digital signal processing (DSP) has motivated research on moving parts of the receiver DSP to an analog lower power frontend",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1015",
    "chunk_position": 1015
  },
  {
    "id": "2205.04263v2",
    "text": ". For instance, photonic neuromorphic comp uting 1 has been studied recently, e.g., for compensating for chromatic dispersion (CD) and nonlinear impairments in short reach optical transmission 2,3. An alternative solution is analog electronic neuromorphic computing, implementing SNNs 4 in analog hardware 5 by mimicking the basic operation principles of the human brain, thereby adopting the brains unchallenged powe r eciency. SNNs are applied in 6 for an inference task on a spectrogram in ber-optic distributed acoustic sensing",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1016",
    "chunk_position": 1016
  },
  {
    "id": "2205.04263v2",
    "text": ". SNNs are applied in 6 for an inference task on a spectrogram in ber-optic distributed acoustic sensing. Recently, in-the-loop training of SNNs on analog hardware has achieved state -of-the-art performance in inference tasks 7. Despite electronics operating slower than photonics, electronic h ardware enables higher scalability and thus greater throughput through parallelizat ion, making it a suitable choice for energy-ecient signal processing. An important aspect to be analyzed is whether SNNs in analog electronic hardware support the accuracy required by com munication systems",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1017",
    "chunk_position": 1017
  },
  {
    "id": "2205.04263v2",
    "text": ". An important aspect to be analyzed is whether SNNs in analog electronic hardware support the accuracy required by com munication systems. To assess the accuracyof SNNs, we design and demapp ing using SNNs suitable for a hardware implementation on the Brain Scale S-2 (BSS-2) system 5. We evaluate our SNN in a software simulation for the detection of a 4-level pulse amplitude modulation (PAM4) signal for an intensity modulationdirect detection (IMDD) link, impaired by CD and additive white Gaussian nois",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1018",
    "chunk_position": 1018
  },
  {
    "id": "2205.04263v2",
    "text": ". Our SNN achieves the bit error rate (BER) of an articial neural network (ANN), outperforming a digital linear minimum mean square error (LMMSE) . 2 and Demapping using Spiking Neural Networks For and demapping, we consider an SNN with a single hidden layer, consisting of 40 spikingleaky-integrate and re (LIF) neurons 4, Sec. 1.3, and an output layer constituted by four non-spiking leaky-integrate (LI) 4, Sec. 1.3 readout neurons. This architecture ts in size on the BSS-2 system 5",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1019",
    "chunk_position": 1019
  },
  {
    "id": "2205.04263v2",
    "text": ". 1.3 readout neurons. This architecture ts in size on the BSS-2 system 5. Each LIF neuron jmaintains an internal membrane state vjdescribed by the 1 ordinary dierential mv (vvleak)I with I N1summationdisplay i0summationdisplay sspikesi-th neuron Wji(s i)expparenleftbigg s i synparenrightbigg , (1) integrating synaptic input I, caused by pre-synaptic events s i, onto its membrane. As the membrane potential exceeds a threshold , the neuron emits a post-synaptic spike a time s j, after which it is set to a reset potential vr. LI neurons exhibit the same dynamics, without the ability to spike",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1020",
    "chunk_position": 1020
  },
  {
    "id": "2205.04263v2",
    "text": ". LI neurons exhibit the same dynamics, without the ability to spike. The parameters synandmare the time constants of the synaptic current and the membrane potential, respectively. Areceivedsample ytand itsntap2predecessorsand successor aretranslatedto 10 input spike events per sample by a spike encoder (see . 1), potentially replacing power-hungry analog-to-digital conversion (ADC) in hardware. To this end, each input neuron emits a spike at times igiven by the scaled log-distance 8 to a reference point i, assigned to each input neuron",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1021",
    "chunk_position": 1021
  },
  {
    "id": "2205.04263v2",
    "text": ". To this end, each input neuron emits a spike at times igiven by the scaled log-distance 8 to a reference point i, assigned to each input neuron. The input sample ytgets classied with the label k 0,1,2,3of the output neuron with the maximum membrane value v over the considered time frame. Hence, the network learns to place hidden spike events in time, such that the readout traces are adjusted appropriately. For training our SNNs we rely on backpropagation through time (BPTT) with the Adam optimizer and surrogate gradients (Super Spike 8) to account for the discontinuity of spiking LIF neurons",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1022",
    "chunk_position": 1022
  },
  {
    "id": "2205.04263v2",
    "text": ". Note that our simulations are implemented in hxtorch 9, also supporting execution on the BSS-2 system. 3 Results and Conclusions In .2A, we display a simulated IMDD link. Bits are mapped to a PAM4 constella tion, the signal is upsampled and ltered by a root-raised-cosine (RRC). The signal is then shifted to the positive and CD is applied. At the receiver, a PD squares the sig nal and AWGN is added. The signal is then RRC ltered and downsampled. The resultin g signal yis and demapped",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1023",
    "chunk_position": 1023
  },
  {
    "id": "2205.04263v2",
    "text": ". At the receiver, a PD squares the sig nal and AWGN is added. The signal is then RRC ltered and downsampled. The resultin g signal yis and demapped. As reference, we use a digital 17 tap LMMSE r, followed by a demapper with BER optimized decision boundaries, see . 2D (right), and ANNs with one and two hidden layers, respectively, see . 2C. In . 2D (left) we see that joint and demapping by a 17 tap SNN outperforms the LMMSE, and performs as well as th e 17 tap ANN 1, which has 1 hidden layer with 40 neurons, similar to the SNN",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1024",
    "chunk_position": 1024
  },
  {
    "id": "2205.04263v2",
    "text": ". The reference sch emes and the SNN were trained using supervised learning",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1025",
    "chunk_position": 1025
  },
  {
    "id": "2205.04263v2",
    "text": ". By means of software simulation, we have shown that an SNN suitable for analog electronic hardware can eciently compensate im pairments in a simulated t ytntap2 yt ytntap2y0 0 30msspike encoding input spike trains hidden LIF neuronsoutput LI neuronss ilo) withd Ayti and,Aconst.0 20ms 0123 0 25ms0.51.0 vka.u.Wih ji Who kj max(vk) and argmaxk3mapsto10 2mapsto11 1mapsto01 0mapsto00B1B2 1: SNN demapper decision chain 2 B1B200mapsto 3 01mapsto 1 11mapsto1 10mapsto3up RRC Chromatic Dispersion2RRC down DemapperB1B2ybias Z rollo 0.2 rollo 0 .2photo diode A baudrate 100GBd wavelength 1270nm",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1026",
    "chunk_position": 1026
  },
  {
    "id": "2205.04263v2",
    "text": "1 11mapsto1 10mapsto3up RRC Chromatic Dispersion2RRC down DemapperB1B2ybias Z rollo 0.2 rollo 0 .2photo diode A baudrate 100GBd wavelength 1270nm dispersion 5psnmkm berlength 5km B 420 2 4 yt010002000300040005000 Counts1 tap LMMSE 17 tap LMMSE demapper decision boundaries 16 18 20 22 24 10log10(2)104103102BER1 tap LMMSE 17 tap LMMSE 17 tap ANN 1 17 tap SNN 17 tap ANN 2 KP4 FEC threshold D Net Hidden layer width Activation SNN 40 LIF ANN140 Re LU ANN234, 10 Re LUC 2: (A)Simulated IMDD link",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1027",
    "chunk_position": 1027
  },
  {
    "id": "2205.04263v2",
    "text": ". (B)IMDD parameters. (C)NN parameters. (D)Left: BER results for transmission of PAM4 over the simulated IMDD li nk. Right: Histogram of the linear MMSE output. IMDD link. In ongoing research, we implement the proposed SNN on t he BSS-2 system, with the aim to reproduce the reported results on analog hardware",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1028",
    "chunk_position": 1028
  },
  {
    "id": "2205.04263v2",
    "text": ". Funding The contributions of the Electronic Visio group1have been supported by the EC Horizon 2020 Framework Programme under grant agreements 785907 (HBP SGA 2) and 945539 (HBP SGA3), the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy EXC 21811-390900948(the Heidelberg STRU CTURES Excellence Cluster), the Helmholtz Association Initiative and Networking Fund Advanced C omputing Architectures (ACA) under Project SO-092. References 1 B. J",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1029",
    "chunk_position": 1029
  },
  {
    "id": "2205.04263v2",
    "text": ". References 1 B. J. Shastri et al., Photonics for articial intelligence and neuromorphic computing, Nature Photonics , 15, no. 2, 2021. 2 S. Li et al., Micro-ring resonator based photonic reservoir computing for P AM , IEEE Photonics Technology Letters , 33, no. 18, 978981, 2021. 3 S. M. Ranzini et al., Experimental investigation of optoelectronic receiver with rese rvoir computing in short reach optical ber communications, Journal of Lightwave Technology , 39, no. 8, 24602467, 2021. 4 W. Gerstner et al.,Neuronal dynamics: From single neurons to networks and mode ls of cognition",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1030",
    "chunk_position": 1030
  },
  {
    "id": "2205.04263v2",
    "text": ". 8, 24602467, 2021. 4 W. Gerstner et al.,Neuronal dynamics: From single neurons to networks and mode ls of cognition. Cambridge University Press, 2014. 5 C. Pehle et al., The Brain Scale S-2 accelerated neuromorphic system with hybrid plasticity, Frontiers in Neuroscience , 16, 2022. 6 H.Wu et al., Improvedgeneralizationinsignalidenticationwithunsupervised spikingneuron networks for ber-optic distributed acoustic sensor, Journal of Lightwave Technology , 2022. 7 B. Cramer et al., Surrogate gradients for analog neuromorphic computing, Proc. National Academy of Sciences , 119, no. 4, 2022",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1031",
    "chunk_position": 1031
  },
  {
    "id": "2205.04263v2",
    "text": ". 7 B. Cramer et al., Surrogate gradients for analog neuromorphic computing, Proc. National Academy of Sciences , 119, no. 4, 2022. 3 8 E. O. Neftci et al., Surrogategradientlearningin spikingneural networks: Bringing the power of gradient-based optimization to spiking neural networks, IEEE Signal Processing Magazine , 36, no. 6, 5163, 2019. 9 E. M uller et al., A scalable approach to modeling on accelerated neuromorphic har dware, ar Xiv 2203.11102 , Feb. 2022. to Frontiers in Neuromorphic Engineering. 4 This trace.png is available in png format from:",
    "source": "arXiv",
    "chunk_id": "2205.04263v2_1032",
    "chunk_position": 1032
  }
]