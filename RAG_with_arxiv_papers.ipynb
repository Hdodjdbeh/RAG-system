{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_dC4DBznw6Zy"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv -q\n",
        "!pip install sentence-transformers faiss-cpu transformers torch -q\n",
        "!pip install arxiv pdfplumber tqdm python-dotenv fitz tools -q\n",
        "!pip install PyPDF2 -q\n",
        "!pip install googletrans -q\n",
        "!pip install ragas -q\n",
        "!pip install torch -q\n",
        "!pip install langchain_community -q\n",
        "!pip install rouge bert_score -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pBxdZJ-U2su5"
      },
      "outputs": [],
      "source": [
        "# if u need to clear papers, then uncomment code below\n",
        "# ///////////////////////////////////////////////////\n",
        "\n",
        "# import os\n",
        "# def delete_files_in_folder(folder_path):\n",
        "#     for filename in os.listdir(folder_path):\n",
        "#         file_path = os.path.join(folder_path, filename)\n",
        "#         try:\n",
        "#             if os.path.isfile(file_path):\n",
        "#                 os.remove(file_path)\n",
        "#         except Exception as e:\n",
        "#             print(f'Ошибка при удалении файла {file_path}: {e}')\n",
        "\n",
        "# # Пример вызова\n",
        "# delete_files_in_folder(\"papers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olVgZT6lvzUf",
        "outputId": "ca21223d-9e30-47e0-b17d-a90b3c393f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 10it [00:44,  4.43s/it]\n"
          ]
        }
      ],
      "source": [
        "import arxiv\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "client = arxiv.Client()\n",
        "def download_articles(keywords, client, max_results=100, output_dir=\"papers\"):\n",
        "    # Создаем директорию для сохранения PDF\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Формируем запрос\n",
        "    query = \" AND \".join([f'abs:\"{kw}\"' for kw in keywords])\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "    results = client.results(search)\n",
        "    # Скачивание статей\n",
        "    for result in tqdm(results, desc=\"Downloading\"):\n",
        "        try:\n",
        "            result.download_pdf(dirpath=output_dir, filename=f\"{result.get_short_id()}.pdf\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при загрузке {result.entry_id}: {e}\")\n",
        "\n",
        "# Пример вызова\n",
        "keywords = [\"spiking neural network\"]\n",
        "download_articles(keywords, client, max_results=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iJ71kzIwf9w",
        "outputId": "2c0e74d4-80fd-44c9-c642-04ee2587d6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PDFs:  40%|████      | 4/10 [00:03<00:06,  1.05s/it]/usr/local/lib/python3.11/dist-packages/PyPDF2/_cmap.py:142: PdfReadWarning: Advanced encoding /UniGB-UTF16-H not implemented yet\n",
            "  warnings.warn(\n",
            "Processing PDFs: 100%|██████████| 10/10 [00:23<00:00,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл articles.json успешно создан!\n",
            "ar Xiv:2205.04263v2 eess.SP 1 Jun 2022Spiking Neural Network for IMDD Optical Communication Elias Arnold1,, Georg B ocherer2,, Eric M uller1, Philipp Spilger1, Johannes Schemmel1, Stefano Calabr o2, Maxim Kuschnerov2 1Electronic Visio, Kirchho-Institute for Physics, He idelberg University, Germany 2Huawei Technologies Duesseldorf Gmb H, Munich Research Cen ter, Germany Abstract A spiking neural network (SNN) model suitable for electronic neuromorphic hardware is designed for an IMDD link. The SNN achieves the s ame bit-error-rate as an articial neural network, outperforming linear ion. 1 Introduction Low cost and low power optical transceivers are indispensable for s upporting the exponentially growing data center trac caused by cloud-based services. The h igh power consumption of digital signal processing (DSP) has motivated research on moving parts of the receiver DSP to an analog lower power frontend. For instance, photonic neuromorphic comp uting 1 has been studied recently, e.g., for compensating for chromatic dispersion (CD) and nonlinear impairments in short reach optical transmission 2,3. An alternative solution is analog electronic neuromorphic computing, implementing SNNs 4 in analog hardware 5 by mimicking the basic operation principles of the human brain, thereby adopting the brains unchallenged powe r eciency. SNNs are applied in 6 for an inference task on a spectrogram in ber-optic distributed acoustic sensing. Recently, in-the-loop training of SNNs on analog hardware has achieved state -of-the-art performance in inference tasks 7. Despite electronics operating slower than photonics, electronic h ardware enables higher scalability and thus greater throughput through parallelizat ion, making it a suitable choice for energy-ecient signal processing. An important aspect to be analyzed is whether SNNs in analog electronic hardware support the accuracy required by com munication systems. To assess the accuracyof SNNs, we design and demapp ing using SNNs suitable for a hardware implementation on the Brain Scale S-2 (BSS-2) system 5. We evaluate our SNN in a software simulation for the detection of a 4-level pulse amplitude modulation (PAM4) signal for an intensity modulationdirect detection (IMDD) link, impaired by CD and additive white Gaussian nois. Our SNN achieves the bit error rate (BER) of an articial neural network (ANN), outperforming a digital linear minimum mean square error (LMMSE) . 2 and Demapping using Spiking Neural Networks For and demapping, we consider an SNN with a single hidden layer, consisting of 40 spikingleaky-integrate and re (LIF) neurons 4, Sec. 1.3, and an output layer constituted by four non-spiking leaky-integrate (LI) 4, Sec. 1.3 readout neurons. This architecture ts in size on the BSS-2 system 5. Each LIF neuron jmaintains an internal membrane state vjdescribed by the 1 ordinary dierential mv (vvleak)I with I N1summationdisplay i0summationdisplay sspikesi-th neuron Wji(s i)expparenleftbigg s i synparenrightbigg , (1) integrating synaptic input I, caused by pre-synaptic events s i, onto its membrane. As the membrane potential exceeds a threshold , the neuron emits a post-synaptic spike a time s j, after which it is set to a reset potential vr. LI neurons exhibit the same dynamics, without the ability to spike. The parameters synandmare the time constants of the synaptic current and the membrane potential, respectively. Areceivedsample ytand itsntap2predecessorsand successor aretranslatedto 10 input spike events per sample by a spike encoder (see . 1), potentially replacing power-hungry analog-to-digital conversion (ADC) in hardware. To this end, each input neuron emits a spike at times igiven by the scaled log-distance 8 to a reference point i, assigned to each input neuron. The input sample ytgets classied with the label k 0,1,2,3of the output neuron with the maximum membrane value v over the considered time frame. Hence, the network learns to place hidden spike events in time, such that the readout traces are adjusted appropriately. For training our SNNs we rely on backpropagation through time (BPTT) with the Adam optimizer and surrogate gradients (Super Spike 8) to account for the discontinuity of spiking LIF neurons. Note that our simulations are implemented in hxtorch 9, also supporting execution on the BSS-2 system. 3 Results and Conclusions In .2A, we display a simulated IMDD link. Bits are mapped to a PAM4 constella tion, the signal is upsampled and ltered by a root-raised-cosine (RRC). The signal is then shifted to the positive and CD is applied. At the receiver, a PD squares the sig nal and AWGN is added. The signal is then RRC ltered and downsampled. The resultin g signal yis and demapped. As reference, we use a digital 17 tap LMMSE r, followed by a demapper with BER optimized decision boundaries, see . 2D (right), and ANNs with one and two hidden layers, respectively, see . 2C. In . 2D (left) we see that joint and demapping by a 17 tap SNN outperforms the LMMSE, and performs as well as th e 17 tap ANN 1, which has 1 hidden layer with 40 neurons, similar to the SNN. The reference sch emes and the SNN were trained using supervised learning. By means of software simulation, we have shown that an SNN suitable for analog electronic hardware can eciently compensate im pairments in a simulated t ytntap2 yt ytntap2y0 0 30msspike encoding input spike trains hidden LIF neuronsoutput LI neuronss ilo) withd Ayti and,Aconst.0 20ms 0123 0 25ms0.51.0 vka.u.Wih ji Who kj max(vk) and argmaxk3mapsto10 2mapsto11 1mapsto01 0mapsto00B1B2 1: SNN demapper decision chain 2 B1B200mapsto 3 01mapsto 1 11mapsto1 10mapsto3up RRC Chromatic Dispersion2RRC down DemapperB1B2ybias Z rollo 0.2 rollo 0 .2photo diode A baudrate 100GBd wavelength 1270nm dispersion 5psnmkm berlength 5km B 420 2 4 yt010002000300040005000 Counts1 tap LMMSE 17 tap LMMSE demapper decision boundaries 16 18 20 22 24 10log10(2)104103102BER1 tap LMMSE 17 tap LMMSE 17 tap ANN 1 17 tap SNN 17 tap ANN 2 KP4 FEC threshold D Net Hidden layer width Activation SNN 40 LIF ANN140 Re LU ANN234, 10 Re LUC 2: (A)Simulated IMDD link. (B)IMDD parameters. (C)NN parameters. (D)Left: BER results for transmission of PAM4 over the simulated IMDD li nk. Right: Histogram of the linear MMSE output. IMDD link. In ongoing research, we implement the proposed SNN on t he BSS-2 system, with the aim to reproduce the reported results on analog hardware. Funding The contributions of the Electronic Visio group1have been supported by the EC Horizon 2020 Framework Programme under grant agreements 785907 (HBP SGA 2) and 945539 (HBP SGA3), the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy EXC 21811-390900948(the Heidelberg STRU CTURES Excellence Cluster), the Helmholtz Association Initiative and Networking Fund Advanced C omputing Architectures (ACA) under Project SO-092. References 1 B. J. Shastri et al., Photonics for articial intelligence and neuromorphic computing, Nature Photonics , 15, no. 2, 2021. 2 S. Li et al., Micro-ring resonator based photonic reservoir computing for P AM , IEEE Photonics Technology Letters , 33, no. 18, 978981, 2021. 3 S. M. Ranzini et al., Experimental investigation of optoelectronic receiver with rese rvoir computing in short reach optical ber communications, Journal of Lightwave Technology , 39, no. 8, 24602467, 2021. 4 W. Gerstner et al.,Neuronal dynamics: From single neurons to networks and mode ls of cognition. Cambridge University Press, 2014. 5 C. Pehle et al., The Brain Scale S-2 accelerated neuromorphic system with hybrid plasticity, Frontiers in Neuroscience , 16, 2022. 6 H.Wu et al., Improvedgeneralizationinsignalidenticationwithunsupervised spikingneuron networks for ber-optic distributed acoustic sensor, Journal of Lightwave Technology , 2022. 7 B. Cramer et al., Surrogate gradients for analog neuromorphic computing, Proc. National Academy of Sciences , 119, no. 4, 2022. 3 8 E. O. Neftci et al., Surrogategradientlearningin spikingneural networks: Bringing the power of gradient-based optimization to spiking neural networks, IEEE Signal Processing Magazine , 36, no. 6, 5163, 2019. 9 E. M uller et al., A scalable approach to modeling on accelerated neuromorphic har dware, ar Xiv 2203.11102 , Feb. 2022. to Frontiers in Neuromorphic Engineering. 4 This trace.png is available in png format from:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# import pdfplumber  # склеивает слова\n",
        "import json\n",
        "import logging\n",
        "import PyPDF2\n",
        "import re\n",
        "from googletrans import Translator\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Удаление формул в $...$ и $$...$$\n",
        "    text = re.sub(r\"\\$.*?\\$\", \"\", text, flags=re.DOTALL) # DOTALL\n",
        "    text = re.sub(r\"\\$\\$.*?\\$\\$\", \"\", text, flags=re.DOTALL) # DOTALL\n",
        "    text = re.sub(r\"\\b(fig\\w*|pic\\w*)\\b\", '', text, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    # Удаление URL и спецсимволов\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s.,;:!?()-]\", \"\", text)\n",
        "    text = re.sub(r\"\\b(fig\\w*|pic\\w*)\\b\", '', text, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    text = re.sub(r\"[a-zA-Z]\\([^\\)]*\\)\", \"\", text)\n",
        "    # Вставить пробелы между слитными словами (буква + заглавная буква)\n",
        "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
        "    # Удалить спецсимволы, кроме базовых знаков препинания\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s.,;:!?()-]\", \"\", text)\n",
        "    # Удалить лишние пробелы\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
        "    text = re.sub(r'(Section|Appendix|Fig\\.?)\\s+[IVXLCDM0-9]+', '', text)\n",
        "    # Удаление библиографических ссылок\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "\n",
        "    #касательно научных текстов очистка\n",
        "\n",
        "    text = re.sub(r'\\$.*?\\$', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'\\\\begin{equation}.*?\\\\end{equation}', '', text, flags=re.DOTALL)\n",
        "\n",
        "    # Удаление ссылок на структурные элементы\n",
        "    text = re.sub(\n",
        "        r'\\b(?:Figure|Fig|Table|Equation|Eq|Section|Appendix|Chapter|Algorithm|Code)\\s*[A-Za-z0-9]+\\b',\n",
        "        '',\n",
        "        text,\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    # Удаление библиографических ссылок (все форматы)\n",
        "    text = re.sub(r'\\[[\\d,-]+\\]', '', text)  # [1], [2-5]\n",
        "    text = re.sub(r'\\([A-Za-z]+\\s+et\\s+al\\.?,?\\s?\\d{4}\\)', '', text)  # (Smith et al., 2020)\n",
        "\n",
        "    # Удаление технических артефактов\n",
        "    text = re.sub(\n",
        "        r'\\b(?:arxiv|doi|issn|isbn|vol|pp|pages?|http|https|www\\.|preprint|submitted|version)\\b[^\\s]*',\n",
        "        '',\n",
        "        text,\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "    # Удаление подписей к рисункам/таблицам\n",
        "    text = re.sub(r'^\\s*(Caption|Source|Note):.*$', '', text, flags=re.IGNORECASE|re.MULTILINE)\n",
        "\n",
        "    # Обработка специальных символов\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?%()\\-–/]', '', text)  # Сохраняем основные знаки препинания\n",
        "\n",
        "    # Удаление LaTeX-команд\n",
        "    text = re.sub(r'\\\\[a-z]+(\\{[^}]+\\})?', '', text)\n",
        "\n",
        "    # Удаление маркеров перечисления\n",
        "    text = re.sub(r'^\\s*[\\d•■♦➢]+[\\s.)]*', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Нормализация пробелов и переносов\n",
        "    text = re.sub(r'(?<=\\w)-\\s+(?=\\w)', '', text)  # Соединение перенесенных слов\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    text = \" \".join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "def process_pdfs(input_dir=\"papers\", output_file=\"articles.json\"):\n",
        "    articles = []\n",
        "    if not os.path.exists(input_dir):\n",
        "        print(f\"Ошибка: директория {input_dir} не найдена!\")\n",
        "        return\n",
        "    for filename in tqdm(os.listdir(input_dir), desc=\"Processing PDFs\"):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(input_dir, filename)\n",
        "            try:\n",
        "                text = extract_text_from_pdf(pdf_path)\n",
        "                text = clean_text(text)\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка обработки {filename}: {e}\")\n",
        "            articles.append({\n",
        "                    \"id\": filename.replace(\".pdf\", \"\"),\n",
        "                    \"text\": text,\n",
        "                    \"source\": \"arXiv\"\n",
        "                })\n",
        "    try:\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(articles, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Файл {output_file} успешно создан!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при сохранении JSON: {e}\")\n",
        "    # Сохранение в JSON\n",
        "    return text\n",
        "\n",
        "last_text = process_pdfs()\n",
        "print(last_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyPdf2 работает лучше для научных статей в пдф формате чем классический pdfminer"
      ],
      "metadata": {
        "id": "6TuwhRCQz3Ib"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "94beAu_Od77N"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Загрузка данных\n",
        "with open(\"articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Создаем объекты Document с метаданными\n",
        "documents = []\n",
        "for item in data:\n",
        "    doc = Document(\n",
        "        page_content=item[\"text\"],\n",
        "        metadata={\n",
        "            \"id\": item[\"id\"],\n",
        "            \"source\": item[\"source\"]\n",
        "        }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "# Настройка сплиттера\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=600,       # Размер чанка в символах\n",
        "    chunk_overlap=150,    # Перекрытие между чанками\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Разбиваем на чанки\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Сохраняем результат\n",
        "output_data = []\n",
        "MIN_CHUNK_LENGTH = 50\n",
        "for i, chunk in enumerate(chunks):\n",
        "    if len(chunk.page_content) >= MIN_CHUNK_LENGTH:\n",
        "      output_data.append({\n",
        "          \"id\": chunk.metadata[\"id\"],\n",
        "          \"text\": chunk.page_content,\n",
        "          \"source\": chunk.metadata[\"source\"],\n",
        "          \"chunk_id\": f\"{chunk.metadata['id']}_{len(output_data)}\",\n",
        "          \"chunk_position\": i  # Уникальный ID чанка\n",
        "      })\n",
        "\n",
        "with open(\"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYAuSyksgKej",
        "outputId": "aacb61bf-c19a-4653-cc62-0591e6ae00a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего статей: 10\n",
            "Всего чанков: 1033\n",
            "\n",
            "Пример чанка:\n",
            "ID: 2303.10780v2_0\n",
            "Текст: A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best ...\n"
          ]
        }
      ],
      "source": [
        "print(f\"Всего статей: {len(data)}\")\n",
        "print(f\"Всего чанков: {len(output_data)}\")\n",
        "print(\"\\nПример чанка:\")\n",
        "print(f\"ID: {output_data[0]['chunk_id']}\")\n",
        "print(f\"Текст: {output_data[0]['text'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "88b59e7776034c28a5cc7cc11cdf8406",
            "0b91811b37b149a2bdd13d2217c914ac",
            "761195530f724b6cb2e565a453de53f9",
            "25956a426be24ff69f18d37780a6cbc4",
            "2e40bac2c3014512a0ef50aed89ee0ff",
            "d9bd4c4d0ee34abf85b5b5212ea70d09",
            "4f471c51b8394ae986f87e2763776550",
            "ca4a6a7230da48da9224d14a34937a48",
            "363c57ecef72400d9faf1b828698217e",
            "8e0ef26383844c97982c19893760d34d",
            "085222aafa35420c90e324aefafa8c40"
          ]
        },
        "id": "wULbWWIbgkuR",
        "outputId": "e5e3afb2-385b-4ab0-d127-bc07129caa92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88b59e7776034c28a5cc7cc11cdf8406"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размерность эмбеддингов: (1033, 512)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Загрузка чанков из JSON\n",
        "with open(\"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "texts = [chunk[\"text\"] for chunk in chunks]\n",
        "metadatas = [{\"id\": chunk[\"id\"], \"source\": chunk[\"source\"], \"chunk_id\": chunk[\"chunk_id\"], \"chunk_position\": chunk[\"chunk_position\"]} for chunk in chunks]\n",
        "\n",
        "# Загрузка модели для эмбеддингов (1B параметров)\n",
        "model = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v2\", device=\"cuda\")  # Для GPU: device=\"cuda\"\n",
        "\n",
        "# Преобразование текстов в векторыремонт ноутбуков москва\n",
        "embeddings = model.encode(\n",
        "    texts,\n",
        "    batch_size=32,  # Оптимизация для больших данных\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "print(f\"Размерность эмбеддингов: {embeddings.shape}\")  # (num_chunks, 768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QcRM0kbyiZeK"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "# Создание индекса\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)  # Индекс с L2-метрикой\n",
        "\n",
        "# Добавление векторов в индекс\n",
        "index.add(embeddings)\n",
        "\n",
        "# Сохранение индекса\n",
        "faiss.write_index(index, \"bio_articles.index\")\n",
        "\n",
        "# Сохранение маппинга id -> метаданные\n",
        "id_to_metadata = {i: md for i, md in enumerate(metadatas)}\n",
        "with open(\"id_to_metadata.json\", \"w\") as f:\n",
        "    json.dump(id_to_metadata, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jYefkRWFi1uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb98ede-34ef-46ca-8a82-4874e1f369fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Топ-5 результатов:\n",
            "ID: 2308.08218v2\n",
            "Текст: . How to describe neuronal activity: Spikes, rates, or assemblies? In J. Cowan, G. Tesauro, and J. Alspector, editors, NIPS 1993 , volume 6. MorganKaufmann, 1993. Wulfram Gerstner, Werner M. Kistler, Richard Naud, and Liam Paninski. Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition . Cambridge University Press, 2014. Paul W. Goldberg and Mark R. Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. Machine Learning , 18(2):1...\n",
            "\n",
            "ID: 2308.08218v2\n",
            "Текст: . TDSNN: From deep neural networks to deep spike neural networks with temporal-coding. In AAAI 2019 , volume 33, 13191326, 2019. 10.1609aaai.v33i01.33011319. Shao-Qun Zhang and Zhi-Hua Zhou. Theoretically provable spiking neural networks. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Neur IPS 2022 , volume 35, 1934519356. Curran Associates, Inc., 2022. 16 . Proofs Outline We start by introducing the spiking network calculus in .1 to compose and parallelize differ...\n",
            "\n",
            "ID: 2401.15212v1\n",
            "Текст: . The synapses of the network are simple featuring only discrete weights wand delays d. Additionally, the neurons have the option of enabling total leak, where at the end of a neurons cycle, if there is any charge remaining, it is all leaked away and the neurons potential reset to its resting potential. Other features common in spiking architectures like spike timing dependent plasticity (STDP) 10 for learning are not necessary for this work. In this way, our network specifications are elegant r...\n",
            "\n",
            "ID: 2308.08218v2\n",
            "Текст: . 10.1109 TNNLS.2017.2726060. Philipp Petersen and Felix V oigtlaender. Optimal approximation of piecewise smooth functions using deep relu neural networks. Neural Networks , 108:296330, 2018. 10.1016j.neunet. 2018.08.019. Bodo Rueckauer and Shih-Chii Liu. Conversion of analog to spiking neural networks using sparse temporal coding. In ISCAS 2018 , 15, 2018. 10.1109ISCAS.2018.8351295. Bodo Rueckauer and Shih-Chii Liu. Temporal pattern coding in deep spiking neural networks. In IJCNN 2021 , 18, 2...\n",
            "\n",
            "ID: 2308.08218v2\n",
            "Текст: . Expression of fractals through neural network functions. IEEE Journal on Selected Areas in Information Theory , 1(1):5766, 2020. 10. 1109JSAIT.2020.2991422. Wulfram Gerstner. Time structure of the activity in neural network models. Physical Review E , 51: 738758, 1995. Wulfram Gerstner and Werner M. Kistler. Spiking Neuron Models: Single Neurons, Populations, Plasticity . Cambridge University Press, 2002. Wulfram Gerstner and J. van Hemmen. How to describe neuronal activity: Spikes, rates, or ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Загрузка индекса и маппинга\n",
        "index = faiss.read_index(\"bio_articles.index\")\n",
        "with open(\"id_to_metadata.json\", \"r\") as f:\n",
        "    id_to_metadata = json.load(f)\n",
        "\n",
        "# Поиск по запросу\n",
        "query = \"Tell me about spiking neural networks\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "k = 5  # Количество результатов\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Топ-5 результатов:\")\n",
        "for idx in indices[0]:\n",
        "    print(f\"ID: {id_to_metadata[str(idx)]['id']}\")\n",
        "    print(f\"Текст: {texts[idx][:500]}...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAffcHXejSmz",
        "outputId": "cf702ec6-abc8-488f-ea36-07bc53b7084e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-c4103d56317f>:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n",
            "Device set to use cuda:0\n",
            "<ipython-input-13-c4103d56317f>:44: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=generator)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a spiking neuron, information is represented with a series of short electrical impulses known as spikes, as opposed to the numerical representation of articial neurons. In addition, there are various neuron models to represent a Spiking Neuron. One of the most popular and simplest neuron model is the Leaky Integrate-and-Fire (LIF) neuron. Action-potentials or spikes are short electrical pulses that are the result of electrical and biochemical properties of a biological neuron . Sec. III briey introduces DECOLLE approach.III. B ACKGROUND A. SpiKING Neural Networks\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import json\n",
        "\n",
        "# 1. Загрузка данных\n",
        "with open(\"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "texts = [chunk[\"text\"] for chunk in chunks]\n",
        "metadatas = [{\"id\": chunk[\"id\"], \"source\": chunk[\"source\"], \"chunk_id\": chunk[\"chunk_id\"], \"chunk_position\": chunk[\"chunk_position\"]} for chunk in chunks]\n",
        "\n",
        "# 2. Инициализация эмбеддингов\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name= \"sentence-transformers/all-mpnet-base-v2\" # \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "\n",
        "# 3. Обновление векторного хранилища\n",
        "vector_store = FAISS.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embeddings,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "generator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=300,  # Укоротите для фокусировки\n",
        "    do_sample=True,\n",
        "    temperature=0.7,  # Уменьшите случайность\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.5,\n",
        "    no_repeat_ngram_size=3,  # Блокировка повторяющихся фраз\n",
        "    forced_eos_token_id=tokenizer.eos_token_id,  # Четкое завершение\n",
        "    num_beams=4\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "def is_context_relevant(query, context_chunks, threshold=0.3):\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "    chunk_embeddings = embeddings.embed_documents(context_chunks)\n",
        "\n",
        "    max_similarity = max(\n",
        "        np.dot(query_embedding, chunk_emb)\n",
        "        for chunk_emb in chunk_embeddings\n",
        "    )\n",
        "    return max_similarity > threshold\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rag_answer(query):\n",
        "    # Поиск релевантных чанков\n",
        "    docs = vector_store.similarity_search(query, k=2)\n",
        "    context_chunks = [d.page_content for d in docs]\n",
        "\n",
        "    # Проверка релевантности\n",
        "    if not context_chunks or not is_context_relevant(query, context_chunks):\n",
        "        return \"I don't know\"\n",
        "\n",
        "    # Сбор контекста\n",
        "    context = \"\\n\".join(context_chunks)\n",
        "\n",
        "\n",
        "    # Генерация ответа\n",
        "    prompt = f\"\"\"\n",
        "    Write a comprehensive answer in academic English, synthesizing information from the context.\n",
        "    Ignore section numbers (e.g. Sec.), citations (e.g. [12]), roman numerals, artifacts of scientific texts and equations. Structure your answer as:\n",
        "    If there's no relevant info in context - answer 'I don't know'\n",
        "    Context: {context}\n",
        "    Question: {query}\n",
        "    Answer (concise, academic style):\n",
        "    \"\"\"\n",
        "    # print(context)\n",
        "    response = generator(prompt,max_length=400,\n",
        "    min_length=150,\n",
        "    length_penalty=2.0)[0][\"generated_text\"]\n",
        "    final_answer = response.split(\"Ответ:\")[-1].strip()  # Вырезаем только ответ\n",
        "    if not final_answer or final_answer.lower().startswith(\"i don't\"):\n",
        "        return \"I don't know\"\n",
        "    return final_answer\n",
        "\n",
        "# Пример использования\n",
        "query = \"Tell me about spiking neural networks\"\n",
        "answer = rag_answer(query)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "H47zBUylxdWO"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "questions = [\"What is the main advantage of Spiking Neural Networks (SNNs) compared to traditional Artificial Neural Networks (ANNs) in the context of computer vision??\",\n",
        "             \"How does the DECOLLE learning rule enable supervised training in deep SNNs, and what are its key benefits??\",\n",
        "             \"Describe the architecture of the proposed DCSNN model for single object localization. How does it process grayscale images and produce bounding box predictions?\",\n",
        "            ]\n",
        "ground_truths = [[\"The main advantage of SNNs over traditional ANNs is their potential for much lower energy consumption. SNNs are inspired by biological neurons and communicate through discrete spikes, making them highly efficient for implementation on neuromorphic hardware. This efficiency makes SNNs attractive for energy-constrained computer vision applications, even though their performance has historically lagged behind ANNs.\"],\n",
        "                [\"DECOLLE (Deep Continuous Local Learning) enables supervised training in deep SNNs by using local surrogate gradients. In DECOLLE, each layer has a readout layer with fixed random weights that computes a local error at each timestep. This allows each layer to optimize its own local error function, facilitating online learning and reducing memory requirements. The locality of the learning rule also makes DECOLLE suitable for neuromorphic hardware and easy to implement with popular machine learning frameworks.\"],\n",
        "                [\"The proposed DCSNN architecture follows an encoder-decoder paradigm using convolutional Leaky Integrate-and-Fire (LIF) layers. The encoder consists of three convolutional LIF layers to extract semantic features, while the decoder reconstructs spatial details using three more convolutional LIF layers, connected via residual links. Grayscale images are encoded into spike trains using rate coding (via a Poisson process), and the network predicts bounding box coordinates through linear readout layers. The final bounding box prediction is taken from the last timestep and the deepest readout layer, corresponding to the most refined hierarchical representation.\"]]\n",
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "# Inference\n",
        "for query in questions:\n",
        "  answers.append(rag_answer(query))\n",
        "  contexts.append([docs.page_content for docs in vector_store.similarity_search(query, k=5)])\n",
        "\n",
        "# To dict\n",
        "data = {\n",
        "    \"question\": questions,\n",
        "    \"answer\": answers,\n",
        "    \"contexts\": contexts,\n",
        "    \"ground_truths\": ground_truths\n",
        "}\n",
        "\n",
        "# Convert dict to dataset\n",
        "dataset = Dataset.from_dict(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quality of system"
      ],
      "metadata": {
        "id": "i7yVx0Iprc4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from rouge import Rouge\n",
        "from bert_score import score\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Модель для оценки семантического сходства\n",
        "similarity_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Метрики для ретривера\n",
        "def evaluate_retriever(contexts, ground_truths):\n",
        "    \"\"\"\n",
        "    Оценка качества поиска контекста\n",
        "    \"\"\"\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "\n",
        "    for ctx_list, gt_list in zip(contexts, ground_truths):\n",
        "        # Эмбеддинги для сравнения\n",
        "        ctx_emb = similarity_model.encode(ctx_list)\n",
        "        gt_emb = similarity_model.encode(gt_list)\n",
        "\n",
        "        # Попарное сходство\n",
        "        similarity_matrix = util.cos_sim(ctx_emb, gt_emb)\n",
        "\n",
        "        # Precision@k\n",
        "        prec = np.max(similarity_matrix.numpy(), axis=1).mean()\n",
        "        precisions.append(prec)\n",
        "\n",
        "        # Recall@k\n",
        "        rec = np.max(similarity_matrix.numpy(), axis=0).mean()\n",
        "        recalls.append(rec)\n",
        "\n",
        "    return {\n",
        "        'retriever_precision@5': np.mean(precisions),\n",
        "        'retriever_recall@5': np.mean(recalls),\n",
        "        'retriever_f1@5': 2 * (np.mean(precisions)*np.mean(recalls)) / (np.mean(precisions)+np.mean(recalls))\n",
        "    }\n",
        "\n",
        "# Метрики для генератора\n",
        "def evaluate_generator(answers, ground_truths):\n",
        "    \"\"\"\n",
        "    Оценка качества генерации ответов\n",
        "    \"\"\"\n",
        "    # Текстовые метрики\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = rouge.get_scores(answers, [gt[0] for gt in ground_truths], avg=True)\n",
        "\n",
        "    # Семантическое сходство\n",
        "    answer_emb = similarity_model.encode(answers)\n",
        "    gt_emb = similarity_model.encode([gt[0] for gt in ground_truths])\n",
        "    semantic_sim = np.diag(util.cos_sim(answer_emb, gt_emb)).mean()\n",
        "\n",
        "    # BERTScore\n",
        "    P, R, F1 = score(answers, [gt[0] for gt in ground_truths], lang='en')\n",
        "\n",
        "    return {\n",
        "        'rouge-1': rouge_scores['rouge-1']['f'],\n",
        "        'rouge-l': rouge_scores['rouge-l']['f'],\n",
        "        'semantic_similarity': semantic_sim.item(),\n",
        "        'bert_score': F1.mean().item()\n",
        "    }\n",
        "\n",
        "# Проверка на галлюцинации\n",
        "def check_hallucinations(answers, contexts):\n",
        "    \"\"\"\n",
        "    Оценка соответствия ответов контексту\n",
        "    \"\"\"\n",
        "    hall_scores = []\n",
        "\n",
        "    for ans, ctx in zip(answers, contexts):\n",
        "        ans_emb = similarity_model.encode(ans)\n",
        "        ctx_emb = similarity_model.encode(' '.join(ctx))\n",
        "        hall_scores.append(util.cos_sim(ans_emb, ctx_emb).item())\n",
        "\n",
        "    return {\n",
        "        'faithfulness_score': np.mean(hall_scores)\n",
        "    }\n",
        "\n",
        "def full_evaluation(data):\n",
        "    retriever_metrics = evaluate_retriever(data['contexts'], data['ground_truths'])\n",
        "    generator_metrics = evaluate_generator(data['answer'], data['ground_truths'])\n",
        "    hallucination_metrics = check_hallucinations(data['answer'], data['contexts'])\n",
        "\n",
        "    return {**retriever_metrics, **generator_metrics, **hallucination_metrics}\n",
        "\n",
        "results = full_evaluation(dataset)\n",
        "\n",
        "print(\"Retriever Metrics:\")\n",
        "print(f\"Precision@5: {results['retriever_precision@5']:.2f}\")\n",
        "print(f\"Recall@5: {results['retriever_recall@5']:.2f}\")\n",
        "\n",
        "print(\"\\nGenerator Metrics:\")\n",
        "print(f\"Semantic Similarity: {results['semantic_similarity']:.2f}\")\n",
        "print(f\"BERTScore F1: {results['bert_score']:.2f}\")\n",
        "\n",
        "print(\"\\nHallucination Metrics:\")\n",
        "print(f\"Faithfulness: {results['faithfulness_score']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0pcJx_4rglw",
        "outputId": "d1c9745f-1a26-45d5-e98e-a4f45184ec9d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retriever Metrics:\n",
            "Precision@5: 0.63\n",
            "Recall@5: 0.72\n",
            "\n",
            "Generator Metrics:\n",
            "Semantic Similarity: 0.65\n",
            "BERTScore F1: 0.82\n",
            "\n",
            "Hallucination Metrics:\n",
            "Faithfulness: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The final RAG system turned out to be quite good, but further improvements are possible. For example, using a hybrid search with BM25 give an improvement."
      ],
      "metadata": {
        "id": "j3oI6B_H3924"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eb0iOUoB2uRA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LLpBjSopuiNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88b59e7776034c28a5cc7cc11cdf8406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b91811b37b149a2bdd13d2217c914ac",
              "IPY_MODEL_761195530f724b6cb2e565a453de53f9",
              "IPY_MODEL_25956a426be24ff69f18d37780a6cbc4"
            ],
            "layout": "IPY_MODEL_2e40bac2c3014512a0ef50aed89ee0ff"
          }
        },
        "0b91811b37b149a2bdd13d2217c914ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9bd4c4d0ee34abf85b5b5212ea70d09",
            "placeholder": "​",
            "style": "IPY_MODEL_4f471c51b8394ae986f87e2763776550",
            "value": "Batches: 100%"
          }
        },
        "761195530f724b6cb2e565a453de53f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca4a6a7230da48da9224d14a34937a48",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_363c57ecef72400d9faf1b828698217e",
            "value": 33
          }
        },
        "25956a426be24ff69f18d37780a6cbc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e0ef26383844c97982c19893760d34d",
            "placeholder": "​",
            "style": "IPY_MODEL_085222aafa35420c90e324aefafa8c40",
            "value": " 33/33 [00:04&lt;00:00,  8.89it/s]"
          }
        },
        "2e40bac2c3014512a0ef50aed89ee0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9bd4c4d0ee34abf85b5b5212ea70d09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f471c51b8394ae986f87e2763776550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca4a6a7230da48da9224d14a34937a48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "363c57ecef72400d9faf1b828698217e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e0ef26383844c97982c19893760d34d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "085222aafa35420c90e324aefafa8c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}