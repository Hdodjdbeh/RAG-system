{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_dC4DBznw6Zy"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv -q\n",
        "!pip install sentence-transformers faiss-cpu transformers torch -q\n",
        "!pip install arxiv pdfplumber tqdm python-dotenv fitz tools -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mKjexaQL7aT9"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QaddzefMNqW1"
      },
      "outputs": [],
      "source": [
        "!pip install googletrans -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pBxdZJ-U2su5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def delete_files_in_folder(folder_path):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "        except Exception as e:\n",
        "            print(f'Ошибка при удалении файла {file_path}: {e}')\n",
        "\n",
        "# Пример вызова\n",
        "delete_files_in_folder(\"papers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olVgZT6lvzUf",
        "outputId": "b89c1de4-c8e2-4054-bf63-1c44ed310d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 10it [00:18,  1.87s/it]\n"
          ]
        }
      ],
      "source": [
        "import arxiv\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "client = arxiv.Client()\n",
        "def download_articles(keywords, client, max_results=100, output_dir=\"papers\"):\n",
        "    # Создаем директорию для сохранения PDF\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Формируем запрос\n",
        "    query = \" AND \".join([f'abs:\"{kw}\"' for kw in keywords])\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "    results = client.results(search)\n",
        "    # Скачивание статей\n",
        "    for result in tqdm(results, desc=\"Downloading\"):\n",
        "        try:\n",
        "            result.download_pdf(dirpath=output_dir, filename=f\"{result.get_short_id()}.pdf\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при загрузке {result.entry_id}: {e}\")\n",
        "\n",
        "# Пример вызова\n",
        "keywords = [\"spiking neural network\"]\n",
        "download_articles(keywords, client, max_results=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iJ71kzIwf9w",
        "outputId": "66bea08f-fb6f-4d5e-ad6f-818a9058b9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PDFs:  40%|████      | 4/10 [00:04<00:06,  1.15s/it]/usr/local/lib/python3.11/dist-packages/PyPDF2/_cmap.py:142: PdfReadWarning: Advanced encoding /UniGB-UTF16-H not implemented yet\n",
            "  warnings.warn(\n",
            "Processing PDFs: 100%|██████████| 10/10 [00:14<00:00,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл articles.json успешно создан!\n",
            "ar Xiv:2205.04263v2 eess.SP 1 Jun 2022Spiking Neural Network for IMDD Optical Communication Elias Arnold1,, Georg B ocherer2,, Eric M uller1, Philipp Spilger1, Johannes Schemmel1, Stefano Calabr o2, Maxim Kuschnerov2 1Electronic Visio, Kirchho-Institute for Physics, He idelberg University, Germany 2Huawei Technologies Duesseldorf Gmb H, Munich Research Cen ter, Germany Abstract A spiking neural network (SNN) model suitable for electronic neuromorphic hardware is designed for an IMDD link. The SNN achieves the s ame bit-error-rate as an articial neural network, outperforming linear ion. 1 Introduction Low cost and low power optical transceivers are indispensable for s upporting the exponentially growing data center trac caused by cloud-based services. The h igh power consumption of digital signal processing (DSP) has motivated research on moving parts of the receiver DSP to an analog lower power frontend. For instance, photonic neuromorphic comp uting 1 has been studied recently, e.g., for compensating for chromatic dispersion (CD) and nonlinear impairments in short reach optical transmission 2,3. An alternative solution is analog electronic neuromorphic computing, implementing SNNs 4 in analog hardware 5 by mimicking the basic operation principles of the human brain, thereby adopting the brains unchallenged powe r eciency. SNNs are applied in 6 for an inference task on a spectrogram in ber-optic distributed acoustic sensing. Recently, in-the-loop training of SNNs on analog hardware has achieved state -of-the-art performance in inference tasks 7. Despite electronics operating slower than photonics, electronic h ardware enables higher scalability and thus greater throughput through parallelizat ion, making it a suitable choice for energy-ecient signal processing. An important aspect to be analyzed is whether SNNs in analog electronic hardware support the accuracy required by com munication systems. To assess the accuracyof SNNs, we design and demapp ing using SNNs suitable for a hardware implementation on the Brain Scale S-2 (BSS-2) system 5. We evaluate our SNN in a software simulation for the detection of a 4-level pulse amplitude modulation (PAM4) signal for an intensity modulationdirect detection (IMDD) link, impaired by CD and additive white Gaussian nois. Our SNN achieves the bit error rate (BER) of an articial neural network (ANN), outperforming a digital linear minimum mean square error (LMMSE) . 2 and Demapping using Spiking Neural Networks For and demapping, we consider an SNN with a single hidden layer, consisting of 40 spikingleaky-integrate and re (LIF) neurons 4, Sec. 1.3, and an output layer constituted by four non-spiking leaky-integrate (LI) 4, Sec. 1.3 readout neurons. This architecture ts in size on the BSS-2 system 5. Each LIF neuron jmaintains an internal membrane state vjdescribed by the 1 ordinary dierential mv (vvleak)I with I N1summationdisplay i0summationdisplay sspikesi-th neuron Wji(s i)expparenleftbigg s i synparenrightbigg , (1) integrating synaptic input I, caused by pre-synaptic events s i, onto its membrane. As the membrane potential exceeds a threshold , the neuron emits a post-synaptic spike a time s j, after which it is set to a reset potential vr. LI neurons exhibit the same dynamics, without the ability to spike. The parameters synandmare the time constants of the synaptic current and the membrane potential, respectively. Areceivedsample ytand itsntap2predecessorsand successor aretranslatedto 10 input spike events per sample by a spike encoder (see . 1), potentially replacing power-hungry analog-to-digital conversion (ADC) in hardware. To this end, each input neuron emits a spike at times igiven by the scaled log-distance 8 to a reference point i, assigned to each input neuron. The input sample ytgets classied with the label k 0,1,2,3of the output neuron with the maximum membrane value v over the considered time frame. Hence, the network learns to place hidden spike events in time, such that the readout traces are adjusted appropriately. For training our SNNs we rely on backpropagation through time (BPTT) with the Adam optimizer and surrogate gradients (Super Spike 8) to account for the discontinuity of spiking LIF neurons. Note that our simulations are implemented in hxtorch 9, also supporting execution on the BSS-2 system. 3 Results and Conclusions In .2A, we display a simulated IMDD link. Bits are mapped to a PAM4 constella tion, the signal is upsampled and ltered by a root-raised-cosine (RRC). The signal is then shifted to the positive and CD is applied. At the receiver, a PD squares the sig nal and AWGN is added. The signal is then RRC ltered and downsampled. The resultin g signal yis and demapped. As reference, we use a digital 17 tap LMMSE r, followed by a demapper with BER optimized decision boundaries, see . 2D (right), and ANNs with one and two hidden layers, respectively, see . 2C. In . 2D (left) we see that joint and demapping by a 17 tap SNN outperforms the LMMSE, and performs as well as th e 17 tap ANN 1, which has 1 hidden layer with 40 neurons, similar to the SNN. The reference sch emes and the SNN were trained using supervised learning. By means of software simulation, we have shown that an SNN suitable for analog electronic hardware can eciently compensate im pairments in a simulated t ytntap2 yt ytntap2y0 0 30msspike encoding input spike trains hidden LIF neuronsoutput LI neuronss ilo) withd Ayti and,Aconst.0 20ms 0123 0 25ms0.51.0 vka.u.Wih ji Who kj max(vk) and argmaxk3mapsto10 2mapsto11 1mapsto01 0mapsto00B1B2 1: SNN demapper decision chain 2 B1B200mapsto 3 01mapsto 1 11mapsto1 10mapsto3up RRC Chromatic Dispersion2RRC down DemapperB1B2ybias Z rollo 0.2 rollo 0 .2photo diode A baudrate 100GBd wavelength 1270nm dispersion 5psnmkm berlength 5km B 420 2 4 yt010002000300040005000 Counts1 tap LMMSE 17 tap LMMSE demapper decision boundaries 16 18 20 22 24 10log10(2)104103102BER1 tap LMMSE 17 tap LMMSE 17 tap ANN 1 17 tap SNN 17 tap ANN 2 KP4 FEC threshold D Net Hidden layer width Activation SNN 40 LIF ANN140 Re LU ANN234, 10 Re LUC 2: (A)Simulated IMDD link. (B)IMDD parameters. (C)NN parameters. (D)Left: BER results for transmission of PAM4 over the simulated IMDD li nk. Right: Histogram of the linear MMSE output. IMDD link. In ongoing research, we implement the proposed SNN on t he BSS-2 system, with the aim to reproduce the reported results on analog hardware. Funding The contributions of the Electronic Visio group1have been supported by the EC Horizon 2020 Framework Programme under grant agreements 785907 (HBP SGA 2) and 945539 (HBP SGA3), the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy EXC 21811-390900948(the Heidelberg STRU CTURES Excellence Cluster), the Helmholtz Association Initiative and Networking Fund Advanced C omputing Architectures (ACA) under Project SO-092. References 1 B. J. Shastri et al., Photonics for articial intelligence and neuromorphic computing, Nature Photonics , 15, no. 2, 2021. 2 S. Li et al., Micro-ring resonator based photonic reservoir computing for P AM , IEEE Photonics Technology Letters , 33, no. 18, 978981, 2021. 3 S. M. Ranzini et al., Experimental investigation of optoelectronic receiver with rese rvoir computing in short reach optical ber communications, Journal of Lightwave Technology , 39, no. 8, 24602467, 2021. 4 W. Gerstner et al.,Neuronal dynamics: From single neurons to networks and mode ls of cognition. Cambridge University Press, 2014. 5 C. Pehle et al., The Brain Scale S-2 accelerated neuromorphic system with hybrid plasticity, Frontiers in Neuroscience , 16, 2022. 6 H.Wu et al., Improvedgeneralizationinsignalidenticationwithunsupervised spikingneuron networks for ber-optic distributed acoustic sensor, Journal of Lightwave Technology , 2022. 7 B. Cramer et al., Surrogate gradients for analog neuromorphic computing, Proc. National Academy of Sciences , 119, no. 4, 2022. 3 8 E. O. Neftci et al., Surrogategradientlearningin spikingneural networks: Bringing the power of gradient-based optimization to spiking neural networks, IEEE Signal Processing Magazine , 36, no. 6, 5163, 2019. 9 E. M uller et al., A scalable approach to modeling on accelerated neuromorphic har dware, ar Xiv 2203.11102 , Feb. 2022. to Frontiers in Neuromorphic Engineering. 4 This trace.png is available in png format from:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# import pdfplumber  # склеивает слова\n",
        "# import fitz\n",
        "import json\n",
        "import logging\n",
        "import PyPDF2\n",
        "import re\n",
        "from googletrans import Translator\n",
        "\n",
        "\n",
        "# Отключаем логирование pdfminer\n",
        "# logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
        "\n",
        "def clean_text(text):\n",
        "    # Удаление формул в $...$ и $$...$$\n",
        "    text = re.sub(r\"\\$.*?\\$\", \"\", text, flags=re.DOTALL) # DOTALL\n",
        "    text = re.sub(r\"\\$\\$.*?\\$\\$\", \"\", text, flags=re.DOTALL) # DOTALL\n",
        "    text = re.sub(r\"\\b(fig\\w*|pic\\w*)\\b\", '', text, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    # Удаление URL и спецсимволов\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s.,;:!?()-]\", \"\", text)\n",
        "    text = re.sub(r\"\\b(fig\\w*|pic\\w*)\\b\", '', text, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    text = re.sub(r\"[a-zA-Z]\\([^\\)]*\\)\", \"\", text)\n",
        "    # Вставить пробелы между слитными словами (буква + заглавная буква)\n",
        "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
        "    # Удалить спецсимволы, кроме базовых знаков препинания\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s.,;:!?()-]\", \"\", text)\n",
        "    # Удалить лишние пробелы\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
        "    text = re.sub(r'(Section|Appendix|Fig\\.?)\\s+[IVXLCDM0-9]+', '', text)\n",
        "    # Удаление библиографических ссылок\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "\n",
        "    #касательно научных текстов очистка\n",
        "\n",
        "    text = re.sub(r'\\$.*?\\$', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'\\\\begin{equation}.*?\\\\end{equation}', '', text, flags=re.DOTALL)\n",
        "\n",
        "    # Удаление ссылок на структурные элементы\n",
        "    text = re.sub(\n",
        "        r'\\b(?:Figure|Fig|Table|Equation|Eq|Section|Appendix|Chapter|Algorithm|Code)\\s*[A-Za-z0-9]+\\b',\n",
        "        '',\n",
        "        text,\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    # Удаление библиографических ссылок (все форматы)\n",
        "    text = re.sub(r'\\[[\\d,-]+\\]', '', text)  # [1], [2-5]\n",
        "    text = re.sub(r'\\([A-Za-z]+\\s+et\\s+al\\.?,?\\s?\\d{4}\\)', '', text)  # (Smith et al., 2020)\n",
        "\n",
        "    # Удаление технических артефактов\n",
        "    text = re.sub(\n",
        "        r'\\b(?:arxiv|doi|issn|isbn|vol|pp|pages?|http|https|www\\.|preprint|submitted|version)\\b[^\\s]*',\n",
        "        '',\n",
        "        text,\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    # Удаление подписей к рисункам/таблицам\n",
        "    text = re.sub(r'^\\s*(Caption|Source|Note):.*$', '', text, flags=re.IGNORECASE|re.MULTILINE)\n",
        "\n",
        "    # Обработка специальных символов\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?%()\\-–/]', '', text)  # Сохраняем основные знаки препинания\n",
        "\n",
        "    # Удаление LaTeX-команд\n",
        "    text = re.sub(r'\\\\[a-z]+(\\{[^}]+\\})?', '', text)\n",
        "\n",
        "    # Удаление маркеров перечисления\n",
        "    text = re.sub(r'^\\s*[\\d•■♦➢]+[\\s.)]*', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Нормализация пробелов и переносов\n",
        "    text = re.sub(r'(?<=\\w)-\\s+(?=\\w)', '', text)  # Соединение перенесенных слов\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Удаление служебных блоков (эмпирически выверенные паттерны)\n",
        "    # text = re.sub(\n",
        "    #     r'(?:This\\s(?:paper|study)\\s.*?(?:limitations|contribution|propose))\\.?',\n",
        "    #     '',\n",
        "    #     text,\n",
        "    #     flags=re.IGNORECASE)\n",
        "    # text = re.sub(\n",
        "    #     r'^\\s*(Abstract|Introduction|Related Work|Methodology|Results|Conclusion|References)\\s*$',\n",
        "    #     '', text,\n",
        "    #     flags=re.IGNORECASE|re.MULTILINE\n",
        "    # )\n",
        "    # text = re.sub(\n",
        "    #     r'\\b([IVXLCDM]+|[A-Z])(\\s*\\.?\\s*)([A-Z][a-z]*\\b)?',\n",
        "    #     lambda m: m.group(3) if m.group(3) else '',\n",
        "    #     text,\n",
        "    #     flags=re.IGNORECASE\n",
        "    # )\n",
        "    # text = re.sub(\n",
        "    #     r'(^|\\n)\\s*[A-Z0-9][\\.\\)]?\\s+([A-Z][a-z]+(\\s+[A-Z][a-z]+)*)\\b',\n",
        "    #     r'\\1\\2',\n",
        "    #     text\n",
        "    # )\n",
        "\n",
        "    # # Нормализация пробелов после знаков препинания\n",
        "    # text = re.sub(r'([.,;:])([A-Za-z])', r'\\1 \\2', text)\n",
        "\n",
        "    # # Удаление изолированных заглавных букв в середине текста\n",
        "    # text = re.sub(r'(?<!\\b[A-Z])\\s+[A-Z]\\s+(?!\\b[A-Z])', ' ', text)\n",
        "\n",
        "    # # Фильтрация странных комбинаций регистров\n",
        "    # text = re.sub(\n",
        "    #     r'\\b([A-Z]{2,}[a-z]*|[A-Z][a-z]*[A-Z]{2,}[a-z]*)\\b',\n",
        "    #     lambda m: m.group().capitalize(),\n",
        "    #     text\n",
        "    # )\n",
        "    # text = re.sub(\n",
        "    #     r'(methodology|experimental setup|results and discussion)\\s*:\\s*',\n",
        "    #     '',\n",
        "    #     text,\n",
        "    #     flags=re.IGNORECASE\n",
        "    # )\n",
        "\n",
        "    # # Удаление повторяющихся фраз с небольшими вариациями\n",
        "    # text = re.sub(\n",
        "    #     r'(\\b\\w+\\b)(?:\\s+\\1)+',\n",
        "    #     r'\\1',\n",
        "    #     text,\n",
        "    #     flags=re.IGNORECASE\n",
        "    # )\n",
        "    # Удаление лишних пробелов\n",
        "    text = \" \".join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "# def split_text(text, max_length=500):\n",
        "#     # Разбить текст на предложения\n",
        "#     sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "#     chunks = []\n",
        "#     chunk = \"\"\n",
        "#     for sentence in sentences:\n",
        "#         if len(chunk) + len(sentence) < max_length:\n",
        "#             chunk += \" \" + sentence\n",
        "#         else:\n",
        "#             if chunk:\n",
        "#                 chunks.append(chunk.strip())\n",
        "#             chunk = sentence\n",
        "#     if chunk:\n",
        "#         chunks.append(chunk.strip())\n",
        "#     return chunks\n",
        "\n",
        "# def translate_long_text(text, src='en', dest='ru'):\n",
        "#     translator = Translator()\n",
        "#     cleaned_text = clean_for_translation(text)\n",
        "#     chunks = split_text(cleaned_text)\n",
        "#     translated_chunks = []\n",
        "#     for i, chunk in enumerate(chunks):\n",
        "#         if chunk.strip():\n",
        "#             try:\n",
        "#                 translated = translator.translate(chunk, src=src, dest=dest)\n",
        "#                 translated_chunks.append(translated.text)\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Ошибка перевода части {i}: {e}\")\n",
        "#                 translated_chunks.append(chunk)  # Вставляем оригинал, если ошибка\n",
        "#     return \" \".join(translated_chunks)\n",
        "\n",
        "# def translate_text(text, src='en', dest='ru'):\n",
        "#     translator = Translator()\n",
        "#     try:\n",
        "#         translated = translator.translate(text, src=src, dest=dest)\n",
        "#         return translated.text\n",
        "#     except Exception as e:\n",
        "#         print(f\"Ошибка перевода: {e}\")\n",
        "#         print(text)\n",
        "#         return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "def process_pdfs(input_dir=\"papers\", output_file=\"articles.json\"):\n",
        "    articles = []\n",
        "    if not os.path.exists(input_dir):\n",
        "        print(f\"Ошибка: директория {input_dir} не найдена!\")\n",
        "        return\n",
        "    for filename in tqdm(os.listdir(input_dir), desc=\"Processing PDFs\"):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(input_dir, filename)\n",
        "            try:\n",
        "                text = extract_text_from_pdf(pdf_path)\n",
        "                # text = clean_text(text)  # <-- Добавляем очистку\n",
        "                text = clean_text(text)\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка обработки {filename}: {e}\")\n",
        "            articles.append({\n",
        "                    \"id\": filename.replace(\".pdf\", \"\"),\n",
        "                    \"text\": text,\n",
        "                    \"source\": \"arXiv\"\n",
        "                })\n",
        "    try:\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(articles, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Файл {output_file} успешно создан!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при сохранении JSON: {e}\")\n",
        "    # Сохранение в JSON\n",
        "    return text\n",
        "\n",
        "last_text = process_pdfs()\n",
        "print(last_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "94beAu_Od77N"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Загрузка данных\n",
        "with open(\"articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Создаем объекты Document с метаданными\n",
        "documents = []\n",
        "for item in data:\n",
        "    doc = Document(\n",
        "        page_content=item[\"text\"],\n",
        "        metadata={\n",
        "            \"id\": item[\"id\"],\n",
        "            \"source\": item[\"source\"]\n",
        "        }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "# Настройка сплиттера\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=600,       # Размер чанка в символах\n",
        "    chunk_overlap=150,    # Перекрытие между чанками\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Разбиваем на чанки\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Сохраняем результат\n",
        "output_data = []\n",
        "MIN_CHUNK_LENGTH = 50\n",
        "for i, chunk in enumerate(chunks):\n",
        "    if len(chunk.page_content) >= MIN_CHUNK_LENGTH:\n",
        "      output_data.append({\n",
        "          \"id\": chunk.metadata[\"id\"],\n",
        "          \"text\": chunk.page_content,\n",
        "          \"source\": chunk.metadata[\"source\"],\n",
        "          \"chunk_id\": f\"{chunk.metadata['id']}_{len(output_data)}\",\n",
        "          \"chunk_position\": i  # Уникальный ID чанка\n",
        "      })\n",
        "\n",
        "with open(\"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYAuSyksgKej",
        "outputId": "3b8539ce-7cb4-46c7-ea72-b1b4b9df4442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего статей: 10\n",
            "Всего чанков: 1033\n",
            "\n",
            "Пример чанка:\n",
            "ID: 2303.10780v2_0\n",
            "Текст: A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efciency, and Best ...\n"
          ]
        }
      ],
      "source": [
        "print(f\"Всего статей: {len(data)}\")\n",
        "print(f\"Всего чанков: {len(output_data)}\")\n",
        "print(\"\\nПример чанка:\")\n",
        "print(f\"ID: {output_data[0]['chunk_id']}\")\n",
        "print(f\"Текст: {output_data[0]['text'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "4d7dec8610894e1ea0b5bba14907f48a",
            "f5ce24eed4a5446dbe1d27e2a76ea53f",
            "8f53ff733e30497da8665e0d4d33abf4",
            "324ac4ca2f934819a3f88762dfd3c2dc",
            "418f14f7a66d4e22851ca06a519c6eda",
            "e450ac25fe0c404a91837349ccee8664",
            "4b2d80d1eedd4a84aa1a94865d085cec",
            "d7bee0656cb047d2ac536b89b38a40a7",
            "5392f91a4ff04388be58e48f1a8789c9",
            "fb4535d9254040fc8253d9eb625d05f3",
            "003b7e68166345c1acd9009714ecd9b1"
          ]
        },
        "id": "wULbWWIbgkuR",
        "outputId": "97195114-5563-4edf-9825-b4f7e248b1c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d7dec8610894e1ea0b5bba14907f48a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размерность эмбеддингов: (1033, 512)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Загрузка чанков из JSON\n",
        "with open(\"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "texts = [chunk[\"text\"] for chunk in chunks]\n",
        "metadatas = [{\"id\": chunk[\"id\"], \"source\": chunk[\"source\"], \"chunk_id\": chunk[\"chunk_id\"], \"chunk_position\": chunk[\"chunk_position\"]} for chunk in chunks]\n",
        "\n",
        "# Загрузка модели для эмбеддингов (1B параметров)\n",
        "model = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v2\", device=\"cuda\")  # Для GPU: device=\"cuda\"\n",
        "\n",
        "# Преобразование текстов в векторыремонт ноутбуков москва\n",
        "embeddings = model.encode(\n",
        "    texts,\n",
        "    batch_size=32,  # Оптимизация для больших данных\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "print(f\"Размерность эмбеддингов: {embeddings.shape}\")  # (num_chunks, 768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QcRM0kbyiZeK"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "# Создание индекса\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)  # Индекс с L2-метрикой\n",
        "\n",
        "# Добавление векторов в индекс\n",
        "index.add(embeddings)\n",
        "\n",
        "# Сохранение индекса\n",
        "faiss.write_index(index, \"bio_articles.index\")\n",
        "\n",
        "# Сохранение маппинга id -> метаданные\n",
        "id_to_metadata = {i: md for i, md in enumerate(metadatas)}\n",
        "with open(\"id_to_metadata.json\", \"w\") as f:\n",
        "    json.dump(id_to_metadata, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jYefkRWFi1uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b84ed1-fd84-46d2-83f1-017e04d84317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Топ-5 результатов:\n",
            "ID: 2308.08218v2\n",
            "Текст: . How to describe neuronal activity: Spikes, rates, or assemblies? In J. Cowan, G. Tesauro, and J. Alspector, editors, NIPS 1993 , volume 6. MorganKaufmann, 1993. Wulfram Gerstner, Werner M. Kistler, Richard Naud, and Liam Paninski. Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition . Cambridge University Press, 2014. Paul W. Goldberg and Mark R. Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. Machine Learning , 18(2):1...\n",
            "\n",
            "ID: 2308.08218v2\n",
            "Текст: . TDSNN: From deep neural networks to deep spike neural networks with temporal-coding. In AAAI 2019 , volume 33, 13191326, 2019. 10.1609aaai.v33i01.33011319. Shao-Qun Zhang and Zhi-Hua Zhou. Theoretically provable spiking neural networks. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Neur IPS 2022 , volume 35, 1934519356. Curran Associates, Inc., 2022. 16 . Proofs Outline We start by introducing the spiking network calculus in .1 to compose and parallelize differ...\n",
            "\n",
            "ID: 2401.15212v1\n",
            "Текст: . The synapses of the network are simple featuring only discrete weights wand delays d. Additionally, the neurons have the option of enabling total leak, where at the end of a neurons cycle, if there is any charge remaining, it is all leaked away and the neurons potential reset to its resting potential. Other features common in spiking architectures like spike timing dependent plasticity (STDP) 10 for learning are not necessary for this work. In this way, our network specifications are elegant r...\n",
            "\n",
            "ID: 2308.08218v2\n",
            "Текст: . 10.1109 TNNLS.2017.2726060. Philipp Petersen and Felix V oigtlaender. Optimal approximation of piecewise smooth functions using deep relu neural networks. Neural Networks , 108:296330, 2018. 10.1016j.neunet. 2018.08.019. Bodo Rueckauer and Shih-Chii Liu. Conversion of analog to spiking neural networks using sparse temporal coding. In ISCAS 2018 , 15, 2018. 10.1109ISCAS.2018.8351295. Bodo Rueckauer and Shih-Chii Liu. Temporal pattern coding in deep spiking neural networks. In IJCNN 2021 , 18, 2...\n",
            "\n",
            "ID: 2308.08218v2\n",
            "Текст: . Expression of fractals through neural network functions. IEEE Journal on Selected Areas in Information Theory , 1(1):5766, 2020. 10. 1109JSAIT.2020.2991422. Wulfram Gerstner. Time structure of the activity in neural network models. Physical Review E , 51: 738758, 1995. Wulfram Gerstner and Werner M. Kistler. Spiking Neuron Models: Single Neurons, Populations, Plasticity . Cambridge University Press, 2002. Wulfram Gerstner and J. van Hemmen. How to describe neuronal activity: Spikes, rates, or ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Загрузка индекса и маппинга\n",
        "index = faiss.read_index(\"bio_articles.index\")\n",
        "with open(\"id_to_metadata.json\", \"r\") as f:\n",
        "    id_to_metadata = json.load(f)\n",
        "\n",
        "# Поиск по запросу\n",
        "query = \"Tell me about spiking neural networks\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "k = 5  # Количество результатов\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Топ-5 результатов:\")\n",
        "for idx in indices[0]:\n",
        "    print(f\"ID: {id_to_metadata[str(idx)]['id']}\")\n",
        "    print(f\"Текст: {texts[idx][:500]}...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b_eBQsOvwZ_R"
      },
      "outputs": [],
      "source": [
        "!pip install torch -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "id": "szHfwgcqxk5F"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "cQ4beRtOCCoW"
      },
      "outputs": [],
      "source": [
        "!pip install googletrans -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWVamSoIHgIj"
      },
      "outputs": [],
      "source": [
        "!pip install yandex-translater -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAffcHXejSmz",
        "outputId": "66af18bf-2d1a-4e3d-bd63-a49e58e37067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a spiking neuron, information is represented with a series of short electrical impulses known as spikes, as opposed to the numerical representation of articial neurons. In addition, there are various neuron models to represent a Spiking Neuron. One of the most popular and simplest neuron model is the Leaky Integrate-and-Fire (LIF) neuron. Action-potentials or spikes are short electrical pulses that are the result of electrical and biochemical properties of a biological neuron . Sec. III briey introduces DECOLLE approach.III. B ACKGROUND A. SpiKING Neural Networks\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import json\n",
        "from googletrans import Translator\n",
        "\n",
        "def translate_text(text, src='ru', dest='en'):\n",
        "    translator = Translator()\n",
        "    try:\n",
        "        result = translator.translate(text, src=src, dest=dest)\n",
        "        return result.text\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка перевода: {e}\")\n",
        "        print(text)\n",
        "        return text\n",
        "\n",
        "# 1. Загрузка данных\n",
        "with open(\"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "texts = [chunk[\"text\"] for chunk in chunks]\n",
        "metadatas = [{\"id\": chunk[\"id\"], \"source\": chunk[\"source\"], \"chunk_id\": chunk[\"chunk_id\"], \"chunk_position\": chunk[\"chunk_position\"]} for chunk in chunks]\n",
        "\n",
        "# 2. Инициализация эмбеддингов\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name= \"sentence-transformers/all-mpnet-base-v2\" #\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "\n",
        "# 3. Обновление векторного хранилища\n",
        "vector_store = FAISS.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embeddings,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "generator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=300,  # Укоротите для фокусировки\n",
        "    do_sample=True,\n",
        "    temperature=0.7,  # Уменьшите случайность\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.5,\n",
        "    no_repeat_ngram_size=3,  # Блокировка повторяющихся фраз\n",
        "    forced_eos_token_id=tokenizer.eos_token_id,  # Четкое завершение\n",
        "    num_beams=4  # Добавьте beam search для научных текстов\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "def is_context_relevant(query, context_chunks, threshold=0.3):\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "    chunk_embeddings = embeddings.embed_documents(context_chunks)\n",
        "\n",
        "    max_similarity = max(\n",
        "        np.dot(query_embedding, chunk_emb)\n",
        "        for chunk_emb in chunk_embeddings\n",
        "    )\n",
        "    return max_similarity > threshold\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rag_answer(query):\n",
        "    # Поиск релевантных чанков\n",
        "    docs = vector_store.similarity_search(query, k=2)\n",
        "    context_chunks = [d.page_content for d in docs]\n",
        "\n",
        "    # Проверка релевантности\n",
        "    if not context_chunks or not is_context_relevant(query, context_chunks):\n",
        "        return \"I don't know\"\n",
        "\n",
        "    # Сбор контекста\n",
        "    context = \"\\n\".join(context_chunks)\n",
        "    # Перевод запроса\n",
        "    # translated_query = translate_text(query)\n",
        "    # print(translated_query)\n",
        "\n",
        "    # Генерация ответа\n",
        "    prompt = f\"\"\"\n",
        "    Write a comprehensive answer in academic English, synthesizing information from the context.\n",
        "    Ignore section numbers, citations (e.g. [12]), and equations. Structure your answer as:\n",
        "    1. Core definition\n",
        "    2. Key characteristics\n",
        "    3. Common applications\n",
        "    If there's no relevant info in context - answer 'I don't know'\n",
        "    Context: {context}\n",
        "    Question: {query}\n",
        "    Answer (concise, academic style):\n",
        "    \"\"\"\n",
        "    # print(context)\n",
        "    response = generator(prompt,max_length=400,\n",
        "    min_length=150,\n",
        "    length_penalty=2.0)[0][\"generated_text\"]\n",
        "    final_answer = response.split(\"Ответ:\")[-1].strip()  # Вырезаем только ответ\n",
        "    if not final_answer or final_answer.lower().startswith(\"i don't\"):\n",
        "        return \"I don't know\"\n",
        "    return final_answer\n",
        "\n",
        "# Пример использования\n",
        "query = \"Tell me about spiking neural networks\"\n",
        "answer = rag_answer(query)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H47zBUylxdWO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d7dec8610894e1ea0b5bba14907f48a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5ce24eed4a5446dbe1d27e2a76ea53f",
              "IPY_MODEL_8f53ff733e30497da8665e0d4d33abf4",
              "IPY_MODEL_324ac4ca2f934819a3f88762dfd3c2dc"
            ],
            "layout": "IPY_MODEL_418f14f7a66d4e22851ca06a519c6eda"
          }
        },
        "f5ce24eed4a5446dbe1d27e2a76ea53f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e450ac25fe0c404a91837349ccee8664",
            "placeholder": "​",
            "style": "IPY_MODEL_4b2d80d1eedd4a84aa1a94865d085cec",
            "value": "Batches: 100%"
          }
        },
        "8f53ff733e30497da8665e0d4d33abf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7bee0656cb047d2ac536b89b38a40a7",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5392f91a4ff04388be58e48f1a8789c9",
            "value": 33
          }
        },
        "324ac4ca2f934819a3f88762dfd3c2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb4535d9254040fc8253d9eb625d05f3",
            "placeholder": "​",
            "style": "IPY_MODEL_003b7e68166345c1acd9009714ecd9b1",
            "value": " 33/33 [00:03&lt;00:00,  9.57it/s]"
          }
        },
        "418f14f7a66d4e22851ca06a519c6eda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e450ac25fe0c404a91837349ccee8664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b2d80d1eedd4a84aa1a94865d085cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7bee0656cb047d2ac536b89b38a40a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5392f91a4ff04388be58e48f1a8789c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb4535d9254040fc8253d9eb625d05f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "003b7e68166345c1acd9009714ecd9b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}